{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is regression ?\n",
    "\n",
    "> Modeling problems where the output is a continuous numeric value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression and least squares\n",
    "\n",
    "A linear regression model will try to predict the single value of the dependent variable $y$ given the independent variable $x$, with the most generic form being: \n",
    "\n",
    "$$y = f (x | β), $$\n",
    "\n",
    "where $x$ corresponds to the input variable(s) and $β$ is an array of parameters.\n",
    "\n",
    "The simplest linear model we can think of is a line:\n",
    "\n",
    "$$y = β_0 + β_1 x $$\n",
    "\n",
    "where $β_0$ and $β_1$ have the usual meaning of intercept and slope of a line. \n",
    "\n",
    "Generalizing a bit, for an input vector $x^T = (x_1, x_2, ..., x_N)$,  where $N$ is the total number of observations (or samples), the linear model to predict the real-valued output $y$ is:\n",
    "\n",
    "$$ f(x) = β_0 + \\sum_{i=1}^{N} x_i β_i. $$\n",
    "\n",
    "To find these parameters we use the Ordinary Least Squares approach, i.e. we try to minimize the residual sum of the squares between the observations and the predictions by the model (i.e. the cost function):\n",
    "\n",
    "$$ RSS(β) = \\sum_{i=1}^{N} (y_i - f(x_i))^2 =  \\sum_{i=1}^{N} (y_i - β_0 - x_i β_i)^2 .$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias - Variance tradeoff\n",
    "\n",
    "In the previous section we used a linear function to describe a problem. Using the data we can estimate the parameters of the function. However, we **assume** that the underlying function is linear in nature. In reality, when we examine data from physical (e.g. astronomical) problems this assumption may not hold, and a non-linear function may be more appropriate. Consequently we end up with some errors on our predictions. The total error (estimated on the test set) is:\n",
    "\n",
    "<p style=\"text-align: center;\">Total Error = Variance + Bias$^2$ + Irreducible error,</p>\n",
    "\n",
    "(for its derivation see [Understanding the Bias-Variance Tradeoff, by Scott Fortmann-Roe](http://scott.fortmann-roe.com/docs/BiasVariance.html) or [Bias–variance tradeoff, Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)) where: \n",
    "\n",
    "- **Bias**: is the error introduced by the model's assumptions and it shows how off our model predictions are from the true values - in other words, how well our model represents the problem. <br>\n",
    "An example of the error imposed by the bias is when we try to fit a non-linear distribution of data with a line. In this case the model has a high bias, so it **underfits** the data.<br>  \n",
    "\n",
    "- **Variance**: is the error introduced by changing the training set, and reflects how sensitive the model is to the specifics of the training set. <br>In the case that the error (note: on the test set) changes a lot using different training samples then the model has a large variance, so it **overfits**.\n",
    "\n",
    "- **Irreducible error**: is simply the error that cannot be removed from any model (e.g. the noise of the measurements).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/bullseye.png\" width=400> \n",
    "Figure 3.1. Graphical illustration of bias and variance. <br>\n",
    "(Credit: <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Bias-Variance Tradeoff, by Scott Fortmann-Roe </a>)\n",
    "    </img>\n",
    "    </div>\n",
    "\n",
    "Ideally, we would like a model to have **both** low bias and variance, i.e. to be accurate and robust. Unfortunately, this is not the case (since we do not know the real function that governs the data) and there is a tradeoff between these two parameters.  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/biasvariance.png\"> \n",
    "Figure 3.2. Bias, variance and total error with complexity. <br>\n",
    "(Credit: <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Bias-Variance Tradeoff, by Scott Fortmann-Roe </a>)\n",
    "    </img>\n",
    "    </div>\n",
    "    \n",
    "**High bias models tend to have low variance**. For example, Linear (or Logistic) Regression assumes a simple linear relationship between input data and outcome (high bias) but it is relative robust to the selection of the training sets (low variance). \n",
    "\n",
    "**Low bias models tend to have high variance**. For example, a Decision Tree (or k-Nearest Neighbors, or Support Vector Machines) model does not assume a specific relation between the input data and outcome (low bias) but it can easily fully learn each different training set (high variance).\n",
    "\n",
    "Therefore, our goal is to **minimize the Total Error**, and not necessarily the bias or the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How can we achieved this ?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for some ideas</summary>\n",
    "<ul>\n",
    "    <li>check for correlations (linear regression)\n",
    "    <li>pre-procesing of data (e.g. scaling)\n",
    "    <li>test different algortihms or more complex configurations (e.g. bagging, boosting, or automl)\n",
    "    <li>use cross-validation to optimize hyperparameters (e.g. pruning Decision Trees, finding best k for kNN or C parameters for SVM)\n",
    "    <li>more data (not always possible...)\n",
    "    </ul>\n",
    "    \n",
    "For linear regression check the next section !\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso regularization \n",
    "\n",
    "> Regularization: The application of certain procedures that try to reduce the complexity of a model and avoid overfitting.\n",
    "\n",
    "In the case of linear regression there is a number of **shrinkage** approaches, where the coefficients $β$ \"shrink\" or \"penalized\". \n",
    "\n",
    "**Why to do that ?** Because, lower coefficients are less sensitive to changes of the training sample and, consequently, less prone to overfit. Additionally, correlation between variables can lead to poor determination of their coefficients and to high variance. For example, it is possible to find a large positive and a large negative coefficient on two different variables that cancel out. By constraining these we effectively decrease the variance without affecting the prediction error.\n",
    " \n",
    "By taking into account a set of $N$ observations with $p$ features now (advancing slowly to more general and complex problems), the residual sum of squares with the penalty function (i.e. cost function) can be rewritten as:\n",
    "\n",
    "$$ RSS_{penalized}(β) = \\sum_{i=1}^{N} (y_i - f(x_i))^2 + P(β) = \\sum_{i=1}^{N} (y_i - β_0 - \\sum_{j=1}^{p}x_{ij} β_j)^2 + P(β). $$\n",
    "\n",
    "**Ridge** \n",
    "\n",
    "In this case the penalty is imposed by the absolute square sum of the coefficients (also referred to as L2 regularization). So the modified cost function becomes:   \n",
    "\n",
    "$$ RSS_{L2}(β) = \\sum_{i=1}^{N} (y_i - β_0 - \\sum_{j=1}^{p}x_{ij} β_j)^2 + λ \\sum_{j=1}^{p} β_j^2. $$\n",
    "\n",
    "**Lasso**\n",
    "\n",
    "Similarly, the Lasso (least absolute shrinkage and selection operator) is a penatly imposed by the absolute sum of the coefficients (referred to as L1 regularizaiton). The modified cost function becomes:\n",
    "\n",
    "$$ RSS_{L1}(β) = \\sum_{i=1}^{N} (y_i - β_0 - \\sum_{j=1}^{p}x_{ij} β_j)^2 + λ \\sum_{j=1}^{p} |β_j|. $$\n",
    " \n",
    "In both cases $λ\\geq0$ is a complexity parameter controling the amount of shrinkage (the greater it is the greater the coefficients will shrink). If $λ=0$ then no penatly is enforced and the coefficients become the same to those derived from the Ordinary Least Squares. \n",
    "\n",
    "Although both Ridge and Lasso can shrink the coefficients, Lasso can actually remove them by setting some of them to 0. That way it can remove features and work as a sort of feature selection.  \n",
    "\n",
    "NOTE: $λ$ is a hyperparameter in sklearn (denoted actually as alpha). \n",
    "\n",
    "&#9733;  For more details see [Trevor Hastie, Robert Tibshirani, and Jerome Friedman - The Elements of Statistical Learning (2009)](https://hastie.su.domains/ElemStatLearn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On metrics ... or how well can we do\n",
    "\n",
    "> _Accuracy is a measure for classification not regression_<br>\n",
    "> _We cannot calculate accuracy for a regression model_\n",
    ">\n",
    ">  _by Jason Brownlee [Regression Metrics for Machine Learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)_\n",
    "\n",
    "In regression we are dealing with continuous values. Therefore, it is actually impossible to predict the exact same values. The idea is to get an estimate of how close the predictions are to the expected values.\n",
    "\n",
    "We are going to refer only a few of the available metrics in [sklearn](https://machinelearningmastery.com/regression-metrics-for-machine-learning/). In the following $y$ refers to the dependent values while $\\hat{y}$ to the predicted values.\n",
    "\n",
    "**--> Mean Squared Error (MSE)**\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 $$  \n",
    "\n",
    "It is actually the cost function of the Ordinary Least Squares (check $RSS(β)$ above). The units returned in this case are squared. Best score is 0.\n",
    "\n",
    "**--> Root Mean Squared Error (RMSE)**\n",
    "\n",
    "$$ RMSE = \\sqrt{ \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 } $$  \n",
    "\n",
    "It returns the square root of MSE so that the units match the units of the target value (so better interpretation).  Best score is 0.\n",
    "\n",
    "**--> Mean Absolute Error (MAE)**\n",
    "\n",
    "$$ MAE = {1 \\over N}\\sum_i^N{|  y_i-\\hat{y_i} |}$$  \n",
    "\n",
    "It is less sensitive to large errors when compared to (R)MSE. The score is in units of the target value and the best \n",
    "\n",
    "> Comment: Although arithetically the best scores for (R)MSE and MAE is 0 this cannot be the case in real-life problems. Instead a baseline model has to be determined and calculate its score. Then, any model that can achieve a score better that the baselie model is accepted as a skilful model.  \n",
    "\n",
    "\n",
    "**--> R2 (coefficient of determination)**\n",
    "\n",
    "$$R^2 = 1 - {\\sum_{i=1}^N{(y_i-\\hat{y_i})^2} \\over \\sum_{i=1}^N{(y_i - \\bar{y})^2}}, $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^N y_i$. \n",
    "\n",
    "It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 1: HECATE\n",
    "\n",
    "We will use data derived from the Heraklion Extragalactic Catalogue (HECATE; [Kovlakas et al, 2021](https://ui.adsabs.harvard.edu/abs/2021MNRAS.506.1896K/abstract)) which is an all-sky galaxy catalogue, containing about 200k galaxies (up to z=0.047, D≲200Mpc), and it offers positions, sizes, distances, morphological classifications, star formation rates, stellar masses, metallicities, and nuclear activity classifications. In particular, we are going to use the data for velocity and distance in order to replicate Figure 3 from the paper and fit it with linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = \"data/HECATE_V_D.csv\"\n",
    "\n",
    "data = np.genfromtxt(dfile, dtype=None, \n",
    "                     comments='#', delimiter=',', \n",
    "                     names=True, autostrip='Yes'\n",
    "                     )\n",
    "#examine data\n",
    "print(\"Let us see what we have:\\n\")\n",
    "print(\"The column names:\")\n",
    "print(data.dtype.names)\n",
    "print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 'V_VIR' as input x and 'MOD' as an ouput y \n",
    "X_plot = data['V_VIR'] \n",
    "y_plot = data['MOD']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "plt.scatter(X_plot, y_plot, c='k', marker='.')\n",
    "\n",
    "plt.xlabel('Virgo-infall corrected velocity (km$s^{-1}$)')\n",
    "plt.ylabel('Distance modulus (mag)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that the X axis has values from 0-14000 while the y axis has values approximately in the range 17-40. \n",
    "\n",
    "So we can **rescale** the data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dt = np.column_stack((data['V_VIR'],data['MOD']))\n",
    "scaler.fit( dt )\n",
    "\n",
    "print(f'Means: {scaler.mean_}')\n",
    "print(f'Variance: {scaler.var_}')\n",
    "\n",
    "new_dt = scaler.transform( dt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_dt[:,0].reshape(-1, 1)\n",
    "y = new_dt[:,1]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "plt.scatter(X, y, c='k', marker='.')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a subsample (using indeces) for illustration purposes. \n",
    "\n",
    "&#9755; In this example there are so many points that the linear fit is pretty robust, so we chose to work with a more limited sample to showcase the variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "minX = -2  \n",
    "maxX = 1.5     \n",
    "rand = 300\n",
    "\n",
    "random.seed() \n",
    "range_indcs = np.where( (X>minX) & (X<maxX) )[0]\n",
    "sel_indcs = random.sample(list(range_indcs), rand)   # sample from the previous range\n",
    "\n",
    "print(f'The full length of X is {len(X)} points,')\n",
    "print(f'while there are {len(range_indcs)} in the range {minX}-{maxX}.')\n",
    "print(f'We randomly select {rand} of them')\n",
    "#print(sel_indcs)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[sel_indcs], y[sel_indcs],\n",
    "                        test_size=0.3) #, random_state=42) \n",
    "\n",
    "print(f'- From {len(X)} sources:')\n",
    "print(f'   {len(X_train)} (train)')\n",
    "print(f'   {len(X_test)} (test)') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(f'Score for the training set: {reg.score(X_train, y_train):0.4f}')\n",
    "print(f'Score for the test set: {reg.score(X_test, y_test):0.4f}')\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.plot(X_train, y_train, 'k.', label='train')\n",
    "plt.plot(X_test, y_test, 'go', label='test' ) \n",
    "plt.plot(X_test, y_pred, '--r', label='model' ) \n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2s = r2_score(y_test, y_pred)\n",
    "print('Errors:')\n",
    "print(f'- MSE: {mse:0.3f}')\n",
    "print(f'- RMSE: {rmse:0.3f}')\n",
    "print(f'- MAE: {mae:0.3f}')\n",
    "print(f'- R2: {r2s:0.3f}')\n",
    "\n",
    "      \n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The more general linear regression\n",
    "\n",
    "Linear regression refers to modeling functions that are linear with respect to the parameters (coefficients) and not with respect to the variables! For example, the function:\n",
    "\n",
    "$$f (x|β) = \\sum_{i=1}^N β_i g_i(x) = β_1 g_1(x) + β_2 g_2(x)~+~...~+~β_N g_N(x) $$\n",
    "\n",
    "describes a linear problem as long as the sub-functions $g_i(x)$ do not depend on any of the parameters $β_i$. This is not the most generic formulation of the linear regression but we are going to use this form in the following applications.\n",
    "\n",
    "## Benefits of machine learning regression\n",
    "\n",
    "Ideally, we would like to use a theoretically-derived funtion which could describe the observed distribution of the data. By fitting its parameters to the data we could simply preform the regression (we have seen this e.g. in the Bayesian session). However this is not always possible because e.g. we might be facing a new problem or because we are simply not satisfied by the available options in the literature. \n",
    "\n",
    "Even if we do not know the intrinsic form, we can still mimic any function by using a model $f (x|β)$ composed of an arbitrary number of sub-functions. One common approach is to use \"Basis Functions\", i.e. expand $f(x)$ over a _specific_ \"family\" of functions $g_i(x)$, for example a series of Gaussians with different means and variances.\n",
    "\n",
    "How many functions do we need? We do cannot decide that a priori. The more functions we use, the better the model will fit the data, but the number of parameters increases (i.e. the size of $β_i$) and so does the risk overfitting. \n",
    "\n",
    "That is exactly where Ridge and Lasso apply, since the penalize the individual functions  $g_i(x)$ (while Lasso can actually remove them as uneccesary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models for HECATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to create the gaussian models\n",
    "\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "n_gaussians = 100\n",
    "centers = np.linspace(np.min(X), np.max(X), n_gaussians)\n",
    "widths = 1    \n",
    "\n",
    "########################################################\n",
    "\n",
    "# models to work with\n",
    "models = dict(\n",
    "    Linear = LinearRegression(), \n",
    "    Polynomial = make_pipeline(PolynomialFeatures(3), LinearRegression()),\n",
    "    Gaussian = LinearRegression()\n",
    ")   \n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[sel_indcs], y[sel_indcs] , \n",
    "                                            test_size=0.3, random_state=12) \n",
    "\n",
    "# figure plot\n",
    "fig, axs = plt.subplots(1,3, figsize=(14,6), sharey=True)\n",
    "\n",
    "# dummy array for illustration purposes\n",
    "X_plot = np.linspace(np.min(X), np.max(X), 1000)[:, None]\n",
    "\n",
    "for k, ax in enumerate(axs.reshape(-1)):\n",
    "    name = list(models.keys())[k]\n",
    "    mod = list(models.values())[k]\n",
    "    print(f'> Running {name} with {mod}')\n",
    "    \n",
    "    # plot all data as background\n",
    "    ax.plot(X, y, '.', color='gray')\n",
    "\n",
    "    if name == 'Linear' or name == 'Polynomial':\n",
    "        # fit and predict\n",
    "        mod.fit(X_train, y_train)\n",
    "        y_pred = mod.predict(X_test)\n",
    "\n",
    "        y_pred_plot = mod.predict(X_plot.reshape(-1,1))\n",
    "\n",
    "    else:        \n",
    "        mod.fit( gaussian_basis(X_train, centers, widths), y_train)\n",
    "        y_pred = mod.predict( gaussian_basis(X_test, centers, widths) )\n",
    "        \n",
    "        y_pred_plot = mod.predict( gaussian_basis(X_plot, centers, widths) )\n",
    "        \n",
    "    # calculate errors\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2s = r2_score(y_test, y_pred)\n",
    "\n",
    "    ax.plot(X_plot,y_pred_plot , ls='-', lw=5, \n",
    "            label=f\"MSE: {mse:0.3f}, R2: {r2s:0.3f}\")\n",
    "\n",
    "\n",
    "    ax.set_title( name )\n",
    "    ax.legend()\n",
    "\n",
    "plt.ylim(-12,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# models to work with\n",
    "models2 = dict(\n",
    "    Linear = LinearRegression(),\n",
    "    Ridge = Ridge(0.01), \n",
    "    Lasso = Lasso(0.005, max_iter=2000) \n",
    ")   \n",
    "\n",
    "# split dataset\n",
    "gX = gaussian_basis(X, centers, widths)\n",
    "gX_train, gX_test, y_train, y_test = train_test_split(gX[sel_indcs], y[sel_indcs] , \n",
    "                                            test_size=0.3, random_state=12) \n",
    "\n",
    "# figure plot\n",
    "fig, axs = plt.subplots(1,3, figsize=(14,6), sharey=True)\n",
    "\n",
    "# dummy array for illustration purposes\n",
    "X_plot = np.linspace(np.min(X), np.max(X), 5000)[:, None]\n",
    "gX_plot = gaussian_basis(X_plot, centers, widths)\n",
    "\n",
    "for k, ax in enumerate(axs.reshape(-1)):\n",
    "    name = list(models2.keys())[k]\n",
    "    mod = list(models2.values())[k]\n",
    "    print(f'> Running {name} with {mod}')\n",
    "    \n",
    "    # plot all data as background\n",
    "    ax.plot(X, y, '.', color='gray')\n",
    "\n",
    "    # fit and predict\n",
    "    mod.fit(gX_train, y_train)\n",
    "    y_pred = mod.predict(gX_test)\n",
    "    \n",
    "   \n",
    "    # calculate errors\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2s = r2_score(y_test, y_pred)\n",
    "\n",
    "    ax.plot(X_plot, mod.predict(gX_plot), ls='-', lw=5, \n",
    "            label=f\"MSE: {mse:0.3f}, R2: {r2s:0.3f}\")\n",
    "\n",
    "    ax.set_title( name )\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "plt.ylim(-10,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice? \n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "The application of regularization helps to constrain the model (reduce MSE annd increase R2). Ridge seems to work better in this case than Lasso.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better illustration of the influence on the parameters\n",
    "\n",
    "Let's suppose to have data for the distance modulus $\\mu$ of SNs as a function of $z$. This is a complicated function which we opt to ignore. Intead the function $\\mu(z)$ (the predictive function) is expanded over a base of 100 Gaussians.\n",
    "\n",
    "For semplicity, we will assume the Gaussians all have the same $\\sigma$ = 2. The centers of the Gaussians are also fixed (one centered every $\\Delta z \\sim$ 0.025). What is left to be fit are then just the normalizations of the Gaussians. These normalizations will constitute the values of the set **θ** for this example. For each Gaussian centered at a given $z$ we will then have one $θ_p$($z$).\n",
    "\n",
    "Note: The example is adopted from \"Statistics, Data Mining, and Machine Learning in Astronomy\" -  §§ 8, with the [code available publicly](http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html#book-fig-chapter8-fig-rbf-ridge-mu-z)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/parameters.png\"> \n",
    "Figure 7.1. The influence of the Ridge and Lasso regularization on the parameters. <br>\n",
    "    </img>\n",
    "    </div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you observe?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "-- Unconstrained linear regression drastically overfits. Moreover the range of parameters (and their distribution's variance) is very large. Looks like one component cancels out with the next one, therefore a lot of components are needed.\n",
    "\n",
    "-- Ridge regression \"regularizes\" the distribution of the parameter values, but introduces a sinusoidal correlation (?).\n",
    "\n",
    "-- LASSO regression crops out several Gaussian components, as it allows some of the parameters to go to 0. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks on Ridge/LASSO penalization\n",
    "\n",
    "**Pros**\n",
    "* Good when data points are few but still want to expand the function $f$ over a family of functions $g_i(x)$ while limiting over-fitting\n",
    "* Useful with noisy data where the underlying functional form is not clear\n",
    " \n",
    "**Cons**\n",
    "* The selection of the surviving \"significant parameters\" $β_i$ is extremely arbitrary: not good for defining physically meaningful functions \n",
    "* Introduces parameter correlation\n",
    "* Computationally intensive\n",
    "* May not be very accurate when the sample has too few data points\n",
    "  <br>\n",
    "  (Choosing the right bandwidth value can mitigate this problem)\n",
    "\n",
    "**Caveat**<br>\n",
    "As usual, the value $\\lambda$ must be optimized using a cross-validation sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 2: Use regression to fit and subtract galaxy continuum\n",
    "\n",
    "In previous sessions (e.g. Introduction and MCMC) we have explored some methods to fit spectral lines.\n",
    "\n",
    "In this exercise, we want to isolate the spectral emission lines of a late-type galaxy (e.g. a spiral) by removing the stellar continuum from the spectrum. This procedure represents a very common step in the spectral data reductions, and it is performed through a variety of methods. The simplest approaches focus on a line at a time, removing only the local continuum by fitting it e.g. with a polynomial. Sophisticated methods fit the whole spectrum using appropriate stellar population templates. Here we will use linear regression as a quick method to obtain a fast continuum subtraction without dealing with detailed tuning.\n",
    "\n",
    "We will use an optical spectrum from SDSS from the\n",
    "[Spectral cross-correlation templates](http://classic.sdss.org/dr5/algorithms/spectemplates/)\n",
    "\n",
    "--- \n",
    "\n",
    "**TASK 1**: Use the Gaussian Basis expansion and the Ridge/LASSO regularization to fit the spectrum\n",
    "\n",
    "HINT: Try to run the regression with the full spectrum, and then removing the features we are most interested in isolating.\n",
    "\n",
    "**TASK 2**: Compare your results with the background-subtracted spectrum distributed along with the SDSS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the SDSS spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE SDSS SPECTRUM\n",
    "#\n",
    "# To understand the SDSS file format, consult:\n",
    "#   http://www.sdss2.org/dr3/dm/flatFiles/spSpec.html\n",
    "\n",
    "# FITS manipulation:\n",
    "from astropy.io import fits\n",
    "\n",
    "PATH_spectrum = \"data/late-type.fits\"\n",
    "    \n",
    "hdulist = fits.open(PATH_spectrum)\n",
    "data = hdulist[0].data\n",
    "# Header keywords to perform wavelength calibration:\n",
    "coeff0 = hdulist[0].header['coeff0']\n",
    "coeff1 = hdulist[0].header['coeff1']\n",
    "spectrum_flux  = data[0] # observed spectrum\n",
    "spectrum_noBG  = data[1] # contimuum-subtracted spectrum\n",
    "hdulist.close()\n",
    "\n",
    "spectrum_flux = spectrum_flux[:-100]\n",
    "spectrum_noBG = spectrum_noBG[:-100]\n",
    "# NOTE: Removing last 100 data points, which are bogus\n",
    "\n",
    "# Creating arbitrary uncertainty array proportional to the flux:\n",
    "spectrum_flux_err = spectrum_flux * np.random.uniform(0.01, 0.1, size=len(spectrum_flux))\n",
    "\n",
    "# Creating wavelength array using header calibrations:\n",
    "spectrum_wave = 10.0 ** (coeff0 + coeff1 * np.arange(len(spectrum_flux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the spectrum\n",
    "\n",
    "In plotting, we will highlight 2 strong lines in the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining ranges of strong features:\n",
    "#   http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\n",
    "feature_0 = [6550, 6580] # H_alpha 6562.819\n",
    "feature_1 = [6710, 6750] # SII 6730.810\n",
    "# wavelength min and max for each feature [A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Displaying spectrum:    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "# fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "# ax = plt.subplot(111)\n",
    "\n",
    "plt.title('Late-type galaxy spectrum', fontsize=14)\n",
    "\n",
    "plt.xlabel('$\\lambda$ [A]', fontsize=14)\n",
    "plt.ylabel('$f_{v}$ [Jy]',  fontsize=14)\n",
    "#\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.gca().tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "plt.gca().tick_params(axis = 'both', which = 'minor', labelsize = 14)\n",
    "\n",
    "plt.errorbar(spectrum_wave, spectrum_flux, spectrum_flux_err, fmt='blue', ecolor='gray', lw=1, ms=4)\n",
    "\n",
    "# Marking emission features:\n",
    "plt.axvspan(feature_0[0], feature_0[1] , color='grey', alpha=0.2)\n",
    "plt.axvspan(feature_1[0], feature_1[1],  color='grey', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gaussian Basis expansion and the Ridge/LASSO regularization to fit the spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING FEATURES FROM DATA\n",
    "#\n",
    "# We remove the wavelength ranges corresponding to the features we want to isolate,\n",
    "# to avoid that some of the Gaussians in the Gaussian Basis attempt to fit the features\n",
    "\n",
    "valid_range = (((spectrum_wave <  feature_0[0] ) | (spectrum_wave > feature_0[1]  )) &\n",
    "               ((spectrum_wave <  feature_1[0] ) | (spectrum_wave > feature_1[1] )))\n",
    "\n",
    "wave        = spectrum_wave[valid_range]\n",
    "flux        = spectrum_flux[valid_range]\n",
    "flux_err    = spectrum_flux_err[valid_range]\n",
    "\n",
    "# Restore feature ranges in data:\n",
    "# wave        = spectrum_wave\n",
    "# flux        = spectrum_flux\n",
    "# flux_err    = spectrum_flux_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS:\n",
    "n_gaussians = ...\n",
    "widths = ...\n",
    "\n",
    "ridge_alfa = ...\n",
    "lasso_alfa = ...\n",
    "\n",
    "# MODEL PARAMETERS:\n",
    "\n",
    "# Manually converting data to a gaussian basis:\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(np.min(wave), np.max(wave), n_gaussians)\n",
    "\n",
    "X = gaussian_basis(wave[:, np.newaxis], centers, widths)\n",
    "\n",
    "########################################################\n",
    "\n",
    "# NOTE: Most of the code in this block represents an edited version of:\n",
    "# http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html#book-fig-chapter8-fig-rbf-ridge-mu-z\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# sampled array for plotting purposes\n",
    "x_plot = np.linspace(np.min(wave), np.max(wave), 1000)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95, hspace=0.15, wspace=0.2)\n",
    "\n",
    "classifier = [LinearRegression, Ridge, Lasso]\n",
    "kwargs = [dict(), dict(alpha=ridge_alfa), dict(alpha=lasso_alfa)]\n",
    "labels = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf....\n",
    "    w = clf.coef_\n",
    "    # USE THE SAMPLED ARRAY FOR PLOTTING PURPOSES HERE \n",
    "    fit = ...\n",
    "   \n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(231 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    #\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$f_{v}$ [Jy]')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        axis_ymin, axis_ymax = min(flux), max(flux)\n",
    "        w_plot = (0.3 * w / max(w)) * (axis_ymax - axis_ymin) + axis_ymin\n",
    "        curves = w_plot * gaussian_basis(x_plot[:, np.newaxis], centers, widths)\n",
    "\n",
    "        ax.plot(x_plot, curves, c='gray', lw=1, alpha=0.5)\n",
    "    \n",
    "    # NOTE: converting map to list or else plotting won't work\n",
    "    \n",
    "    ax.plot(x_plot, fit, '-k')\n",
    "\n",
    "    ax.errorbar(wave, flux, flux_err, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(np.min(wave),np.max(wave))\n",
    "    ax.set_ylim(np.min(flux),np.max(flux))\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "    \n",
    "    \n",
    "    # plot weights\n",
    "    ax = plt.subplot(234 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$\\lambda$ [A]')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\beta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(np.min(centers), np.max(centers))\n",
    "\n",
    "    ax.set_ylim(np.min(w), np.max(w))\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice wrt to the parameters?\n",
    "<br>\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "We see how the parameters shrink as we move from linear regression to ridge and lasso, where some of them are actually 0.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing background subtraction with SDSS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESAMPLING LASSO FITTED CURVE TO DATA\n",
    "#\n",
    "# In order to plot the fitting curve produced by the classifier (\"fit\")\n",
    "#   along with \"spectrum_flux\", we first resample it to the same\n",
    "#   array, i.e. \"sample_wave\"\n",
    "# This is because \"fit\" has been sampled on \"x_plot\", which has\n",
    "#   a different sampling than \"spectrum_wave\"\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "clf = classifier[2](fit_intercept=True, **kwargs[i]) # LASSO\n",
    "clf.fit(X, flux)\n",
    "\n",
    "fit = clf.predict(gaussian_basis(x_plot[:, None], centers, widths))\n",
    "\n",
    "f = interp1d(x_plot, fit, kind='cubic')\n",
    "# interpolation function\n",
    "# - linear\n",
    "# - quadratic\n",
    "# - cubic\n",
    "fit_interp = f(spectrum_wave) \n",
    "\n",
    "\n",
    "# PLOTTING\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# > Displaying spectrum:    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.set_title('Late-type galaxy - continuum-subracted spectrum', fontsize=14)\n",
    "\n",
    "ax.set_xlabel('$\\lambda$ [A]', fontsize=14)\n",
    "ax.set_ylabel('$f_{v}$ [Jy]', fontsize=14)\n",
    "\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = 14)\n",
    "\n",
    "ax.plot... # plot continuum-subtracted spectrum from our analysis\n",
    "ax.plot... # plot continuum-subtracted spectrum from SDSS\n",
    "\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The SDSS spectrum have been arbitrarily shifted for presentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
