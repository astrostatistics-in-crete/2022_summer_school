{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification as a prototype of machine learning technique\n",
    "\n",
    "One typical example of machine learning is the classification problem. This is the process of attributing a \"**class**\" (or equivalently, a \"**label**\") to an object of an arbitrary type (e.g. a string, an image, or numbers) in order to catalogue it based on its **properties** (the actual information that we pass to the machine, e.g. the pixel intensities in the case of an image).\n",
    "\n",
    "<center><img src=\"images/classification.png\" width=500> \n",
    "Figure 1.1. Schematic classification in a 2D plot.<br>\n",
    "(Credit: <a href=\"https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d\"  target=\"_blank\" rel=\"noopener noreferrer\">Supervised vs. Unsupervised Learning, by Devin Soni</a>)</center>\n",
    "\n",
    "## Types of Classifications\n",
    "\n",
    "Classification can come in two major flavors, based on the type of intervention by the user:\n",
    "\n",
    "- **Unsupervised**: The classification is defined \"unsupervised\" when the user does not provide labels during the training process and the machine learns the definition of each class from the data. This is exactly the case of \"Clustering\" that we saw in the previous session.  \n",
    "\n",
    ">    _In practice, the machine learns to cluster objects with similar properties._\n",
    "\n",
    "\n",
    "- **Supervised**: The classification is defined \"supervised\" when the user provides a label for each object in the training set. In this case, the idea is that we can train the model to associate the label with some given characteristics of the training data.\n",
    "\n",
    ">    _In practice, the machine learns to find similarities between objects with the same label._\n",
    "\n",
    "## Supervised classification - generative / discriminative classification\n",
    "\n",
    "One approach to the classification problem involves the recognition of the function describing the density distribution of each class in the parameter space. This type of problem is called **generative classification** because it implies that we can find the distribution from which the data are generated (or better said, sampled).\n",
    "\n",
    "**Discriminative classification** instead, includes any method which separates classes by \"drawing\" a boundary in the parameter space.\n",
    "\n",
    "In the reminder, we will only focus on the latter type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification algorithms overview\n",
    "\n",
    "We start by presenting a set of [**sklearn**](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) algorithms with toy datasets, and then describe some of them. \n",
    "\n",
    "This serves as a showcase of the available methods and how they compare. You can easily adapt any of these methods to the following examples or your own problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We provide the code fully here for completeness although it does not run\n",
    "# # due to the fact that DecisionBoundaryDisplay function is in the experimental version\n",
    "\n",
    "# ######################################################################################\n",
    "\n",
    "# # Code source: Gaël Varoquaux\n",
    "# #              Andreas Müller\n",
    "# # Modified for documentation by Jaques Grobler\n",
    "# # License: BSD 3 clause\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# # Generate toy dataset\n",
    "\n",
    "# X, y = make_classification(\n",
    "#     n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    "# )\n",
    "# rng = np.random.RandomState(2)\n",
    "# X += 2 * rng.uniform(size=X.shape)\n",
    "# linearly_separable = (X, y)\n",
    "\n",
    "# datasets = [\n",
    "#     make_moons(noise=0.3, random_state=0),\n",
    "#     make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "#     linearly_separable,\n",
    "# ]\n",
    "\n",
    "# # Setting up the classification algorithms (parameters)\n",
    "\n",
    "# names = [\n",
    "#     \"Nearest Neighbors\",\n",
    "#     \"Linear SVM\",\n",
    "#     \"RBF SVM\",\n",
    "#     \"Gaussian Process\",\n",
    "#     \"Decision Tree\",\n",
    "#     \"Random Forest\",\n",
    "#     \"Neural Net\",\n",
    "#     \"AdaBoost\",\n",
    "#     \"Naive Bayes\",\n",
    "#     \"QDA\",\n",
    "# ]\n",
    "\n",
    "# classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "#     SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "# ]\n",
    "\n",
    "# # Running and plotting\n",
    "\n",
    "# figure = plt.figure(figsize=(27, 9))\n",
    "# i = 1\n",
    "# # iterate over datasets\n",
    "# for ds_cnt, ds in enumerate(datasets):\n",
    "#     # preprocess dataset, split into train and test part\n",
    "#     X, y = ds\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.4, random_state=42\n",
    "#     )\n",
    "\n",
    "#     x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "#     y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "#     # just plot the dataset first\n",
    "#     cm = plt.cm.RdBu\n",
    "#     cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "#     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#     if ds_cnt == 0:\n",
    "#         ax.set_title(\"Input data\")\n",
    "#     # Plot the training points\n",
    "#     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "#     # Plot the testing points\n",
    "#     ax.scatter(\n",
    "#         X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "#     )\n",
    "#     ax.set_xlim(x_min, x_max)\n",
    "#     ax.set_ylim(y_min, y_max)\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     i += 1\n",
    "\n",
    "#     # iterate over classifiers\n",
    "#     for name, clf in zip(names, classifiers):\n",
    "#         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         score = clf.score(X_test, y_test)\n",
    "#         DecisionBoundaryDisplay.from_estimator(\n",
    "#             clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "#         )\n",
    "\n",
    "#         # Plot the training points\n",
    "#         ax.scatter(\n",
    "#             X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "#         )\n",
    "#         # Plot the testing points\n",
    "#         ax.scatter(\n",
    "#             X_test[:, 0],\n",
    "#             X_test[:, 1],\n",
    "#             c=y_test,\n",
    "#             cmap=cm_bright,\n",
    "#             edgecolors=\"k\",\n",
    "#             alpha=0.6,\n",
    "#         )\n",
    "\n",
    "#         ax.set_xlim(x_min, x_max)\n",
    "#         ax.set_ylim(y_min, y_max)\n",
    "#         ax.set_xticks(())\n",
    "#         ax.set_yticks(())\n",
    "#         if ds_cnt == 0:\n",
    "#             ax.set_title(name)\n",
    "#         ax.text(\n",
    "#             x_max - 0.3,\n",
    "#             y_min + 0.3,\n",
    "#             (\"%.2f\" % score).lstrip(\"0\"),\n",
    "#             size=15,\n",
    "#             horizontalalignment=\"right\",\n",
    "#         )\n",
    "#         i += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/sklearn_classification.png\" width=1200> \n",
    "Figure 2.1. Overview of the clustering algorithms in sklearn\n",
    "</img>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizz time: take a few moments and explore the results - what do you notice? \n",
    "\n",
    "Write some points here:\n",
    "\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The sample for our classification example\n",
    "\n",
    "To properly classify a star we need a spectrum. However, this is time consuming and limited to one (single slit spectroscopy) or a few tens of sources (multi-object spectroscopy). On the contrary, imaging in various different filters can be done easily and for thousands of sources per image. Photometry at different wavelengths result in a very low-resolution \"spectrum\".</br> \n",
    "\n",
    "<center><img src=\"images/Girardi2002-photometric systems.gif\"> \n",
    "Figure 2.1. The filter+detector transmission curves for a number of different systems, along with indicative spectra of Vega, the Sun, and a M5 giant. <br>\n",
    "(Fig 3. from <a href=\"https://ui.adsabs.harvard.edu/abs/2002A%26A...391..195G/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Girardi et al. (2002)]</a>)</center>\n",
    "\n",
    "In this example we are using a set of photometric measurements (from optical to mid-IR bands) for a sample of massive evolved stars in the Large Magellanic Cloud (based on these works: [Bonanos et al. (2009) AJ, 138, 1003](https://ui.adsabs.harvard.edu/abs/2009AJ....138.1003B/abstract), [Neugent et al. (2012), ApJ, 749, 177](https://ui.adsabs.harvard.edu/abs/2012ApJ...749..177N/abstract), and [Davies, Crowther & Beasor (2018), MNRAS, 478, 313](https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.3138D/abstract)). \n",
    "\n",
    "<center><img src=\"images/Massey2013-HRD.png\" width=600> \n",
    "Figure 2.1. A slightly modified version of Fig. 1 from <a href=\"https://ui.adsabs.harvard.edu/abs/2013NewAR..57...14M/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Massey et al. (2013)]</a>)</center>\n",
    "\n",
    "Our aim is to use a method that will help us **distinguish different classes** of objects. For our purposes we will use OBA stars (main-sequense objects), OBAe (a subcategory of OBA stars with circumstellar disks and emission lines), Wolf-Rayet stars (hot evolved stars with strong stellar winds that actually strip their envelopes), Yellow and Red supergiants (evolved massive stars). For convenience we will use OBA, OBAe, WR, YSG, and RSG, respectively, as labels.\n",
    "\n",
    "&#9733; A similar, but more elaborated, implementation is performed in [Maravelias et al. (2022), arXiv: 2203.08125](https://arxiv.org/abs/2203.08125).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9755; You do not need ```pandas``` to do everything with files! ```genfromtxt``` is very powerful but has its tweaks! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = \"data/LMC_phot_data.csv\"\n",
    "miss_value = -999.0  # when entries are missing\n",
    "\n",
    "data = np.genfromtxt(dfile, dtype=None, \n",
    "                     comments='#', delimiter=',', \n",
    "                     filling_values = miss_value, \n",
    "                     names=True, autostrip='Yes')\n",
    "\n",
    "#examine data\n",
    "print(\"Let us see what we have:\\n\")\n",
    "print(\"The column names:\")\n",
    "print(data.dtype.names)\n",
    "print(\"-\"*25)\n",
    "print(\"Let's print the spectral types only:\")\n",
    "print(data['SpT'])\n",
    "print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: We want to group the different spectral classes found, and for this we are using the indeces and not the data values directly. in that way we can select all data for the same objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "classes = defaultdict(list)\n",
    "\n",
    "for i in range(0,len(data['SpT'])): \n",
    "#    print(i, data['SpT'][i].decode('utf-8'))\n",
    "    classes[data['SpT'][i].decode('utf-8')].append(i)\n",
    "\n",
    "#print(classes)\n",
    "unique_cls = sorted(set(classes.keys()))\n",
    "print(unique_cls)\n",
    "print(\"> SUMMARY of loaded data:\")\n",
    "print(\"=========================\")\n",
    "for sptype in unique_cls:\n",
    "    number = len(classes[sptype])\n",
    "    print(f\"{sptype:-<6s}--> {number:>3} stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [b for b in data.dtype.names[3:-1] if 'e_' not in b]\n",
    "\n",
    "def reminder():\n",
    "    \"\"\" \n",
    "    A simple function to print all bands\n",
    "    and classes available.\n",
    "    \"\"\"\n",
    "    print('Available bands to use: ')\n",
    "    print(','.join(bands))\n",
    "    print('-'*25)\n",
    "    print('Available classes to use:')\n",
    "    print(','.join(unique_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, Column\n",
    "\n",
    "print(f'Available photometry for: {\", \".join(bands)}')     \n",
    "\n",
    "# Constructing the table for the statistics \n",
    "phot_data_col_names = ['Class', 'All'] + [bb for bb in bands]\n",
    "phot_data_per = Table( names = phot_data_col_names, dtype = ['S3']+['i4']+['f2']*(len(bands)))\n",
    "\n",
    "for spt in unique_cls:\n",
    "    indcs = classes[spt]\n",
    "    starsWbands = defaultdict(list) # keep those with measurements across all\n",
    "    for star in indcs:\n",
    "#        print(spt, star)\n",
    "        for bnd in bands:\n",
    "            mag = data[star][bnd]\n",
    "#            print(mag)\n",
    "            if mag!=miss_value:\n",
    "                starsWbands[bnd].append(star)    \n",
    "    row_data_per = [spt, len(indcs)] + [(len(starsWbands[bb])/len(indcs))*100 for bb in bands]\n",
    "    phot_data_per.add_row ( row_data_per )\n",
    "    \n",
    "print(\"\\nNumber of stars per band (in %)\\n\")\n",
    "phot_data_per    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9755; 36, 45, 58, 89, 24 corresponds to the 3.6um, 4.5um, 5.8um, 8.9um, 24um bands of *Spitzer*. One common representation of these bands is [3.6], [4.5], [5.8], [8.0], [24] but the use of \"[]\" and \".\" is inconvenient, so we drop them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What do you notice here ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data - select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selmags( band1, band2, cls):\n",
    "    \"\"\"\n",
    "    Function to select sources of a specific\n",
    "    spectral class (cls) and return the magnitudes\n",
    "    that correspond to bands 1 and 2.\n",
    "        \"\"\"\n",
    "    # all indeces of the particular class\n",
    "    cls_indcs = np.asarray( classes[cls] ) \n",
    "    # selecting those indeces of the class\n",
    "    # that do not contain missing values, ie -999\n",
    "    sel_cls_indcs = np.where( (data[band1][cls_indcs]!=miss_value)\n",
    "                        & (data[band2][cls_indcs]!=miss_value) )[0]\n",
    "\n",
    "    sel_indcs = cls_indcs[sel_cls_indcs]\n",
    "    rem_indcs = len(cls_indcs)-len(sel_indcs)\n",
    "    print(f'-- {cls}: excluding {rem_indcs} out of {len(cls_indcs)} sources ({rem_indcs/len(cls_indcs)*100:.1f}%)')\n",
    "    mag1, mag2 = data[band1][sel_indcs], data[band2][sel_indcs]    \n",
    "    \n",
    "    return mag1, mag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(12, 12))\n",
    "\n",
    "selected_spt = unique_cls              # if you want to print all\n",
    "#selected_spt = ['RSG', 'OBA', 'WR']     # put your selection here\n",
    "\n",
    "# plot 1\n",
    "band1_1 = ''\n",
    "band1_2 = ''\n",
    "\n",
    "# plot 2\n",
    "band2_1 = ''\n",
    "band2_2 = ''\n",
    "\n",
    "print('- plot1:')\n",
    "for s in selected_spt: \n",
    "    plt1 = selmags( band1_1, band1_2, s)\n",
    "\n",
    "    ax[0,0].plot(plt1[0], plt1[1], 'o', label=f'{s}: {len(plt1[0])}')\n",
    "    ax[0,0].set_xlabel(f'{band1_1}') #'-{band1_2}')\n",
    "    ax[0,0].set_ylabel(band1_2)\n",
    "    ax[0,0].invert_yaxis()\n",
    "    ax[0,0].invert_xaxis()\n",
    "    ax[0,0].legend()\n",
    "\n",
    "    ax[0,1].plot(plt1[0]-plt1[1], plt1[1], 'o', label=f'{s}: {len(plt1[0])}')\n",
    "    ax[0,1].set_xlabel(f'{band1_1}-{band1_2}')\n",
    "    ax[0,1].set_ylabel(band1_2)\n",
    "    ax[0,1].invert_yaxis()\n",
    "    ax[0,1].legend()\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('- plot2:')        \n",
    "for s in selected_spt:\n",
    "    plt2 = selmags( band2_1, band2_2, s)\n",
    "    ax[1,0].plot(plt2[0], plt2[1], 'o', label=f'{s}: {len(plt2[0])}')\n",
    "    ax[1,0].set_xlabel(f'{band2_1}') #'-{band2_2}')\n",
    "    ax[1,0].set_ylabel(band2_2)\n",
    "    ax[1,0].invert_yaxis()\n",
    "    ax[1,0].invert_xaxis()    \n",
    "    ax[1,0].legend()\n",
    "    \n",
    "    ax[1,1].plot(plt2[0]-plt2[1], plt2[1], 'o', label=f'{s}: {len(plt2[0])}')\n",
    "    ax[1,1].set_xlabel(f'{band2_1}-{band2_2}')\n",
    "    ax[1,1].set_ylabel(band2_2)\n",
    "    ax[1,1].invert_yaxis()\n",
    "    ax[1,1].legend()\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time: Experiment with various combinations and try to answer\n",
    "\n",
    "### 1. What happens if you start increasing the number of classes to consider ?\n",
    "\n",
    "    \n",
    "### 2. How the selection of bands influence the objects to keep ? \n",
    "\n",
    "\n",
    "### 3. Would you prefer to use  combinations with few or more objects ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machine (SVM) is a way of choosing a decision boundary between different classes.\n",
    "\n",
    "The classification boundary is provided by the hyperplane maximizing the distance between the hyperplane itself and the closest point from either class. This distance is called **margin**. Points on the margins are called **support vectors**.\n",
    "\n",
    "\n",
    "<center>\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_1.png\">\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_2.png\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "    Figure 3.1. Left: Hyperplane (dashed line) separating two classes (_red_ and _green_). Right: The closest points to the hyperplane from each class constitute the \"tip\" of the support vectors.\n",
    "</center>\n",
    "\n",
    "The left panel of Figure 3.1 a shows two different classes distributing in a scatter plot according to variable $x_1$ and $x_2$. The right panel of Figure 3.1 explains the origin of the name support vectors: the closest points _support_ the hyperplanes (solid lines) equally distant from the decision hyperplane (dashed line).\n",
    "\n",
    "Infinite possible boundaries can separate the two classes. SVM algorithms find the one that maximizes the distance between the supported hyperplanes.\n",
    "\n",
    "## Hyperplanes and decision boundary\n",
    "\n",
    "The supported hyperplanes (solid-lines in Figure 3.1) can be defined as:\n",
    "\n",
    "> w$\\cdot$x + b = +1\n",
    ">\n",
    "> w$\\cdot$x + b = -1\n",
    "\n",
    "where x is the coordinate on the (x1, x2) plane, w is a 2$\\times$1 matrix and b a scalar. It turns out that these hyperplanes are separated by a distance 2 / ||w||. Finding the ideal classification boundary, i.e. the one maximizing the distance, is therefore a problem of minimizing the norm ||w||. This is what SVM algorithms do.\n",
    "\n",
    "&#9733; For a complete mathematical formulation, consult the [Idiot’s guide to Support vector\n",
    "machines, by Robert Berwick]( http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf).\n",
    "\n",
    "## Separatable classes (or not)\n",
    "\n",
    "We cannot always assume that 2 classes are separable without \"contamination\". That is why SVM algorithms includes a tunable parameter ($C$) which penalizes misclassifications.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/svm-parameter-c-example.png\" width=800> \n",
    "Figure 3.2. The effect of <i>C</i> parameter in the misclassifications.<br>\n",
    "(Credit: <a href=\"https://learnopencv.com/svm-using-scikit-learn-in-python/\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\">SVM: What makes it superior to the Maximal-Margin and Support Vector Classifiers?, by Shivam Sharma</a>)\n",
    "    </img>\n",
    "    </div>\n",
    "    \n",
    "- Small $C$ &#8594; wide margin &#8594; allows more misclassification <br>\n",
    "- Large $C$ &#8594; narrow margin &#8594; allows less misclassification    \n",
    "\n",
    "However, the SVM finds the hyperplane that maximizes the margin, and indirectly minimizes the misclassifications. In other words, SVM is not designed to minimize the contamination _per se_.\n",
    "\n",
    "\n",
    "## Multiple classes\n",
    "\n",
    "The SVM method can be applied for multiple classes as well.\n",
    "\n",
    "<center><img src=\"images/svm_many_classes.png\" width=400> \n",
    "Figure 3.3. SVM applied to 3 different classes.<br>\n",
    "</center>\n",
    "\n",
    "## Multiple dimensions\n",
    "\n",
    "If our sample characterized by three parameters (X, Y, Z), then the scatter plot has 3 dimensions. The boundary between the classes in the 3-D plot is a plane. Because of the fact that the method can be extrapolated at N-dimensions, the boundary is a *hyperplane*.\n",
    "\n",
    "<img src=\"images/svm_3d.png\" width=400>\n",
    "<center>\n",
    "    Figure 3.4: Support vector machine applied for 3-D features and three classes.\n",
    "</center>\n",
    "\n",
    "## Non-linear boundaries\n",
    "\n",
    "Sometimes, linear boundaries may not be optimal and a non-linear SVM should be used instead. The left panel of Figure 3.5 shows an 2D scatter plot of two different classes (e.g. red and green stars with different radii and temperatures) which cannot be linearly separated.\n",
    "\n",
    "In order to find non-linear boundaries we can tackle the problem in an higher dimensional space. We use a process called **kernelization**, which consists in using a kernel function to attribute to our data a value in the additional dimension. Then, we draw the decision hyperplane into this higher dimensional space.\n",
    "\n",
    "The central panel of Figure 3.5 shows that once the 2D data are mapped to a 3D space by attributing a $z$ value through a Gaussian-like function, the classes are easily separable by a 3D hyperplane. Projecting back the plane in 2D, we obtain the non-linear boundary (Figure 3.5, rght panel).\n",
    "\n",
    "<img src=\"images/kernel.png\" width=800>\n",
    "<center>\n",
    "    Figure 3.5. When no linear boundaries can be used the SVM method can be applied by using kernel.\n",
    "</center>\n",
    "\n",
    "## Choosing the kernel function\n",
    "\n",
    "Useful kernel functions shall satisfy specific conditions, so that in practice only a few are used. In the example of Figure 3.5, the Gaussian Radial Basis Function is used:\n",
    "\n",
    "> $K(x,y) = e^{-\\gamma(x-y)^2}$\n",
    "\n",
    "where $\\gamma$ is a hyperparameter which shall be learned (in our example we use an arbitrary value but in principle we should use cross-validation methods).\n",
    "\n",
    "## Final remarks on SVM\n",
    "\n",
    "**Pros**\n",
    "* Good at dealing with high dimensional data\n",
    "* Works well on small data sets\n",
    "\n",
    "**Cons**\n",
    "* Picking the right kernel and parameters can be computationally intensive\n",
    "* It suffers from contamination\n",
    "\n",
    "&#9733; For further information on SVM, consult [Support Vector Machine - Classification, by Saed Sayad](http://www.saedsayad.com/support_vector_machine.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Application 1: SVM in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The binary problem\n",
    "\n",
    "In this case we examine a binary classification problem where we select one class (or more that are groups into a single oen) and the rest as contaminants. The purpose is to check if we can separate efficinetly these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data( bands2use, binary_classes2use  ):\n",
    "    \"\"\"\n",
    "    Process input data to return arrays \n",
    "    of magnitudes and (consecutive) colors\n",
    "    based on the input bands (band2use).\n",
    "\n",
    "    Option to prepare data for binary classification\n",
    "    if binary_classes2use contains classes or not.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_ml_data_mags = []   # working with magnitudes directly\n",
    "    pd_ml_data_clrs = []   # taking color terms, i.e. mag1-mag2\n",
    "    pd_ml_labels    = []\n",
    "    pd_ml_objects   = []\n",
    "\n",
    "    print(f'# stars with mags in: {\",\".join([bb for bb in bands2use])}')\n",
    "    print(\"=========================\")\n",
    "    print(\"Type    initial    final \")\n",
    "    print(\"-------------------------\")\n",
    "    init = 0 # initial total number of stars (added after each iteration)\n",
    "\n",
    "    for sptype in unique_cls:\n",
    "        indcs = classes[sptype]\n",
    "        kept = []\n",
    "        init += len(indcs)\n",
    "        for star in indcs:\n",
    "            mag_list = list(data[star][bands2use])\n",
    "            # rejecting stars with missing values\n",
    "            if miss_value in mag_list:\n",
    "                #print('REJECTING!!! <',data[star])\n",
    "                continue\n",
    "            else:\n",
    "                # creting the magnitude list\n",
    "                mag = [ i for i in mag_list ] #data[star][bands_selected] ]\n",
    "\n",
    "                # creating the color term (index)\n",
    "                clr = [mag[i]-mag[i+1] for i in range(len(mag)-1)]\n",
    "\n",
    "                pd_ml_data_clrs.append(clr)\n",
    "                pd_ml_data_mags.append(mag)\n",
    "                pd_ml_objects.append(data[star]['Name'])\n",
    "                kept.append(sptype)           \n",
    "\n",
    "                # selecting class(es) to examine for binary classifier\n",
    "                if len(binary_classes2use)!=0:\n",
    "                    if sptype in binary_classes2use:\n",
    "        #                print(f'. keeping {sptype}')\n",
    "                        label_sptype = 'SEL'\n",
    "                    else: \n",
    "        #                print(f'. not considering {sptype}')\n",
    "                        label_sptype = 'CON'\n",
    "                    pd_ml_labels.append(label_sptype)\n",
    "                else:\n",
    "                    pd_ml_labels.append(sptype)\n",
    "\n",
    "        print(f'{sptype:<4}  {len(indcs):>9} {len(kept):>8}')\n",
    "    print('-'*24)\n",
    "    print(f'TOTAL:  {init:>7}  {len(pd_ml_data_mags):>7}') \n",
    "    if len(binary_classes2use)!=0:\n",
    "        print('='*24)\n",
    "        print(f'classifying:  {len(pd_ml_labels)-pd_ml_labels.count(\"CON\"):>10}') \n",
    "        print(f'contaminants:  {pd_ml_labels.count(\"CON\"):>9}') \n",
    "\n",
    "\n",
    "    pd_ml_data_mags = np.asarray(pd_ml_data_mags)\n",
    "    pd_ml_data_clrs = np.asarray(pd_ml_data_clrs)\n",
    "    pd_ml_objects   = np.asarray(pd_ml_objects)\n",
    "    pd_ml_labels    = np.asarray(pd_ml_labels)\n",
    "          \n",
    "    return pd_ml_data_mags, pd_ml_data_clrs, pd_ml_objects, pd_ml_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select here the class(es) you would like to distinguish from the rest, along with the bands to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2keep = ['RSG']\n",
    "# Select the bands you want to use here:\n",
    "bands_selected = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected, class2keep)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:the process_data() function examines and keeps only the sources with values across all bands. In other words, we remove sources with missing values (according to the bands selected). \n",
    "\n",
    "Lets print the labels to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ml_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you have more than 2 bands selected the following plot will use the first two. Modify accordingly to plot other combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "conts = np.where( ml_labels=='CON' )[0]\n",
    "clasf = np.where( ml_labels!='CON' )[0]\n",
    "\n",
    "plt.plot( ml_data_mags[conts][:,0], ml_data_mags[conts][:,1], 'o', \n",
    "             label='Contaminants')\n",
    "plt.plot( ml_data_mags[clasf][:,0], ml_data_mags[clasf][:,1], '*', \n",
    "             label=f'Selected ({\"+\".join(class2keep)})')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel(f'{bands_selected[0]}') #'-{bands_selected[1]}')\n",
    "plt.ylabel(bands_selected[1])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing train-test split\n",
    "\n",
    "In supervised approaches we want to \"teach\" the algorithms what they need to learn, before start the predictions. In this case we want the SVM to identify the common properties of the two sub groups (SEL including all possible classes used, and CON as contaminants). However, if we provide all data the algorithm will learn this \"by heart\" and fit them perfectly (called **overfitting**), and when new data appear will probably misclassify. \n",
    "\n",
    "To address this, and to have a way to estimate the performance of the algorithms a standard **train-test split** it performed. As much data as possibly should enter the training sample, with typical values being 70-80%. Then, what is left is treated as a test sample, i.e. data that are not used to train the model. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/train-test-split.png\" width=600> \n",
    "Figure 4.1. Splitting the sample into training and test sets. <br>\n",
    "(Credit: G. Maravelias)\n",
    "    </img>\n",
    "    </div>\n",
    "\n",
    "A better approach is to split the whole sample into **train**, **validation**, and **test** sets. In this way train set defines the model's parameters, while from the validation sample we can get the *hyperparameters* (those parameters whose values determine the learning process), and finally the test sample to evaluate the performance. We will see this and more advanced techniques to estimate performance in the next sessions (ML_Practices).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ml_data_mags, ml_labels, \n",
    "                        test_size=0.3) #, random_state=42) \n",
    "\n",
    "print(f'- From {len(ml_objects)} sources:')\n",
    "print(f'   {len(X_train)} (training)')\n",
    "print(f'   {len(X_test)} (test)') \n",
    "print()\n",
    "print(f'Test labels: {y_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the classifier to fit our training set (X_train) and predict the classes of the test xamples (X_test). We will print some metrics to check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = SVC(kernel='linear') \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "#print(y_pred)\n",
    "\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation metrics\n",
    "\n",
    "There is a number of assessment metrics for the performance of a classifier. We start by introducing the idea of the **confusion matrix**. \n",
    "\n",
    "<center><img src=\"images/confusion_matrix-mod.png\" width=400> \n",
    "Figure 4.1. Confusion matrix for classification.<br>\n",
    "(Credit: <a href=\"https://towardsdatascience.com/precision-vs-recall-386cf9f89488\"  target=\"_blank\" rel=\"noopener noreferrer\">Precision vs Recall by Shruti Saxena</a>)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some metrics:\n",
    "\n",
    "$$ \\rm{Precision} =  \\frac{\\rm{True~Positives}}{\\rm{Actual~Results}} = \\frac{\\rm{True~ Positives}}{\\rm{True~Positives\\,+\\,False~Positives}} $$ \n",
    "\n",
    "$$ \\rm{Recall} = \\frac{\\rm{True~Positives}}{\\rm{Predicted~Results}} = \\frac{\\rm{True~ Positives}}{\\rm{True~Positives + False~Negatives}} $$ \n",
    "\n",
    "$$ \\rm{F1-score} = 2 \\times \\frac{\\rm{Precision}\\times\\rm{Recall}}{\\rm{Precision}+\\rm{Recall}}$$\n",
    "\n",
    "$$ \\rm{Accuracy} = \\frac{\\rm{True~Positives}\\,+\\,\\rm{True~Negatives}}{\\rm{Total}} $$ \n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "Support: number of test objects per class<br><br>\n",
    "Macro avg: averaging the unweighted mean per label<br><br>\n",
    "Weighted avg: averaging the support-weighted mean per label\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Note 1: You may also encounter the terms _sensitivity_ and _specificity_ which corresponds to the recall of the positive and the negative class, repsectively, in binary problems. \n",
    "\n",
    "Note 2: In astrophysics we use the terms _completeness_ and _contamination_ (see [Classification, by Andy Connolly](http://connolly.github.io/introAstroML/blog/classification.html)):\n",
    "\n",
    "$$ \\rm{completeness} = \\frac{\\rm{True~Positives}}{\\rm{All~real~Positives}} = \\frac{True~Positives}{True~Positives + False~Negatives} = recall$$\n",
    "\n",
    "$$ \\rm{contamination} = \\frac{False~Positives}{All~detected~Positives} = \\frac{False~Positives}{True~Positives + False~Positives}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prettier presentation of the same results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "                  \n",
    "                  \n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, alpha=0.5)\n",
    "#    plt.title(title)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('# sources', fontsize=16)\n",
    "    cbar.ax.tick_params(labelsize=16) # (fontsize=15)\n",
    "   \n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=15)\n",
    "        plt.yticks(tick_marks, target_names, fontsize=15)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.3f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\", color=\"black\", fontsize=14 )\n",
    "                     #color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\", color=\"black\") \n",
    "#                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=16)\n",
    "    plt.xlabel('Predicted label (accuracy={:0.2f})'.format(accuracy, misclass), fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix( metrics.confusion_matrix( y_test, y_pred),\n",
    "                      ['SEL','CON'],\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns precent, False raw numbers\n",
    "                      ) # YlOrBr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What happens if we start adding more classes into the selected one ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What happens when we start changing the $C$ parameter? \n",
    "\n",
    "HINT: the default value is 1, so start increasing it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multi-class problem\n",
    "\n",
    "Now we approach the same problem as a multi-class one, i.e. we are using the SVM as a classifier that can handle all classes simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_selected = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected,[])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we print the labels again, we notice that the array is totally different and the multiclass output is evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ml_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test sets for multi-class\n",
    "\n",
    "We are using again the train-test split approach but considering all classes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(ml_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(ml_data_mags, ml_labels,\n",
    "#                                shuffle=True, stratify=ml_labels, \n",
    "                                test_size=0.3 ) #, random_state=42) \n",
    "\n",
    "print(f'> From {len(ml_objects)} sources we use {len(X_train)} for training and {len(X_test)} for testing.') \n",
    "\n",
    "print('\\nStatistics per class:')\n",
    "k = 0\n",
    "for c in unique_cls:\n",
    "    items_train = np.where( y_train==c )[0]\n",
    "    items_test  = np.where( y_test==c )[0]\n",
    "    items_total = np.where( ml_labels==c )[0]\n",
    "    print(f'> For {c} there are {len(items_total)} sources split in {len(items_train)} (train) and {len(items_test)} (test) samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play time: Run train-test split a few times as it is  - do you notice anything strange? \n",
    "\n",
    "HINT: try to reduce the test_size, for the evidence to become clearer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we correct for the effect in the previous question?\n",
    "\n",
    "HINT: Check the documentation and find out which parameters help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important take-away \n",
    "\n",
    "> **sklearn documentation is your friend !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SVC(kernel='linear') \n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "#print(y_pred)\n",
    "\n",
    "#confmatrix = metrics.confusion_matrix( y_test, y_pred)\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "plot_confusion_matrix( metrics.confusion_matrix( y_test, y_pred),\n",
    "                      unique_cls,\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns precent, False raw numbers\n",
    "                      ) # YlOrBr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How does the result changes with respect to the binary case? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How does the result change with kernel ? \n",
    "\n",
    "HINT: check sklearn.svm.SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-away point\n",
    "\n",
    "> Accuracy is **not** the best metric to use when we have imbalanced datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forest\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "A **Decision Tree** (**DT**) is simply a top-to-bottom tree-like structure where each node corresponds to a question (or a set of features more generally) that distinguishes objects to two groups, left and right from the node. A decision tree presents the drawback of learning extremely well the training set. That means that DTs overfit the data and they cannot predict very accurately new data.\n",
    "\n",
    "<center><img src=\"images/DecisionTree.jpg/\" width=400> \n",
    "Figure 5.1. Quick introduction to Decition Trees - how to fulfill an everyday need.<br>\n",
    "(Credit: G. Maravelias)</center>\n",
    " \n",
    "\n",
    "##  Random Forests\n",
    "\n",
    "**Random Forests** (**RF**) or Random Decision Trees ([Breiman (2001), Machine Learning, 45, 5](https://doi.org/10.1023/A:1010933404324)) is a generalization of the DTs, as it utilises a multitude of decision trees. When RF are used as a classification method, for each input datum the final output is the class (/value) given by the mode of the classes of the individual trees.\n",
    "\n",
    "\n",
    "<center><img src=\"images/RandomForests.jpg\" width=800> \n",
    "Figure 5.2. Schematic description of the Random Forest classifier.<br>\n",
    "(Credit: G. Maravelias)</center>\n",
    "\n",
    "\n",
    "RF creates a large number of DTs through random selection of a subset of the training set as well as a random selection of features. This randomness reduces the correlation between the different DTs. Since the DTs have different conditions in their nodes and different overall structures, this diversity yield overall robust predictions. \n",
    "\n",
    "Once the RF has been trained, the data of an **unlabeled** source (to be classified) are  fed into all DTs of the forest. According to its properties and the nodes in each DT, it follows a specific path which leads to a given class. The final output of the RF (the prediction) is an aggregation of all DTs by means of a majority vote.\n",
    "\n",
    "The fact that RF combines the prediction for a number of individual trees makes it an **ensemble** method.\n",
    "\n",
    "&#9733; [Reis, Baron, & Shahaf (2019), AJ, 157, 16](https://ui.adsabs.harvard.edu/abs/2019ascl.soft03009R/abstract) provide an excellent description of RFs (for a two-class problem) and present a  probabilistic RF method which takes into account the uncertainties on the data and their labels.\n",
    "\n",
    "## Final remarks on Random Forests\n",
    "\n",
    "In machine learning most of the effort is actually spent on the **sample** selection and, most importantly, on the selection of the **features** used for the classification (feature engineering).\n",
    "\n",
    "RF partially overcome the latter problem by training each DT on a different sub-set of features, hence training the algorithm to recognize the features which mostly differentiate the objects.\n",
    "\n",
    "**PROS**\n",
    "- No need of scaling or transformation of the initial data.\n",
    "- Implicit feature selection.\n",
    "- Suitable for large datasets with many features.\n",
    "\n",
    "\n",
    "**CONS**\n",
    "- Not easily interpretable.\n",
    "- Hyperparameter needs good tuning for high accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Application 2: Random Forests in practice\n",
    "\n",
    "Using the same dataset we are not approaching the same problem using the Random Forest (both as a binary and a multiclass classifier).\n",
    "\n",
    "---\n",
    "\n",
    "**TASK 1: Complete the train-test split and select magnitudes to work with.**\n",
    "\n",
    "**TASK 2: Find the proper function for RF and make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2keep_RF=[]  # add any class if you want to use RF as binary classifier, or keep it []\n",
    "bands_selected_RF = [] #, '36um', '58um'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_mags, ml_data_clrs, ml_objects, ml_labels = process_data( bands_selected_RF, class2keep_RF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data on magnitudes or colors\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                test_size=0.3) \n",
    "\n",
    "print(f'> From {len(ml_objects)} sources we use {len(X_train)} for training and {len(X_test)} for testing.') \n",
    "\n",
    "# check if in binary or multi-label mode\n",
    "if 'CON' in ml_labels: # binary\n",
    "    confmat_classes = class2keep_RF+['CON']\n",
    "else:\n",
    "    confmat_classes = unique_cls\n",
    "\n",
    "    print('\\nStatistics per class:')\n",
    "    k = 0\n",
    "    for c in unique_cls:\n",
    "        items_train = np.where( y_train==c )[0]\n",
    "        items_test  = np.where( y_test==c )[0]\n",
    "        items_total = np.where( ml_labels==c )[0]\n",
    "        print(f'> For {c} there are {len(items_total)} sources split in {len(items_train)} (train) and {len(items_test)} (test) samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn... import ...\n",
    "\n",
    "clfrf = ...\n",
    "\n",
    "y_pred = \n",
    "\n",
    "print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "plot_confusion_matrix( metrics.confusion_matrix( y_test, y_pred),\n",
    "                      confmat_classes,\n",
    "                      title='Confusion matrix', cmap='BuPu', # for more options see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                      normalize=False  # True returns percent, False raw numbers\n",
    "                      ) # YlOrBr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:  Why the results change by re-running it ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:  What is the difference on the accuracy of the algorithm if we use the colors instead of the magnitudes? Why do it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. k-Nearest Neighbors (KNN) classification\n",
    "\n",
    "k-NN is an unsupervised method to identify clusters. Given the results from this algorithm we can then use it to perform classification. In other words, we can attribute any (new point) to the class which dominates its surroundings. \n",
    "\n",
    "The problem is then how to define the \"neighborhood\" of a point. The trivial solution would be to set a fixed radius. The issue then becomes its size: if too small, we **will not find neighbors** for \"satellite\" points at the edge of a class cluster; if too large, we **will lose resolution** in dense parts, effectively throwing away information. Therefore, ideally we would like to have a *variable bandwidth* selection threshold.\n",
    "\n",
    "> One solution is to use a local average of the labels of the $k$ nearest neighbors:\n",
    ">\n",
    "> $y = \\frac{1}{k} \\sum￼_{x_i \\in N_k(x)} y_{i}$\n",
    ">\n",
    "> where $N_k(x)$ is the neighborhood around $x_i$\n",
    "\n",
    "In this way the classification _is not_ defined based on the distance on the parameter graph, but is rather scale-independent.\n",
    "\n",
    "Let's see a 2D example. We got two parameters and training data that are classified as being *red* or *blue*. The question is how do we classify a new (_i.e. not part of the training set_) point? The following images are taken from [MNIST analysis using KNN, by Gerardo Durán  / ImportQ](https://importq.wordpress.com/2017/11/24/mnist-analysis-using-knn/) (we edited the first one).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_initial.jpg\">\n",
    "        <center>Figure 7.1.a. Training data already possessing a red or blue label, and an arbitrary new point to be classified.</center>\n",
    "    </td>    \n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh.gif\">\n",
    "        <center>Figure 7.1.b. Classification using majority votes of $k$ neighbors, for different values of $k$.</center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_mult.gif\">\n",
    "        <center>Figure 7.1.c. For a fixed $k$, the model can be thought as of a function of the location in the parameter space. Note that the appearing dots are not part of the training set. Instead, they represent the predicted classifications if the new point would fall on that position.</center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The panels a and b already suggest that the KNN classification will be affected by the choice of the **hyperparameter** $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Exercise 3: k-NN in practice\n",
    "\n",
    "In this case we are not only going to apply the algorithm but we are going to explore  the influence of $k$ hyperparameter. \n",
    "\n",
    "---\n",
    "\n",
    "**TASK 1: Complete the missing steps**\n",
    "\n",
    "**TASK 2: Find the function and the accuracy metric**\n",
    "\n",
    "**TASK 3: Perform the fitting of the algorithm for various values for k.**\n",
    "\n",
    "**TASK 4: Plot the accuracy with number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reminder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classes to keep and bands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(  \n",
    "                        stratify = ...,\n",
    "                        test_size=0.3) \n",
    "\n",
    "print(f'- From {len(ml_objects)} sources:')\n",
    "print(f'   {len(X_train)} (train)')\n",
    "print(f'   {len(X_test)} (test)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn... import ...\n",
    "\n",
    "from sklearn... import ...\n",
    "\n",
    "# PERFORM CLASSIFICATION FOR VARIOUS VALUES OF k\n",
    "\n",
    "# for each 'k', store the classifier and predictions on test sample\n",
    "classifiers = []\n",
    "predictions = []\n",
    "accuracies_train, accuracies_test  = [], []\n",
    "kvals = np.arange(1,15,1) #[1, 3, 10] # k values to be used\n",
    "\n",
    "classifiers, predictions = [], []\n",
    "\n",
    "for k in kvals:\n",
    "    \n",
    "    KNN = ...\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_pred = KNN.predict(X_test)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    classifiers.append(KNN)\n",
    "    predictions.append(y_pred)\n",
    "    accuracies_test.append( ...( y_test, y_pred))\n",
    "    accuracies_train.append( ...( y_train, KNN.predict(X_train)))\n",
    "\n",
    "    print(f\"Classification report:\\n\\n {metrics.classification_report(y_test, y_pred)}\") \n",
    "    print(f\"Confusion matrix: \\n\\n {metrics.confusion_matrix(y_test, y_pred)}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting results\n",
    "\n",
    "nn = [ clasf.n_neighbors for clasf in classifiers]\n",
    "\n",
    "plt.plot(nn, ... , label='training')\n",
    "plt.plot(nn, ... , label='test')\n",
    "\n",
    "plt.xlabel('$K$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
