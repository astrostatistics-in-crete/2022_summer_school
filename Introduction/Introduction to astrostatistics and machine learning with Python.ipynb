{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to astrostatistics and machine learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "Hogg et al. (2010) \"Data analysis recipes: Fitting a model to data\", http://adsabs.harvard.edu/abs/2010arXiv1008.4686H  \n",
    "Ivezic et al. (2014) \"Statistics, Data Mining, and Machine Learning in Astronomy\", http://www.astroml.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Fitting a line to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with a simple, straight-forward example often encountered in astronomy: how to fit a line to data. Specifically, we have a set of data $(x, y)$, and we would like to find the best $(m,b)$ such that: \n",
    "\n",
    "$$y = mx+b$$\n",
    "\n",
    "The classical approach to this problem uses a least-squares algorithm, which uses linear algebra. First, we construct our vectors and matrices:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "b \\\\\n",
    "m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ Y = \\left[ \n",
    "             \\begin{matrix} \n",
    "                y_1 \\\\\n",
    "                y_2 \\\\\n",
    "                \\vdots \\\\\n",
    "                y_N\n",
    "             \\end{matrix}\n",
    "    \\right] \n",
    "$$\n",
    "\n",
    "$$ A = \n",
    "    \\begin{bmatrix} \n",
    "        1 & x_1 \\\\\n",
    "        1 & x_2 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        1 & x_N\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ C = \n",
    "    \\begin{bmatrix} \n",
    "        \\sigma^2_{y_1} & 0 & \\dots & 0 \\\\\n",
    "        0 & \\sigma^2_{y_2} & \\dots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & \\dots & \\sigma^2_{y_N}\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using linear algebra we can construct the matrix formula:\n",
    "\n",
    "$$\n",
    "Y = A X\n",
    "$$\n",
    "\n",
    "Take a second to make sure this makes sense with the definitions of the above vectors and matrices. It should quickly become obvious that there is a problem with the above equation: the problem is over-constrained, and nowhere have we taken into account uncertainties on the measurements. This is where the linear algebra becomes helpful. We can think each row of this matrix equation (data point) as deviating from the linear expectation by some small amount $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\epsilon = Y - A X\n",
    "$$\n",
    "\n",
    "We want to weight each data point by the inverse of its uncertainty then reduce dimensionality into a scalar quantity. We call this quantity $\\chi^2$:\n",
    "\n",
    "$$\n",
    "\\chi^2 \\equiv \\left[ Y - A X \\right]^T C^{-1} \\left[ Y - A X \\right]\n",
    "$$\n",
    "\n",
    "You may have seen $\\chi^2$ expressed as the following:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i=1}^N \\frac{\\left[ y_i - f(x_i) \\right]^2}{\\sigma^2_{y,i}}\n",
    "$$\n",
    "\n",
    "For diagonal covariance matrices, these two formulations are equivalent. The linear algebra version allows for covariances between the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"...$\\chi^2$ is like a metric distance in data space.\" - Hogg et al. (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize $\\chi^2$. To get this, we first left multiply both sides of our equation $Y = A X$ by $C^{-1}$ (the inverse of the covariance matrix), which serves to weight each individual data point by its uncertainty. If you think about, this makes sense since smaller uncertainties (smaller values of $\\sigma_{y_i}$) should be weighted stronger. Our matrix equation now becomes:\n",
    "\n",
    "$$\n",
    "C^{-1} Y = C^{-1} A X\n",
    "$$\n",
    "\n",
    "We are almost there. If you look at the dimensionality of the matrices on each side, you'll see that each side is equal to a vector of length $N$. To reduce the dimensionality, we now left-multiply each side by $A^T$. You can convince yourself that this reduces each side to a vector of length 2:\n",
    "\n",
    "$$\n",
    "A^T C^{-1} Y = A^T C^{-1} A X\n",
    "$$\n",
    "\n",
    "As a last step, we want to solve for $X$, so we left-multiply each side by $\\left[ A^T C^{-1} A \\right]^{-1}$ obtaining the simple result:\n",
    "\n",
    "$$ X =\n",
    "\\left[ A^T C^{-1} A \\right]^{-1} \\left[ A^T C^{-1} Y \\right]\n",
    "$$\n",
    "\n",
    "It should be clear that, once the matrices $Y$, $A$, and $C$ are defined, this is an extremely easy matrix equation to solve with standard matrix libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainties are given by:\n",
    "    \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sigma_b^2 & \\sigma_{m,b} \\\\\n",
    "\\sigma_{m,b} & \\sigma_m^2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "A^T C^{-1} A\n",
    "\\end{bmatrix}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take some data from the literature for RR Lyrae stars in M4\n",
    "\n",
    "RR Lyrae stars are variable pulsators that obey a precise period-luminosity relation. As such they are used as a distance indicator. Here we will attempt to determine the distance to the globular cluster M4 using a sample of 37 RR Lyrae stars observed with Spitzer by Neeley et al. (2015): http://cdsads.u-strasbg.fr/abs/2015ApJ...808...11N\n",
    "\n",
    "The FU class of RR Lyrae follow the relation:\n",
    "\n",
    "$$\n",
    "m = m\\ ({\\rm log}\\ P + 0.26) + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we use astropy tables to load the data, which we obtained directly from CDS\n",
    "\n",
    "readme = './data/ReadMe'\n",
    "RR_lyrae_table2 = Table.read('./data/table2.dat', readme=readme, format='cds')\n",
    "\n",
    "# We will select just the 'FU' type RR Lyraes\n",
    "RR_lyrae = RR_lyrae_table2[np.where(RR_lyrae_table2['Mode'] == 'RRab')]\n",
    "\n",
    "# Now remove sources V20 and V21 due to blending\n",
    "RR_lyrae = RR_lyrae[np.where(RR_lyrae['ID'] != 'V20')[0]]\n",
    "RR_lyrae = RR_lyrae[np.where(RR_lyrae['ID'] != 'V21')[0]]\n",
    "print(RR_lyrae.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Axis limits\n",
    "for i in np.arange(2):\n",
    "    ax[i].set_ylim(11.3, 10.35)\n",
    "    ax[i].set_xlim(-0.15, 0.23)\n",
    "    ax[i].set_xticks([-0.1, 0.0, 0.1, 0.2])\n",
    "\n",
    "# Plot 3.6 micron data\n",
    "ax[0].errorbar(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], yerr=RR_lyrae['e_[3.6]'], fmt='.', color='k')\n",
    "# Plot 4.5 micron data\n",
    "ax[1].errorbar(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], yerr=RR_lyrae['e_[4.5]'], fmt='.', color='k')\n",
    "\n",
    "\n",
    "# Plot labels\n",
    "ax[0].set_xlabel('Log Period (days) + 0.26')\n",
    "ax[1].set_xlabel('Log Period (days) + 0.26')\n",
    "ax[0].set_ylabel('IRAC [3.6]')\n",
    "ax[1].set_ylabel('IRAC [4.5]')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's define our matrices as we did above to calculate the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_fit(data_x, data_y, data_y_err):\n",
    "    \"\"\" Calculate the best fit line values \"\"\"\n",
    "    \n",
    "    Y = np.matrix(data_y).T                         # The y-values\n",
    "    N = Y.size                                      # The number of observations\n",
    "\n",
    "    A = np.ones((N, 2))                             # Create a 2xN matrix of 1's\n",
    "    A[:,1] = data_x + 0.26                          # Set the second row to the x-values\n",
    "    A = np.matrix(A)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    C = np.identity(N) * data_y_err                 # Create the covariance matrix\n",
    "    C = np.matrix(C)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    X = inv(A.T * inv(C) * A) * (A.T * inv(C) * Y)  # Our solution vector\n",
    "    \n",
    "    return np.array(X)                              # Convert back from numpy matrix to numpy array\n",
    "\n",
    "\n",
    "def calc_uncertainties(data_x, data_y_err):\n",
    "    \"\"\" Calculate uncertainties on these values \"\"\"\n",
    "    \n",
    "    N = len(data_x)                                 # The number of observations\n",
    "    \n",
    "    A = np.ones((N, 2))                             # Create a 2xN matrix of 1's\n",
    "    A[:,1] = data_x + 0.26                          # Set the second row to the x-values\n",
    "    A = np.matrix(A)\n",
    "    \n",
    "    C = np.identity(N) * data_y_err                 # Create the covariance matrix\n",
    "    C = np.matrix(C)                                # Convert numpy array to a numpy matrix\n",
    "\n",
    "    cov = inv(A.T * inv(C) * A)                     # Calculate the uncertainties\n",
    "\n",
    "    return np.array(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(ax, b, m, mag):\n",
    "    \"\"\" To plot the best fit line \"\"\"\n",
    "    \n",
    "    x = ax.get_xlim()\n",
    "    y = m*x + b\n",
    "    \n",
    "    ax.plot(x, y, color='k')\n",
    "    \n",
    "    # Display the best fit values\n",
    "    ax.text(-0.13, 10.45, mag + '= ' + str(np.around(m[0], decimals=2)) + ' (log P + 0.26) + ' + str(np.around(b[0], decimals=2)))\n",
    "\n",
    "    \n",
    "def plot_lines(ax, b, m, cov, mag):\n",
    "    \"\"\" To plot 50 lines, randomly chosen from the best fit values and their covariances \"\"\"\n",
    "    \n",
    "    # Calculate outputs\n",
    "    mean = np.array([b[0], m[0]])\n",
    "    x_out = np.random.multivariate_normal(mean, cov, size=1000)\n",
    "    \n",
    "    x = ax.get_xlim()\n",
    "\n",
    "    # Plot lines\n",
    "    for i in np.arange(1000):\n",
    "        \n",
    "        b_tmp, m_tmp = x_out[i]\n",
    "        \n",
    "        y = np.array([m_tmp])*x + np.array([b_tmp])\n",
    "        \n",
    "        ax.plot(x, y, color='r', alpha=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Axis limits\n",
    "for i in np.arange(2):\n",
    "    ax[i].set_ylim(11.3, 10.35)\n",
    "    ax[i].set_xlim(-0.15, 0.23)\n",
    "    ax[i].set_xticks([-0.1, 0.0, 0.1, 0.2])\n",
    "\n",
    "# Plot 3.6 micron data\n",
    "b, m = calc_best_fit(RR_lyrae['logP'], RR_lyrae['[3.6]'], RR_lyrae['e_[3.6]'])\n",
    "cov = calc_uncertainties(RR_lyrae['logP'], RR_lyrae['e_[3.6]'])\n",
    "plot_lines(ax[0], b, m, cov, r'$m_{[3.6]}$')\n",
    "plot_line(ax[0], b, m, r'$m_{[3.6]}$')\n",
    "\n",
    "# Plot 4.5 micron data\n",
    "b, m = calc_best_fit(RR_lyrae['logP'], RR_lyrae['[4.5]'], RR_lyrae['e_[4.5]'])\n",
    "cov = calc_uncertainties(RR_lyrae['logP'], RR_lyrae['e_[4.5]'])\n",
    "plot_lines(ax[1], b, m, cov, r'$m_{[4.5]}$')\n",
    "plot_line(ax[1], b, m, r'$m_{[4.5]}$')\n",
    "\n",
    "# Plot points and their uncertainties\n",
    "ax[0].errorbar(RR_lyrae['logP']+0.26, RR_lyrae['[3.6]'], yerr=RR_lyrae['e_[3.6]'], fmt='.', color='k')\n",
    "ax[1].errorbar(RR_lyrae['logP']+0.26, RR_lyrae['[4.5]'], yerr=RR_lyrae['e_[4.5]'], fmt='.', color='k')\n",
    "\n",
    "\n",
    "# Plot labels\n",
    "ax[0].set_xlabel('Log Period (days) + 0.26')\n",
    "ax[1].set_xlabel('Log Period (days) + 0.26')\n",
    "ax[0].set_ylabel('IRAC [3.6]')\n",
    "ax[1].set_ylabel('IRAC [4.5]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the distance from our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will keep the slope we have just derived, but use five Galactic RR Lyrae to calibrate the zero point of the period-luminosity relation. From these, we get the following relations:\n",
    "\n",
    "$$\n",
    "M_{[3.6]} = -2.19\\ {\\rm log}\\ P - 1.176\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{[4.5]} = -2.12\\ {\\rm log}\\ P - 1.199\n",
    "$$\n",
    "\n",
    "Comparing these relations to our observed relations, we get:\n",
    "\n",
    "$$\n",
    "m_{[3.6]} = -2.19\\ ({\\rm log}\\ P + 0.26) + 10.89\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_{[4.5]} = -2.12\\ ({\\rm log}\\ P + 0.26) + 10.84\n",
    "$$\n",
    "\n",
    "Subtracting the two, we get two measurements for a distance modulus:\n",
    "\n",
    "$$\n",
    "\\mu_{[3.6]} = \\left( m_{[3.6]} - M_{[3.6]} \\right) = 11.497\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{[4.5]} = \\left( m_{[4.5]} - M_{[4.5]} \\right) = 11.488\n",
    "$$\n",
    "\n",
    "This modulus implies a distance:\n",
    "\n",
    "$$\n",
    "D = 1993\\ {\\rm pc}: \\lambda\\ =\\ 3.6\\ \\mu {\\rm m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "D = 1984\\ {\\rm pc}: \\lambda\\ =\\ 4.5\\ \\mu {\\rm m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "### This method has several benefits:\n",
    "\n",
    "1. Non-parametric\n",
    "\n",
    "2. Uses standard linear algebra techniques\n",
    "\n",
    "3. Can be abstracted to higher order polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But, this is a somewhat specialized situation. The above equation for $X^2$ has four assumptions built-in:\n",
    "\n",
    "1. Outliers were pruned by hand\n",
    "\n",
    "2. Uncertainties are Gaussian\n",
    "\n",
    "3. Uncertainties only on the y-axis\n",
    "\n",
    "4. Can't add lower or upper limits\n",
    "\n",
    "\n",
    "### When any of these four assumptions are broken, more complicated models are required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Motivation for Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's say you need to obtain a large sample of white dwarfs (WDs). You might decide that SDSS, with its $ugriz$ photometry is a good option to search from. But how do we separate WDs from other stars using this photometric data? Let's take an example from my work (Andrews et al. 2012) where I had to do exactly this. Below is a plot that demonstrates the progression of techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use wand to open pdf images\n",
    "from wand.image import Image\n",
    "\n",
    "# Figure 3 from Andrews et al. (2012)\n",
    "img = Image(filename='./images/catalog_colors_7.pdf')\n",
    "img.rotate(-90)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As they are hot objects, WD are best identified by their colors in the bluest SDSS bands, $u-g$ and $g-r$. Older criteria for identifying WDs (Richards et al. (2002) in the figure) relied on simple \"boxy\" color-color ranges. The blue stars (and black/gray contours) designate spectroscopically confirmed WDs. The white/gray contours designate contaminating, early-type main sequence stars.\n",
    "\n",
    "Clearly, the Richards et al. (2002) criteria (and any \"box\" region, really) are going to both a) miss *lots* of WDs, and b) potentially add non-WDs to our sample.\n",
    "\n",
    "It should be obvious that the Girven et al. (2011) criteria is a substantial improvement. This is a 5th order polynomial fit to the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have we solved the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the figure above, we may be tempted to think we have solved the problem - the WD locus is clearly well defined, and there is no (minimal) overlap with the early-type stellar locus. Unfortunately, the limits placed by Girven et al. (2011) are defined for WDs with $g<19$, but SDSS goes more than a magnitude fainter.\n",
    "\n",
    "\"But wait!\" you say, \"the colors of WDs should not depend on distance.\" True! But at magnitudes fainter than $g\\sim 18$, quasars become a significant source of contamination. See a figure below from a more recent paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5 from Andrews et al. (2015)\n",
    "img = Image(filename='./images/WD_colors_mag.pdf')\n",
    "img.rotate(-90)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photometrically identified WDs (from a subset that has IR photometry) are shown by the dots, while the contours show quasars. At magnitudes fainter than $g\\sim 18$, contamination from quasars is clearly substantial. Blindly using the Girven et al. (2011) criteria will lead to lots of contamination. My solution was to cut off the end of the distribution for fainter magnitudes.\n",
    "\n",
    "### Can we do better? Of course!\n",
    "\n",
    "Note: You can almost *always* do better, but at some point improvements are not worth the time required to implement. As a rule, the approach (and work required to implement) should be justified by the science goal. \n",
    "\n",
    "In data science, this is an example of a genre of problems called classification or supervised learning: We would like to \"classify\" a set of data points of unknown type using a training data set of known type. In this case, we will use catalogs of spectroscopically confirmed WDs (Kleinman et al. 2013) and quasars (Schneider et al. 2010) as our training data sets.\n",
    "\n",
    "Let's plot the WDs, quasars, and a sample of ~1.6 million objects in SDSS with $u-g<0.8$ and $g-r<0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_dir = './data/'\n",
    "\n",
    "SDSS_blue = np.load(data_dir + \"SDSS_blue_objs.npz\")['a']\n",
    "quasars = np.load(data_dir + \"schneider_quasars.npz\")['a']\n",
    "DA_WDs = np.load(data_dir + \"DA_kleinman.npz\")['a']\n",
    "DB_WDs = np.load(data_dir + \"DB_kleinman.npz\")['a']\n",
    "\n",
    "\n",
    "print(\"We have samples of spectroscopically confirmed objects:\")\n",
    "print(len(DA_WDs), \"DA white dwarfs\")\n",
    "print(len(DB_WDs), \"DB white dwarfs\")\n",
    "print(len(quasars), \"quasars\")\n",
    "print(\"\")\n",
    "print(\"We have\", len(SDSS_blue), \"blue stars in SDSS we would like to classify.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot a color-color diagram and a color-magnitude diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ax, SDSS_samp, colors):\n",
    "\n",
    "    # Plot the color-color data\n",
    "    ax[0,0].set_title('Known Sample')\n",
    "    ax[0,0].scatter(quasars['u']-quasars['g'], quasars['g']-quasars['r'], marker='.', alpha=0.01, color='r')\n",
    "    ax[0,0].scatter(DA_WDs['u']-DA_WDs['g'], DA_WDs['g']-DA_WDs['r'], marker='.', alpha=0.01, color='b')\n",
    "    ax[0,0].scatter(DB_WDs['u']-DB_WDs['g'], DB_WDs['g']-DB_WDs['r'], marker='.', alpha=0.1, color='g')\n",
    "    ax[1,0].set_title('Unknown Sample')\n",
    "    ax[1,0].scatter(SDSS_samp['u']-SDSS_samp['g'], SDSS_samp['g']-SDSS_samp['r'], marker='.', alpha=0.1, color=colors)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the color-magnitude\n",
    "    ax[0,1].set_title('Known Sample')\n",
    "    ax[0,1].scatter(quasars['g']-quasars['r'], quasars['g'], marker='.', alpha=0.01, color='r')\n",
    "    ax[0,1].scatter(DA_WDs['g']-DA_WDs['r'], DA_WDs['g'], marker='.', alpha=0.01, color='b')\n",
    "    ax[0,1].scatter(DB_WDs['g']-DB_WDs['r'], DB_WDs['g'], marker='.', alpha=0.1, color='g')\n",
    "    ax[1,1].set_title('Unknown Sample')\n",
    "    ax[1,1].scatter(SDSS_samp['g']-SDSS_samp['r'], SDSS_samp['g'], marker='.', alpha=0.1, color=colors)\n",
    "\n",
    "\n",
    "    # Add the legend\n",
    "    ax[0,0].plot(np.NaN, np.NaN, color='r', label='Quasars')\n",
    "    ax[0,0].plot(np.NaN, np.NaN, color='b', label='DA WDs')\n",
    "    ax[0,0].plot(np.NaN, np.NaN, color='g', label='DB WDs')\n",
    "    leg = ax[0,0].legend(loc=4)\n",
    "    for l in leg.get_lines():\n",
    "        l.set_alpha(1.0)\n",
    "        l.set_marker('.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in np.arange(2):\n",
    "        # Set the plot limits\n",
    "        ax[i,0].set_xlim(-0.8, 1.0)\n",
    "        ax[i,0].set_ylim(-1.0, 0.5)\n",
    "        ax[i,1].set_xlim(-0.6, 0.5)\n",
    "        ax[i,1].set_ylim(16.0, 22.0)\n",
    "\n",
    "        # Add the plot labels\n",
    "        ax[i,0].set_xlabel(r'$u-g$')\n",
    "        ax[i,0].set_ylabel(r'$g-r$')\n",
    "        ax[i,1].set_xlabel(r'$g-r$')\n",
    "        ax[i,1].set_ylabel(r'$g$')\n",
    "\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(2, 2, figsize=(9, 9))\n",
    "\n",
    "\n",
    "# We'll select 10000 random samples of the ~4 million SDSS stars\n",
    "n_samp = 10000\n",
    "idx = np.random.randint(len(SDSS_blue), size=n_samp)\n",
    "SDSS_samp = SDSS_blue[idx]\n",
    "colors = np.ones(n_samp, dtype='S11')\n",
    "colors = 'k'\n",
    "\n",
    "plot_data(ax, SDSS_samp, colors)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use an algorithm called a support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the quasar data\n",
    "n_samp = 15000\n",
    "idx = np.random.randint(len(quasars), size=n_samp)\n",
    "\n",
    "# First, we construct our training sets\n",
    "quasar_mags = quasars.view((float, len(quasars.dtype.names)))[:,2:]\n",
    "DA_WD_mags = DA_WDs.view((float, len(DA_WDs.dtype.names)))[:,2:]\n",
    "DB_WD_mags = DB_WDs.view((float, len(DB_WDs.dtype.names)))[:,2:]\n",
    "\n",
    "data = np.copy(quasar_mags[idx])\n",
    "data = np.vstack((data, DA_WD_mags))\n",
    "data = np.vstack((data, DB_WD_mags))\n",
    "\n",
    "\n",
    "# Generate an array of the class labels\n",
    "classes = 1 * np.ones(n_samp)\n",
    "classes = np.append(classes, 2 * np.ones(len(DA_WD_mags)))\n",
    "classes = np.append(classes, 3 * np.ones(len(DB_WD_mags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "clf = SVC(kernel='rbf', gamma=10.0, class_weight=None)\n",
    "# clf = LinearSVC(penalty='l2')\n",
    "clf.fit(data, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll perform a simple (incorrect) test by trying to guess the classes of known objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_out_quasars = clf.predict(quasar_mags)\n",
    "classes_out_DA_WDs = clf.predict(DA_WD_mags)\n",
    "classes_out_DB_WDs = clf.predict(DB_WD_mags)\n",
    "\n",
    "\n",
    "completeness_quasars = float(len(np.where(classes_out_quasars == 1)[0])) / float(len(quasar_mags))\n",
    "n_tot_quasars = len(np.where(classes_out_quasars == 1)[0]) + len(np.where(classes_out_DA_WDs == 1)[0]) + len(np.where(classes_out_DB_WDs == 1)[0])\n",
    "contamination_quasars = float(len(np.where(classes_out_DA_WDs == 1)[0]) + len(np.where(classes_out_DB_WDs == 1)[0])) / n_tot_quasars\n",
    "\n",
    "completeness_DA_WDs = float(len(np.where(classes_out_DA_WDs == 2)[0])) / float(len(DA_WD_mags))\n",
    "n_tot_DA_WDs = len(np.where(classes_out_quasars == 2)[0]) + len(np.where(classes_out_DA_WDs == 2)[0]) + len(np.where(classes_out_DB_WDs == 2)[0])\n",
    "contamination_DA_WDs = float(len(np.where(classes_out_quasars == 2)[0]) + len(np.where(classes_out_DB_WDs == 2)[0])) / n_tot_DA_WDs\n",
    "\n",
    "completeness_DB_WDs = float(len(np.where(classes_out_DB_WDs == 3)[0])) / float(len(DB_WD_mags))\n",
    "n_tot_DB_WDs = len(np.where(classes_out_quasars == 3)[0]) + len(np.where(classes_out_DA_WDs == 3)[0]) + len(np.where(classes_out_DB_WDs == 3)[0])\n",
    "contamination_DB_WDs = float(len(np.where(classes_out_quasars == 3)[0]) + len(np.where(classes_out_DA_WDs == 3)[0])) / n_tot_DB_WDs\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,4))\n",
    "\n",
    "ax.plot([1, 2, 3], [completeness_quasars, completeness_DA_WDs, completeness_DB_WDs], \n",
    "        marker='o', label='Completeness')\n",
    "ax.plot([1, 2, 3], [contamination_quasars, contamination_DA_WDs, contamination_DB_WDs], \n",
    "        marker='o', linestyle='--', label='Contamination')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xlim(0.9, 3.1)\n",
    "\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels([\"Quasars\", \"DA WDs\", \"DB WDs\"])\n",
    "\n",
    "ax.set_yticks(np.linspace(0,1,6))\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Completeness')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running our samples back through the SVM, we find completenesses of >95%, except for DB WDs. This is not surprising, since we are checking with the same data we trained on. Ideally, we would have kept a small subset of the training data out of the analysis, and run that data through the SVM to validate its effectiveness.\n",
    "\n",
    "### Now, we run the entire 1.6 million SDSS objects through the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a subset of the quasar data\n",
    "n_samp = 100000\n",
    "\n",
    "# Allocate array to zeros\n",
    "classes_SDSS = np.zeros(len(SDSS_blue), dtype='int64')\n",
    "\n",
    "print(\"Running sources now...\")\n",
    "\n",
    "# Calculate SVM class in batches of 10^5\n",
    "for i in np.arange(np.ceil(float(len(SDSS_blue))/float(n_samp))):\n",
    "        \n",
    "    idx = np.arange(np.min([n_samp, np.abs(int(i+1)*n_samp - len(SDSS_blue))])) + int(i)*n_samp\n",
    "    print(np.min(idx), \"sources run...\")\n",
    "    \n",
    "    SDSS_samp = SDSS_blue[idx]\n",
    "    SDSS_samp_mags = SDSS_samp.view((float, len(SDSS_samp.dtype.names)))[:,2:]\n",
    "#     SDSS_samp_mags = SDSS_samp[['u','g','r','i','z']].view(np.float64).reshape(SDSS_samp[['u','g','r','i','z']].shape + (-1,))\n",
    "    \n",
    "    classes_SDSS[idx] = clf.predict(SDSS_samp_mags)\n",
    "    \n",
    "    \n",
    "print(\"... finished classifying objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's compare the training set with the results of our classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(9, 9))\n",
    "\n",
    "\n",
    "# We'll select 10000 random samples of the ~4 million SDSS stars\n",
    "n_samp = 100000\n",
    "idx = np.random.randint(len(SDSS_blue), size=n_samp)\n",
    "SDSS_samp = SDSS_blue[idx]\n",
    "\n",
    "\n",
    "colors = np.ones(n_samp, dtype='U11')\n",
    "colors[classes_SDSS[idx] == 1] = 'r'  # Quasars\n",
    "colors[classes_SDSS[idx] == 2] = 'b'  # DA WDs\n",
    "colors[classes_SDSS[idx] == 3] = 'g'  # DB WDs\n",
    "\n",
    "\n",
    "plot_data(ax, SDSS_samp, colors)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have found\", len(np.where(classes_SDSS == 2)[0]), 'DA WDs')\n",
    "print(\"We have found\", len(np.where(classes_SDSS == 3)[0]), 'DB WDs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pre-Gaia survey to identify WDs photometrically (Fusillo et al. 2015, http://adsabs.harvard.edu/abs/2015ASPC..493..437G) presents a catalog that \"contains over 20,000 high-confidence WDs and WD candidates 11,500 of which have not yet been followed up with Sloan spectroscopy\". With a few dozen lines of code, we've found >180,000 candidates.\n",
    "\n",
    "While a *lot* more work would have to go into making this data set publishable, it should be clear that machine learning methods hold great potential for modern astrophysical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "214aebdcfc9348be8f16b1562d92eba0",
   "lastKernelId": "ff4621a5-555b-4ff7-a2e4-7c6ed936d58f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
