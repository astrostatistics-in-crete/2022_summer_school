{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3c4b8b",
   "metadata": {},
   "source": [
    "<font size=6>**Deep Learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb5de3",
   "metadata": {},
   "source": [
    "**_Deep Learning_** is a type of Machine Learning which is characterized by being **deep**.\n",
    "\n",
    "Meaning, it uses **multiple layers** to process the input information (Figure 0).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Simple_vs_Deep.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 0.  A simple <i>feedforward</i> Neural Network compared with a Deep <i>feedforward</i> Neural Network.<br>\n",
    "            (From <a href=\"https://thedatascientist.com/what-deep-learning-is-and-isnt/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The actual way the depth is designed can be very different. It could be achieved e.g. by **stacking** sequential layers (_feedforward neural networks_), via **recurrent** layers (_recurrent neural networks_), via a \"**mix**\" of these two approaches (_U-nets_), and many other ways.\n",
    "\n",
    "Don't worry: we will explain how to _computationally_ create neurons/layers [later](#Generic_Architecture_and_Neurons)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f7b2f",
   "metadata": {},
   "source": [
    "# Why Deep Learning is cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53866ee",
   "metadata": {},
   "source": [
    "It is not, we are geeks, and that's the truth.\n",
    "\n",
    "However ... we do live in the era of \"Big Data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028131ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sky Survey Project</th>\n",
       "      <th>First Light</th>\n",
       "      <th>Velocty (GB/day)</th>\n",
       "      <th>Volume (TB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2MASS</td>\n",
       "      <td>1997</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sloan Digital Sky Survey (SDSS)</td>\n",
       "      <td>2000</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large Synoptic Survey Telescope (LSST)</td>\n",
       "      <td>2023</td>\n",
       "      <td>30000</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Square Kilometer Array (SKA)</td>\n",
       "      <td>2027</td>\n",
       "      <td>150000</td>\n",
       "      <td>4600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Sky Survey Project  First Light  Velocty (GB/day)  \\\n",
       "0                                   2MASS         1997                20   \n",
       "1         Sloan Digital Sky Survey (SDSS)         2000               200   \n",
       "2  Large Synoptic Survey Telescope (LSST)         2023             30000   \n",
       "3            Square Kilometer Array (SKA)         2027            150000   \n",
       "\n",
       "   Volume (TB)  \n",
       "0           25  \n",
       "1           50  \n",
       "2       200000  \n",
       "3      4600000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_surveys = pd.DataFrame([\n",
    "    ['2MASS',                                  1997,    20, 25.4],\n",
    "    ['Sloan Digital Sky Survey (SDSS)',        2000,   200, 50],\n",
    "    ['Large Synoptic Survey Telescope (LSST)', 2023,  30e3, 200e3],\n",
    "    ['Square Kilometer Array (SKA)',           2027, 150e3, 4.6e6]\n",
    "], columns=['Sky Survey Project', 'First Light', 'Velocty (GB/day)', 'Volume (TB)']).reset_index(drop=True)\n",
    "\n",
    "df_surveys[df_surveys.columns[1:]] = df_surveys[df_surveys.columns[1:]].astype(int)\n",
    "\n",
    "display(df_surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e3b703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "    require.config({\n",
       "        paths: {\n",
       "            'chartXkcd':'https://cdn.jsdelivr.net/npm/chart.xkcd@1.1/dist/chart.xkcd.min'\n",
       "        }\n",
       "    });\n",
       "</script>\n",
       "\n",
       "<div id=\"79dd2c4895c74875b45fdd8a18fa25e9\" class=\"chart-container\" style=\"width: 500px\">\n",
       "        <svg id=\"chart_79dd2c4895c74875b45fdd8a18fa25e9\"></svg>\n",
       "    </div>\n",
       "    <script>\n",
       "        require(['chartXkcd'], function(chartXkcd) {\n",
       "            const svg_79dd2c4895c74875b45fdd8a18fa25e9 = document.querySelector('#chart_79dd2c4895c74875b45fdd8a18fa25e9')\n",
       "            const chart_79dd2c4895c74875b45fdd8a18fa25e9 = new chartXkcd.Line(svg_79dd2c4895c74875b45fdd8a18fa25e9, {\"title\": \"Survey size evolution\", \"data\": {\"datasets\": [{\"label\": \"year\", \"data\": [25, 50, 200000, 4600000]}], \"labels\": [1997, 2000, 2023, 2027]}, \"xLabel\": \"Year\", \"yLabel\": \"Volume (TB)\", \"options\": {\"yTickCount\": 3, \"legendPosition\": 1}});\n",
       "        })\n",
       "    </script>\n"
      ],
      "text/plain": [
       "<cutecharts.render.engine.HTML at 0x7f72543f43a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cutecharts.charts as ctc\n",
    "\n",
    "chart = ctc.Line(\"Survey size evolution\", width='500px')\n",
    "chart.set_options(labels=list(df_surveys['First Light']), x_label='Year', y_label='Volume (TB)')\n",
    "chart.add_series('year',list(df_surveys['Volume (TB)']))\n",
    "chart.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6c322",
   "metadata": {},
   "source": [
    "We cannot expect to humanly inspect these data and derive the intuition for the rules which categorize them.\n",
    "\n",
    "$\\rightarrow$ We have to leverage on:\n",
    "\n",
    "- the large number of examples\n",
    "\n",
    "- algorithms that can abstract arbitrarily complex rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf238b07",
   "metadata": {},
   "source": [
    "## So how does Deep Learning address big data issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfffd77",
   "metadata": {},
   "source": [
    "The basic idea is that each layer constructs **new features**.\n",
    "\n",
    "In practice, Deep Learning systems include implicit **feature engeneering** _on top_ of the learning task (e.g., classificaton or regression).<br>\n",
    "\n",
    "In this way, they are a step forward with respect to \"classic\" ML approaches (Figure 1).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Feature_Engeneering.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.  A Deep Neural Network seen as a combination of feature extractor + learner (e.g. classifier or regressor).<br>\n",
    "            (From <a href=\"https://stats.stackexchange.com/questions/562466/neural-networks-automatically-do-feature-engineering-how/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "From this perspective, the connections between the network neurons represent **potential correlations** betweeen features.\n",
    "\n",
    "<u>What are the implications?</u>\n",
    "\n",
    "The scientist **does _not_ have to get detailed insight of the problem** to build the proper features or select the proper classifier<br>\n",
    "$\\rightarrow$ the DL system does it all for us!\n",
    "\n",
    "This comes particularly handy when we deal with databases with **millions of objects** and **hundreds of features**!\n",
    "\n",
    "<u>References</u>\n",
    "\n",
    "In case you are curious, it has been proven that Deep Neural Networks are indeed \"**_universal approximators_**\"\n",
    "(e.g. [Kurt Hornik (1991), Neural Networks, 4, 2](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub)), meaning that they can in principle explain any linear or non-linear relation beteen the features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b71c64",
   "metadata": {},
   "source": [
    "## Some example applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9ba55",
   "metadata": {},
   "source": [
    "Indeed, Deep Learning (hereafter, **DL**) is being used to solve very _different_ problems, e.g.:\n",
    "\n",
    "- **Self-Driving cars**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_Self_Driving.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2a.  NVIDIA's driverless car simulator.<br>\n",
    "            (From <a href=\"https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf/\">\"End to End Learning for Self-Driving Cars\" (2016)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Protein Structure Prediction**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_AlphaFold.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2b.  Deep Mind's Alpha Fold network for the prediction of molecular structures of proteins.\n",
    "            Original paper: <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Jumper, J., Evans, R., Pritzel, A. et al. 2021,  Nature, 596, 583</a>.<br>\n",
    "            (From <a href\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Natural Language Processing, translation, and text generation**\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_NLP.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2c.  Google's unified text-to-text transformer.<br>\n",
    "            (From <a href\"https://arxiv.org/abs/1910.10683\">Raffel et al. 2021, arxiv/1910.10683</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "- **Computer Vision (lots and lots of it!)**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "    <img src=\"https://scontent.fath3-4.fna.fbcdn.net/v/t39.2365-6/10000000_3947476245325303_7673388906041049088_n.png?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=_le0vi99JGoAX_AwIJA&amp;_nc_ht=scontent.fath3-4.fna&amp;oh=00_AT-sEoik6hHpnpDMf7YSQw0iuzofrF1QJy9bvNZZ6OigoA&amp;oe=62C8BF4D\" alt=\"Detectron example\">\n",
    "        <center>\n",
    "        Figure 2d.  Facebook's Detectron2 for multiple computer vision tasks.<br>\n",
    "        (From <a href\"https://ai.facebook.com/tools/detectron2/\">Meta AI blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "... and many, many other _scary_ applications like:\n",
    "\n",
    "- **Deep Fakes**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/DL_DeepFake.jpg\">\n",
    "        <center>\n",
    "        Figure 2e.  Deep Fakes can be used to bring back actors from when the cinema was actually good (i.e., before 1999!), but also to produce false evidence.  Luckily, there are already ML efforts to uncover Deep Fakes, e.g. <a href\"https://arxiv.org/abs/2101.01456/\">Zi et al. 2021, arXiv/2101.01456\n",
    "</a>.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- **Video Games**\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"https://assets-global.website-files.com/621e749a546b7592125f38ed/62271e2f604e640534eeca99_AlphaStar%2003.gif\">\n",
    "        <center>\n",
    "        Figure 2f.  Deep Mind's Alpha Star absolutely demolishng a human player who later claimed ... erhm ... that the internet connection was bad that day because ... ehrm, mmmh ... someone in the house was watching Netflix.<br>\n",
    "        (From <a href\"https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii\">Deep Mind's blog</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "Catching up with all the new DL developments is becoming physically impossible, but you can follow great channels like [Two Minute Papers](https://www.youtube.com/c/K%C3%A1rolyZsolnai/featured) to try and stay updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff7234",
   "metadata": {},
   "source": [
    "## Deep Learning in Astronomy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71537e40",
   "metadata": {},
   "source": [
    "The application of DL in Astronomy is still at an **amatour level**, with respect to what happens in the industry (prejudice against the \"black box\"?).<br>  However ... \n",
    "\n",
    "Astronomy is the perfect ML lab because it offers:\n",
    "- tough problems to solve\n",
    "- large data\n",
    "\n",
    "In fact, Deep Learning publications are **exploding** in Astronomy (Figure 2)!\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Deep_Learning_astro_papers.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3. Number of astronomy papers containing the text \"Deep Learning\" in their abstracts.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu\">NASA ADS</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<font size=3><u>**Some notable examples**</u><font>\n",
    "\n",
    "<u>Galaxy Classification</u>\n",
    "    \n",
    "- [Dieleman et al. (2015), MNRAS, 450, 1441](https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract) $-$ calculate probabilities for the 37 Galaxy Zoo possible answers\n",
    "\n",
    "    - **training**: classification of 61,578 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: standard CNN\n",
    "\n",
    "<table><tr>\n",
    "    <td width=420>\n",
    "        <img src=\"images/Galaxy_Zoo_flowchart.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4a. Galaxy Zoo classification tree.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2013yCat..74352835W/abstract\">Willet et al. (2013)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dieleman_Fig11.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4b. Activation of the CNN layers.<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D/abstract\">Dieleman et al. (2015)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "- [Ackerman et al. 2017, MNRAS, 479, 415](https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract) $-$ identify mergers\n",
    "\n",
    "    - **training**: classification of ~4000 JPEG images from SDSS with GZ labels\n",
    "    - **architecture**: CNN with transfer learning\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Ackerman_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 5. Some galaxy pairs confidently identified as mergers.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "    \n",
    "<u>Galaxy Morphology</u>\n",
    "    \n",
    "- [Aragon-Calvo et al. 2020, MNRAS, 498, 3713](https://ui.adsabs.harvard.edu/abs/2020MNRAS.498.3713A/abstract) $-$ obtain structural parameters via self-supervised learning\n",
    "\n",
    "    - **training**: re-produce parameters used to generate artificial galaxies\n",
    "    - **architecture**: semantic autoencoder\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Aragon_Semantic_Autoencoder.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 6. Some galaxy pairs confidently identified as mergers.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A/abstract\">Ackerman et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "    \n",
    "<u>Serendipitous Detection</u>\n",
    "       \n",
    "- [Lanusse et al. 2018, MNRAS, 473, 3895](https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract) $-$ spot gravitational lenses\n",
    "\n",
    "    - **training**: 20,000 LSST-like observations\n",
    "    - **architecture**: CNN + ResNet\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/DeepLens_Fig8.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 7. Some images correctly identified as hosting lenses.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L/abstract\">Lanusse et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "- [Dekany \\& Grebel al. 2020, ApJ, 898, 46](https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract) $-$ spot fundamental-mode RR Lyrae stars \n",
    "\n",
    "    - **training**: 10$^7$$-$10$^8$ near-IR photometric time-series\n",
    "    - **architecture**: RNN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Dekani_Fig4.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 8. Spatial distribution of the objects used as training set.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2020ApJ...898...46D/abstract\">Dekany \\& Grebel al. (2020)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "    \n",
    "<u>Image reconstruction</u>\n",
    "       \n",
    "- [Schawinski et al. 2017, MNRAS, 467, 110](https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract) $-$ image denoising\n",
    "\n",
    "    - **training**: 4550 nearby SDSS galaxies\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Schawinski_Fig2.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 9. Degraded image details reconstructed by a GAN.<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S/abstract\">Schawinski et al. (2017)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n",
    "\n",
    "<u>Cosmological simulations</u>\n",
    "    \n",
    "- [Rodríguez et al. 2018, ComAC, 5, 4](https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract) $-$ create fast cosmological simulations\n",
    "\n",
    "    - **training**: 10 independent L-PICOLA simulation boxes\n",
    "    - **architecture**: GAN\n",
    "    \n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/Rodriguez_Fig1.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 10. Comparison between the results of a N-body simulation ($top$) and from a GAN ($bottom$).<br>\n",
    "            (From <a herf=\"https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R/abstract\">Rodríguez et al. (2018)</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c911c5",
   "metadata": {},
   "source": [
    "# Neural Networks (NN) Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a8af2",
   "metadata": {},
   "source": [
    "## Generic Architecture and Neurons\n",
    "<a id='Generic_Architecture_and_Neurons'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32a5d7",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Generic_Architecture.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 11.  A simple, generic <i>feedforward</i> deep neural architecture.  Neurons of a layers might be connected to al the neurons of the neighboring layers, like in this example (<i>fully-connected</i> layers), or not.<br>\n",
    "            (Adapted from <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff85da",
   "metadata": {},
   "source": [
    "<font size=3><u>**Nomenclature**</u><font>\n",
    "    \n",
    "**Neuron**: A simple element in a network, carrying 1 value.\n",
    "    \n",
    "**Layers**: A collection of neurons activated simulataneusly.<br>\n",
    "    \n",
    "    Layers are represented diffrently depending on the architecture.\n",
    "    E.g., fully-connected layers (as in the Figure above), appear as vertical stripes of neurons.\n",
    "  \n",
    "- **input layer**: the data\n",
    "- **hidden layers**: the internal layers (\"_hidden_\" from the point of view of the NN user)\n",
    "- **output layer**: the variable(s) of interest (e.g., class(es) or $y$)\n",
    "    \n",
    "    \n",
    "    E.g., if we provide an image as input, each pixel is 1 neuron of the input layer.\n",
    "\n",
    "\n",
    "Contemporary NNs contain hundreds to thousands of layers, with million to billion of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56018388",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925069b",
   "metadata": {},
   "source": [
    "The core of the functioning of any NN is how the **information flows** through a neuron.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Weights_and_Biases.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 12.  How the information is propagated through a neuron. Don't get confused with $\\hat{y}$: in this image, it only represents the neuron's output, not the target variabe (e.g. the <i>class</i>)<br>\n",
    "            (From <a href=\"https://ui.adsabs.harvard.edu\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "1. **The first stage is <u>linear</u>:**<br>\n",
    "    A neuron takes all the inputs (values) x$_i$ directed into it, multiplies each of them by a different _weight_ ($w_i$), and takes the sum.<br>\n",
    "    Then, it adds a _bias_ ($b$).\n",
    "<br>\n",
    "\n",
    "2. **The second stage is (usually) <u>non-linear</u>:**<br>\n",
    "    The summation is passed to an **activation function**.<br>\n",
    "    The activation function acts as a filter, basically deciding when and how the information shall flow. \n",
    "    \n",
    "<u>**Important**</u>\n",
    "\n",
    "The _weights_ and _biases_ are <u>the</u> elements that are fit during the training of the model! \n",
    "\n",
    "Fitting a model means optimizing **all** the _weights_ and _biases_ within the NN, in order to **approximate** the desired output $y$ given a corresponding example $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b954f9",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3046c55",
   "metadata": {},
   "source": [
    "Activation functions are what make NNs so **efficient** as universal tools.\n",
    "\n",
    "The introduce <u>non-linearities</u> $\\rightarrow$ a NN can create an arbitrarily complex model.\n",
    "\n",
    "They can be basically **any** _filter_-like function, but they better posses some features:\n",
    "\n",
    "- **computationally inexpensive** $\\leftarrow$ hence simple, since they get executed at each neuron\n",
    "\n",
    "- **zero-centered** $\\leftarrow$ not to shift values towards a preferential direction\n",
    "\n",
    "- **differentiable** $\\leftarrow$ because NNs work with [Backpropagation](#Backpropagation)\n",
    "\n",
    "- **avoid vanishing when chained** $\\leftarrow$ more correctly, we need to avoid vanishing gradients (see [Gradient Descent and Loss](#Gradient-Descent-and-Loss))\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=480>\n",
    "        <img src=\"images/Activation_Functions.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 13.  A collection of commonly used activation functions<br>\n",
    "            (Adapted from <a href=\"https://wandb.ai/lavanyashukla/vega-plots/reports/Natural-Language-Processing--Vmlldzo2Nzk2Ng\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13763d",
   "metadata": {},
   "source": [
    "## NN Architecture Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076be32e",
   "metadata": {},
   "source": [
    "# Training NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e2700",
   "metadata": {},
   "source": [
    "## Gradient Descent and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df557462",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- and optimization algos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cbdbb",
   "metadata": {},
   "source": [
    "## Validation curves\n",
    "\n",
    "- overfitting/underfitting\n",
    "\n",
    "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc6b08",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589c623",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "\n",
    "- architecture\n",
    "\n",
    "- conv layers\n",
    "\n",
    "- activation \n",
    "\n",
    "- pooling\n",
    "\n",
    "- fully connected layers\n",
    "\n",
    "- Dropout\n",
    "\n",
    "- BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413d815",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eb264",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc81d5",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955efcdd",
   "metadata": {},
   "source": [
    "## GANs\n",
    "\n",
    "- For deconvolution: Schawinski, Kevin, et al. 2017, \"Generative Adversarial Networks recover features in astrophysical images of\n",
    "galaxies beyond the deconvolution limit.\" arXiv preprint arXiv:1702.00403 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3660889",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e36be",
   "metadata": {},
   "source": [
    "\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3886c2",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "- Sequential API\n",
    "\n",
    "- Functional API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
