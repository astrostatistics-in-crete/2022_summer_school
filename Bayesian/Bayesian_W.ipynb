{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmological Parameters\n",
    "\n",
    "There are many constraints that can be placed on cosmological parameters using various observations. Type Ia supernova and the power spectrum of the cosmological microwave background are two examples. In this exercise we will use a different example: galaxy cluster counts.\n",
    "\n",
    "The idea hypothesis under which we operate is the following: \n",
    "\n",
    "Galaxy cluster counts as a function of halo mass = Halo mass function.\n",
    "\n",
    "$$\n",
    "\\frac{dn(M_{\\rm cluster})}{dM_{\\rm cluster}} = \\frac{dn(M_{\\rm halo})}{dM_{\\rm halo}}\n",
    "$$\n",
    "\n",
    "\n",
    "First, we will load a sample of galaxy cluster masses from Vikhlinin et al. (2009) which can be used to infer cosmological parameters. Cluster masses have been determined in the redshift range $0.025 < z \\leq 0.25$.\n",
    "\n",
    "The goal of this exercise is to model the priors for cosmological parameters $H_0, \\, \\sigma_8$ and $\\Omega_{\\rm M}$ and the likelihood that given a model for the mass function (Tinker et al. 2008), we will observe the cluster counts of Vikhlinin et al. (2009).\n",
    "\n",
    "The likelihood and priors can be used to infer the posterior distribution of the cosmological parameters, given the observational data.\n",
    "\n",
    "Let us start with the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data\n",
    "data_low_z = np.genfromtxt(\"data/redshift_data_updated.txt\")\n",
    "\n",
    "# Extract the useful values from the data\n",
    "mass_low_z = data_low_z[0]\n",
    "mass_low_z_err = data_low_z[1]\n",
    "z_low = data_low_z[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells implement the model of Tinker et al. (2008) for the halo mass function, which depends on multiple cosmological parameters:\n",
    "\n",
    "$$\n",
    "\\frac{dn}{d\\log M} = f(\\sigma) \\frac{\\Omega_{\\rm M}\\rho_{\\rm c}}{M}\\frac{d\\log\\sigma^{-1}}{d\\log M},\n",
    "$$\n",
    "\n",
    "where $\\rho_{\\rm c} = 3H_0^2/(8\\pi G)$ is the critical density of the Universe, $\\sigma=\\sigma(M)$ is the RMS value of cosmological density fluctuations of mass $M$, and $f(\\sigma)$ is a function defined below and provided by Tinker et al. (2008). The model is closed by the relation of Jenkins et al. (2001):\n",
    "\n",
    "$$\n",
    "\\sigma(M)\\approx\\sigma_{8}\\left(\\frac{M}{M_8}\\right)^{-\\frac{n_{\\rm eff}-3}{6}},\n",
    "$$\n",
    "\n",
    "where $M_8$ is the average mass enclosed within 8 Mpc, and $n_{\\rm eff}\\approx-1.0$ for the standard $\\Lambda{\\rm CDM}$ cosmological model.\n",
    "\n",
    "\n",
    "### Let us start implementing the model with a few useful variables and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mpc_in_cm = 3.086e24\n",
    "Mpc_in_km = 3.086e19\n",
    "Msun_in_g = 1.989e33\n",
    "\n",
    "# convert G from cgs (cm^3/g/s^2) to Mpc^3/Msun/s^2\n",
    "G_grav = 6.674e-8 * Msun_in_g / Mpc_in_cm**3\n",
    "\n",
    "# Speed of light in cgs\n",
    "c_light = 3.0e10 \n",
    "\n",
    "\n",
    "# Cosmological parameters\n",
    "mass_function_amplitude = 0.26\n",
    "Delta = 500.0\n",
    "n_eff = -1.0\n",
    "R_8 = 8.0\n",
    "\n",
    "\n",
    "# Trial value for H_0\n",
    "# # H_0 is in units of km/s/Mpc\n",
    "H_0 = 67.74\n",
    "h = H_0/100.0\n",
    "\n",
    "# survey area is 400 degrees^2\n",
    "survey_volume = 400.0\n",
    "z_min_low_z, z_max_low_z = 0.025, 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to load data accounting for the selection function of the observations\n",
    "effective_volume = np.genfromtxt(\"data/effective_volume.csv\", delimiter=',')\n",
    "\n",
    "effective_volume = interp1d(effective_volume[:,0]/h, effective_volume[:,1]/h**3, \n",
    "                            bounds_error=False, fill_value=9.28562539e+08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosmological Functions\n",
    "\n",
    "Let us now implement the rest of the model. These functions contain all the cosmological parameterizations to model the galaxy cluster count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_a(Delta):\n",
    "    return 1.43 + (np.log10(Delta) - 2.3)**1.5\n",
    "def calc_b(Delta):\n",
    "    return 1.0 + (np.log10(Delta) - 1.6)**(-1.5)\n",
    "def calc_c(Delta):\n",
    "    return 1.2 + (np.log10(Delta) - 2.35)**1.6\n",
    "    \n",
    "def calc_M_8(H_0):\n",
    "    return 3.0*(H_0/Mpc_in_km)**2 / (8.0 * np.pi * G_grav) * (4.0*np.pi/3.0) * R_8**3\n",
    "\n",
    "def calc_sigma(mass, H_0, sigma_8):\n",
    "    \n",
    "    M_8 = calc_M_8(H_0)\n",
    "    sigma = sigma_8 * (mass/M_8)**((-3.0-n_eff)/6.0)\n",
    "    \n",
    "    return sigma\n",
    "    \n",
    "    \n",
    "def calc_f_sigma(sigma):\n",
    "    \n",
    "    a = calc_a(Delta)\n",
    "    b = calc_b(Delta)\n",
    "    c = calc_c(Delta)\n",
    "        \n",
    "    f_sigma = np.exp(-c / sigma**2)\n",
    "    f_sigma *= (b/sigma)**a + 1.0\n",
    "    f_sigma *= mass_function_amplitude\n",
    "\n",
    "    return f_sigma\n",
    "\n",
    "\n",
    "def calc_dn_dlnM(mass, H_0, sigma_8, Omega_M):\n",
    "    \n",
    "    sigma = calc_sigma(mass, H_0, sigma_8)\n",
    "    f_sigma = calc_f_sigma(sigma)\n",
    "    \n",
    "    critical_density = 3.0 * (H_0/Mpc_in_km)**2 / (8.0*np.pi*G_grav)\n",
    "    \n",
    "    dn_dlnM = f_sigma * Omega_M/mass * critical_density * (n_eff+3.0)/6.0\n",
    "    \n",
    "    return dn_dlnM\n",
    "    \n",
    "\n",
    "def calc_survey_volume_low_z(H_0):\n",
    "    \n",
    "    volume_low_z_high = 1.0/3.0*(c_light/Mpc_in_cm)**3 / (H_0/Mpc_in_km)**3 * z_max_low_z**3 * survey_volume * (4.0*np.pi/41253.0)\n",
    "    volume_low_z_low = 1.0/3.0*(c_light/Mpc_in_cm)**3 / (H_0/Mpc_in_km)**3 * z_min_low_z**3 * survey_volume * (4.0*np.pi/41253.0)\n",
    "    volume_low_z = volume_low_z_high - volume_low_z_low\n",
    "    \n",
    "    return volume_low_z\n",
    "\n",
    "\n",
    "def calc_dN_dlnM(mass, H_0, sigma_8, Omega_M):\n",
    "    \n",
    "    dn_dlnM = calc_dn_dlnM(mass, H_0, sigma_8, Omega_M)\n",
    "    dN_dlnM = dn_dlnM * calc_survey_volume_low_z(H_0)\n",
    "    \n",
    "    return dN_dlnM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set the cosmological parameters to some fixed values, then convert the analytical halo mass function to cumulative number counts. Cumulative number counts are given by the following formula (accurate only at low redshift):   \n",
    "\n",
    "$$\n",
    "dN(>M) = V\\int_M^{+\\infty} \\frac{dn}{d\\log M}d\\log M,\n",
    "$$\n",
    "\n",
    "where $V$ is the volume where the cluster/halo counts are performed. \n",
    "\n",
    "Let us start by computing the cumulative number counts from the theoretical model, using trial values of the cosmological parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial values\n",
    "H_0_trial = 67.74\n",
    "H_0 = H_0_trial\n",
    "h = H_0/100\n",
    "sigma_8_trial = 0.8159\n",
    "Omega_M_trial = 0.3089\n",
    "sigma_8 = sigma_8_trial\n",
    "Omega_M = Omega_M_trial\n",
    "\n",
    "\n",
    "tmp_M = np.logspace(16, 10, 100)\n",
    "bin_width = np.log10(tmp_M[0])-np.log10(tmp_M[1])\n",
    "\n",
    "dN_dlnM = np.zeros(len(tmp_M))\n",
    "\n",
    "for i, mass in enumerate(tmp_M):\n",
    "    dN_dlnM[i] = calc_dN_dlnM(mass, H_0, sigma_8, Omega_M)\n",
    "    \n",
    "dN_dlnM = np.cumsum(dN_dlnM * bin_width / calc_survey_volume_low_z(H_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two cells below, we also compute cumulative cluster counts from the observational data of Vikhlinin et al., then plot the theoretical model against the data. \n",
    "\n",
    "For the data we generate Poisson error bars, that represent the statistical uncertainty in measurements of the number of clusters in each mass bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data\n",
    "masses = -np.sort(-mass_low_z)\n",
    "y_val = np.cumsum(1.0/effective_volume(masses*1.0e14))\n",
    "y_err = np.sqrt(y_val/calc_survey_volume_low_z(H_0)) # Poisson error bars\n",
    "\n",
    "plt.scatter(masses*1.0e14*h, y_val/h**3)\n",
    "plt.errorbar(masses*1.0e14*h, y_val/h**3, y_err/h**3, fmt='o')\n",
    "\n",
    "\n",
    "# Theoretical model\n",
    "plt.plot(tmp_M*h, dN_dlnM, color='k')\n",
    "\n",
    "\n",
    "\n",
    "# Make plot pretty\n",
    "plt.gca().xaxis.grid(color='gray', linestyle='dashed')\n",
    "plt.gca().yaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(r'Mass $h^{-1}$ (M$_{\\odot}$)')\n",
    "plt.ylabel(r'N(>M) $h^{-3}$ Mpc$^{-3}$')\n",
    "\n",
    "plt.xlim(5.0e13, 2.0e15)\n",
    "plt.ylim(1.0e-10, 1.0e-4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Calculate the x-, y-data\n",
    "masses = -np.sort(-mass_low_z)\n",
    "data_x = masses*1.0e14*h\n",
    "data_y = np.cumsum(1.0/effective_volume(masses*1.0e14)) / h**3\n",
    "data_y_err = np.sqrt(y_val/calc_survey_volume_low_z(H_0))/h**3 # Poisson error bars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_ln_likelihood(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err):\n",
    "    \n",
    "    # Calculate over a mass grid\n",
    "    tmp_M = np.logspace(16, 10, 100)\n",
    "    bin_width = np.log10(tmp_M[0])-np.log10(tmp_M[1])\n",
    "\n",
    "    # Calculate over the mass grid\n",
    "    dN_dlnM_model = np.zeros(len(tmp_M))\n",
    "    for i, mass in enumerate(tmp_M):\n",
    "        dN_dlnM_model[i] = calc_dN_dlnM(mass, H_0, sigma_8, Omega_M)\n",
    "\n",
    "    model_y = np.cumsum(dN_dlnM_model * bin_width / calc_survey_volume_low_z(H_0))\n",
    "    \n",
    "    # Chi2 likelihood\n",
    "    ln_likelihood = 0.0\n",
    "    for j in range(len(data_x)):\n",
    "        idx = np.argmin(np.abs(data_x[j]-tmp_M*h))\n",
    "        ln_likelihood += -(model_y[idx] - data_y[j])**2 / (2.0*data_y_err[j]**2)\n",
    "        \n",
    "            \n",
    "    return ln_likelihood\n",
    "\n",
    "\n",
    "def calc_neg_ln_likelihood(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err):\n",
    "    return -calc_ln_likelihood(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "masses = -np.sort(-mass_low_z)\n",
    "y_val = np.cumsum(1.0/effective_volume(masses*1.0e14))\n",
    "y_err = np.sqrt(y_val/calc_survey_volume_low_z(H_0))/h**3 # Poisson error bars\n",
    "\n",
    "\n",
    "# Observed model\n",
    "plt.scatter(data_x, data_y)\n",
    "plt.errorbar(data_x, data_y, y_err, fmt='o', label='Data')\n",
    "\n",
    "# Theoretical model\n",
    "plt.plot(tmp_M*h, dN_dlnM, color='k', label='Trial Model')\n",
    "\n",
    "\n",
    "\n",
    "# Run a minimization algorithm\n",
    "sol = minimize(calc_neg_ln_likelihood, 0.6, \n",
    "               args=(Omega_M, H_0, data_x, data_y, data_y_err), \n",
    "               tol=1.0e-10)\n",
    "\n",
    "# Select the minimization solution\n",
    "sigma_8_best = sol.x\n",
    "\n",
    "\n",
    "\n",
    "tmp_M = np.logspace(16, 10, 100)\n",
    "bin_width = np.log10(tmp_M[0])-np.log10(tmp_M[1])\n",
    "\n",
    "# Model Definition\n",
    "dN_dlnM_model = np.zeros(len(tmp_M))\n",
    "\n",
    "for i, mass in enumerate(tmp_M):\n",
    "    dN_dlnM_model[i] = calc_dN_dlnM(mass, H_0, sigma_8_best, Omega_M)\n",
    "    \n",
    "dN_dlnM_model = np.cumsum(dN_dlnM_model * bin_width / calc_survey_volume_low_z(H_0))    \n",
    "\n",
    "# Theoretical model\n",
    "plt.plot(tmp_M*h, dN_dlnM_model, color='C1', label='Best-fit')\n",
    "\n",
    "\n",
    "# Add the legend\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Make the plot pretty\n",
    "plt.gca().xaxis.grid(color='gray', linestyle='dashed')\n",
    "plt.gca().yaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlim(5.0e13, 2.0e15)\n",
    "plt.ylim(1.0e-10, 1.0e-4)\n",
    "\n",
    "plt.xlabel(r'Mass (M$_{\\odot}$) $h^{-1}$')\n",
    "plt.ylabel(r'$N(>M)$ Mpc$^{-3}$ $h^{3}$')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the constraints in our parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_0 = H_0_trial\n",
    "\n",
    "# Create our grid\n",
    "N_side = 40\n",
    "sigma_8_set = np.linspace(0.5, 1.0, N_side)\n",
    "Omega_M_set = np.linspace(0.1, 0.5, N_side)\n",
    "XX, YY = np.meshgrid(sigma_8_set, Omega_M_set)\n",
    "\n",
    "# Run the likelihood calculation\n",
    "ln_likelihood = np.zeros((len(sigma_8_set), len(Omega_M_set)))\n",
    "for i, sigma_8 in enumerate(sigma_8_set):\n",
    "    for j, Omega_M in enumerate(Omega_M_set):        \n",
    "        ln_likelihood[j,i] = calc_ln_likelihood(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err)\n",
    "\n",
    "\n",
    "# Plot the likelihood\n",
    "plt.pcolor(XX, YY, ln_likelihood,\n",
    "           norm=colors.Normalize(vmin=np.max(ln_likelihood)-20.0, vmax=np.max(ln_likelihood)),\n",
    "           cmap='viridis')    \n",
    "\n",
    "# Plot the trial value\n",
    "plt.scatter(sigma_8_trial, Omega_M_trial, color='r')\n",
    "\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(r'$\\sigma_8$')\n",
    "plt.ylabel(r'$\\Omega_m$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there is a degeneracy in the $\\sigma_8$-$\\Omega_m$ plane. Copy the code block above into the empty block below, and replace $\\Omega_m$ with $H_0$ to show any degeneracy between $\\sigma_8$ and $H_0$. Remember to reset the value of $\\Omega_m$ to the trial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Test different cosmological parameters\n",
    "\n",
    "It's really annoying to have to plot distributions like this in three dimensions. To get a feel for how things change with cosmological parameters, vary $\\Omega_M$ and $H_0$ in the code blocks above. What happens when you increase $H_0$ to values above 75 km/s/Mpc? When you decrease to values below 65 km/s/Mpc? What happens when you increase $\\Omega_M$ to values above 0.4? To values below 0.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding priors\n",
    "\n",
    "Our test above has provided us with a likelihood on a particular set of cosmological parameters ($H_0$, $\\Omega_M$, $\\sigma_8$). Let's remember our likelihood function. We've already calculated this.\n",
    "$$ P({\\rm data} | H_0, \\Omega_M, \\sigma_8)$$ \n",
    "\n",
    "\n",
    "\n",
    "But what we really want to know is the posterior distribution on the model parameters, $P(H_0, \\Omega_M, \\sigma_8 | {\\rm data})$. So we use Bayes's theorem:\n",
    "\n",
    "$$ P(H_0, \\Omega_M, \\sigma_8 | {\\rm data}) \\sim P({\\rm data} | H_0, \\Omega_M, \\sigma_8) P(H_0, \\Omega_M, \\sigma_8) $$\n",
    "\n",
    "How do we deal with our prior: $P(H_0, \\Omega_M, \\sigma_8)$? One way is to just assume flat priors. Without any additional information, this is often (but not always!) a reasonable assumption. For flat priors, $P(H_0, \\Omega_M, \\sigma_8) = {\\rm constant}$, in which case the posterior distribution mimics the likelihood distribution above.\n",
    "\n",
    "Early cosmologists did not believe in dark energy, and thought that $\\Omega_M = 1$. To mimic this, add a power law prior on $\\Omega_M$:\n",
    "$$ P(\\Omega_M) = \\Omega_M^{\\alpha}, $$\n",
    "\n",
    "where $\\alpha$ is the power law exponent. Note that this prior is unnormalized. However in this case, we can ignore constants that can be divided out. Fill in the prior function below with a value of $\\alpha$ and show the joint constraints on $\\Omega_M$ and $\\sigma_8$ as done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ln_posterior(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err):\n",
    "    \n",
    "    ln_prior = calc_ln_prior(sigma_8, Omega_M, H_0)\n",
    "    ln_likelihood = calc_ln_likelihood(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err)\n",
    "    \n",
    "    ln_posterior = ln_prior + ln_likelihood\n",
    "            \n",
    "    return ln_posterior\n",
    "\n",
    "\n",
    "\n",
    "def calc_ln_prior(sigma_8, Omega_M, H_0):\n",
    "    \n",
    "    # Remember this is the log prior, not the prior. Check your math!\n",
    "    ln_prior = \n",
    "    \n",
    "    return ln_prior\n",
    "\n",
    "\n",
    "\n",
    "H_0 = H_0_trial\n",
    "\n",
    "N_side = 40\n",
    "sigma_8_set = np.linspace(0.5, 1.0, N_side)\n",
    "Omega_M_set = np.linspace(0.1, 0.5, N_side)\n",
    "XX, YY = np.meshgrid(sigma_8_set, Omega_M_set)\n",
    "\n",
    "ln_posterior = np.zeros((len(sigma_8_set), len(Omega_M_set)))\n",
    "\n",
    "for i, sigma_8 in enumerate(sigma_8_set):\n",
    "    for j, Omega_M in enumerate(Omega_M_set):        \n",
    "        ln_posterior[j,i] = calc_ln_posterior(sigma_8, Omega_M, H_0, data_x, data_y, data_y_err)\n",
    "\n",
    "        \n",
    "plt.pcolor(XX, YY, ln_posterior,\n",
    "           norm=colors.Normalize(vmin=np.max(ln_posterior)-20.0, vmax=np.max(ln_posterior)),\n",
    "           cmap='viridis')    \n",
    "\n",
    "plt.scatter(sigma_8_trial, Omega_M_trial, color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(r'$\\sigma_8$')\n",
    "plt.ylabel(r'$\\Omega_m$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: How large does $\\alpha$ have to be before the effect from the prior distribution on $\\Omega_M$ is noticeable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realistic Prior\n",
    "\n",
    "But this is cosmology, and we have lots of constraints from extra information from other observables. Let's include those as priors on $H_0$, $\\Omega_M$, and $\\sigma_8$. We can start by adding data from lensing, as measured by _Planck_.\n",
    "\n",
    "$$ \\sigma_8 \\Omega_m^{0.25} = 0.589\\pm0.020 $$\n",
    "\n",
    "Code this prior up in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ln_prior(sigma_8, Omega_M, H_0):\n",
    "\n",
    "    # Lensing data\n",
    "    sigma_8_omega_M_25 = 0.589\n",
    "    sigma_8_omega_M_25_err = 0.020\n",
    "    \n",
    "    ln_prior = \n",
    "    \n",
    "    return ln_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, in the code blocks below, use a different prior which employs the full constraints from _Planck_.\n",
    "\n",
    "$$ H_0 = 67.9\\pm1.2\\ {\\rm km}\\ {\\rm s}^{-1}\\ {\\rm Mpc} $$\n",
    "$$ \\sigma_8 = 0.811 \\pm 0.019 $$\n",
    "$$ \\Omega_m = 0.303 \\pm 0.017 $$\n",
    "\n",
    "Remake the joint plots shown above using the updated Planck priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ln_prior(sigma_8, Omega_M, H_0):\n",
    "\n",
    "    planck_H_0 = 67.9\n",
    "    planck_H_0_err = 1.2\n",
    "    planck_sigma_8 = 0.811\n",
    "    planck_sigma_8_err = 0.019\n",
    "    planck_Omega_M = 0.303\n",
    "    planck_Omega_M_err = 0.017\n",
    "    \n",
    "    ln_prior = \n",
    "    \n",
    "    return ln_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit: Use `scipy.optimize.minimize` to find the maximum a posteriori point in 3D parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss with your partner how one might go about combining separate data sets to produce *joint* constraints on cosmological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(data = 'images/Planck.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
