{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "\n",
    "## Motivation - distances to stars\n",
    "\n",
    "Let's start by imagining that we have a method to measure the distance to a star. As with all observations, there is some uncertainty associated with that measurement. So let's imagine that our particular star is at a distance of 100 pc, and our measurement method allows us a precision of 20 pc. If we were to measure the same star 10,000 separate times, we would find that half the time the star is _observed_ to be nearer than 100 pc, and the other half it is _observed_ to be farther than 100 pc. In fact, the distance distribution will form (by definition) a Gaussian, centered on 100 pc, with a standard deviation of 20 pc.\n",
    "\n",
    "Now, instead of a single star, let's imagine that we have a population of 10,000 stars spread out evenly within the nearest 100 pc. Do we still find that half of all stars are closer than they appear and half are further than they appear? Let's investigate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's imagine that we observe the parallaxes to 1000 stars, with an angular precision of 2 mas (10$^3$ mas = 1 arcsecond). Let's further assume the stars are evenly distributed within the nearest 100 pc. First, we'll generate the actual distances to those stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "N_stars = 10000\n",
    "\n",
    "# Generate random distances within nearest 100 pc\n",
    "distance = 100.0*np.random.rand(N_stars)**(1.0/3.0)\n",
    "\n",
    "# Plot histogram of distances\n",
    "plt.hist(distance, bins=20, normed=True, label='Distance Distribution')\n",
    "\n",
    "# Plot expectation from a constant stellar density sphere\n",
    "tmp_x = np.linspace(0, 100, 100)\n",
    "plt.plot(tmp_x, (tmp_x)**2*3.0/100.0**3, label='Expectation')\n",
    "\n",
    "\n",
    "# Add plot legend\n",
    "plt.legend()\n",
    "\n",
    "# Make plot pretty\n",
    "plt.gca().set_yticklabels([])\n",
    "\n",
    "plt.xlabel('Distance (pc)')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In two minutes, discuss with your partner why the distribution rises as r$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is hidden here\n",
    "\n",
    "[//]: # (The volume of an infinitely thin shell is r^2 dr, As r increases, the volume scales with r^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's add measurement uncertainties to our observations\n",
    "\n",
    "Practically, this amounts to just adding to the actual distance a random number drawn from a Gaussian distribution centered on zero, with a standard deviation of 20 pc. We show the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "bins = np.linspace(0, 125, 20)\n",
    "\n",
    "# Generate mock observations\n",
    "observed_distance = distance + np.random.normal(0, 20.0, size=len(distance))\n",
    "\n",
    "# Plot histogram of distances and observed distances\n",
    "plt.hist(distance, bins=bins, normed=True, label='Actual Distances')\n",
    "plt.hist(observed_distance, bins=bins, normed=True, label='Observed Distances', histtype='step', linewidth=2)\n",
    "\n",
    "# Plot expectation from a constant stellar density sphere\n",
    "tmp_x = np.linspace(0, 100, 100)\n",
    "plt.plot(tmp_x, (tmp_x)**2*3.0/100.0**3, label='Expected Distances', linewidth=2)\n",
    "\n",
    "\n",
    "# Add plot legend\n",
    "plt.legend()\n",
    "\n",
    "# Make plot pretty\n",
    "plt.gca().set_yticklabels([])\n",
    "\n",
    "plt.xlabel('Distance (pc)')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the observed parallaxes back to distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious difference here is that some distances appear significantly larger than they actually are. But there's a second, more insidious difference between the two distributions. \n",
    "\n",
    "Let's just focus on stars with _apparent_ distances between 70 and 80 pc. Within this distance, I want to compare the number of stars that have apparent distances smaller than their true distances with the number that have larger apparent distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by finding the indices of the array values within 5 pc of 75 pc\n",
    "idx = np.where(np.abs(observed_distance-75.0) < 5.0)[0]\n",
    "\n",
    "num_lower = len(np.where(observed_distance[idx] < distance[idx])[0])\n",
    "num_higher = len(np.where(observed_distance[idx] > distance[idx])[0])\n",
    "\n",
    "print(num_lower, \"stars are further than they appear.\")\n",
    "print(num_higher, \"stars are closer than they appear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most stars have actual distances larger than the apparent distance, by a large majority. This is very strange. Why is it that when I previously measured my star 10,000 times. I found that half the time it appeared closer than its true distance, while when I measure 10,000 separate stars only about a third of the time does the star appear closer than its true distance?\n",
    "\n",
    "You can rerun the above blocks several times to convince yourself that this is not just stochastic, random noise. Rather, this is a bias: the actual distance is systematically larger than the observed distance. This bias is called the Lutz-Kelker bias, after the two authors of the paper that first described this bias (https://ui.adsabs.harvard.edu/abs/1973PASP...85..573L/abstract).\n",
    "\n",
    "\n",
    "\n",
    "### With your partner discuss the possible origin of this bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is hidden here\n",
    "\n",
    "[//]: # (This bias arises from the fact that we have placed stars in the nearby volume with a constant density. Consider one bin in a histogram of stellar distances. Because of the r^2 scaling of stars with distance, there are more stars in the adjacent bin with a slightly larger distance than in the bin with the smaller distance. More stars will \"scatter\" from larger distances to smaller distances than the opposite, simply because more stars exist at larger distances. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics - distance example\n",
    "\n",
    "The easiest way to deal with the Lutz-Kelker bias is by using Bayesian statistics. Let's start by applying the (in)famous Bayes's Theorem to the distance probability:\n",
    "\n",
    "$$ P(r | r_{\\rm obs}) = \\frac{1}{\\mathcal{Z}} P(r_{\\rm obs} | r) P(r). $$\n",
    "\n",
    "Let's go through this Equation. A term $P(A|B)$, is read as \"the probability of A given B.\" In our case, we are interested in the actual distance to a star $r$ given our observed distance $r_{\\rm obs}$. We cannot directly calculate this. Instead, we need to calculate a few things on the right hand side of the equation to get there.\n",
    "\n",
    "First, $\\mathcal{Z}$ is a normalization constant, also called the \"evidence.\" For most circumstances, this term can be ignored as it is usually a constant. \n",
    "\n",
    "Second, we have a term $P(r_{\\rm obs} | r)$, which is known as the likelihood. Using the left-hand-side as analogy, we can interpret this as the probability of observing a star at a distance $r_{\\rm obs}$ given its actual distance $r$. Based on our observational precision, this term is a Gaussian: \n",
    "\n",
    "$$ P(r_{\\rm obs} | r) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[ - \\frac{\\left( r - r_{\\rm obs} \\right)^2}{2 \\sigma^2} \\right] $$\n",
    "\n",
    "Finally, we have the term $P(r)$, known as a prior probability. In many cases, the prior is \"flat\" (which means that it does not scale with $r$) in which case, the posterior probability scales with the likelihood. However, for distances, we unconsciously applied a prior: we are looking into a sphere with a constant stellar density. In this case, the underlying distribution should scale with $r^2$, which produces a very strong prior:\n",
    "\n",
    "$$ P(r) \\sim r^2 $$\n",
    "\n",
    "This must be normalized. Obviously we cannot observe stars at an infinite distance, and doing so would cause the prior to diverge. We must impose, somewhat artificially, a maximum distance, $r_{\\rm max}$, to calculate the normalization constant. Our prior becomes:\n",
    "\n",
    "$$ P(r) = \\cases{ \\begin{align} 3 \\frac{r^2}{r_{\\rm max}^3} & : 0 < r < r_{\\rm max}\\ \\\\ 0 & : {\\rm else} \\end{align}} $$\n",
    "\n",
    "Combining these terms together, we arrive at the posterior probability for our problem at hand:\n",
    "\n",
    "$$ P(r | r_{\\rm obs}) = \\frac{3}{2} \\frac{1}{\\mathcal{Z}} \\frac{r^2}{ r_{\\rm max}^3 \\sqrt{2 \\pi \\sigma^2} } \\exp\\left[ - \\frac{\\left( r - r_{\\rm obs} \\right)^2}{2 \\sigma^2} \\right], $$\n",
    "\n",
    "for $0<r<r_{\\rm max}$.\n",
    "\n",
    "\n",
    "\n",
    "### Let's see the difference added when the prior is included\n",
    "\n",
    "Let's say we observe a star at a distance of 90$\\pm$20 pc. We want to know what the actual distance to that star is. We will compare the Gaussian expectation (likelihood) with the full posterior distribution, which includes the prior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_likelihood(r, r_obs, sigma):\n",
    "    \"\"\" This is just a Gaussian distribution, as discussed above \"\"\"\n",
    "    \n",
    "    return 1.0/np.sqrt(2.0*np.pi*sigma**2) * np.exp( - (r-r_obs)**2/(2.0*sigma**2) )\n",
    "\n",
    "\n",
    "def calc_prior(r, r_max):\n",
    "    \"\"\" Calculate the prior on r \"\"\"\n",
    "    \n",
    "    prior_r = 3.0 * r**2 / r_max**3  # Our prior from above\n",
    "    prior_r[r<0] = 0.0  # We cannot have negative distances\n",
    "    prior_r[r>r_max] = 0.0  # Stars cannot be (by assumption) farther than 100 pc away\n",
    "    \n",
    "    return prior_r\n",
    "\n",
    "\n",
    "def calc_posterior(r, r_obs, sigma, r_max):\n",
    "    \"\"\" Calculate the posterior probability, from the product of the prior probability and likelihood \"\"\"\n",
    "    \n",
    "    return calc_prior(r, r_max) * calc_likelihood(r, r_obs, sigma)\n",
    "\n",
    "\n",
    "\n",
    "# Our observations\n",
    "r_obs = 90.0  # Observed distance\n",
    "sigma = 20.0  # Measurement precision\n",
    "r_max = 100.0  # Maximum distance of the distribution\n",
    "\n",
    "# Dummy array for testing different distances\n",
    "tmp_r = np.linspace(0, 150, 500)\n",
    "\n",
    "# Call the functions to calculate the likelihood and posterior\n",
    "likelihood = calc_likelihood(tmp_r, r_obs, sigma)\n",
    "posterior = calc_posterior(tmp_r, r_obs, sigma, r_max)\n",
    "\n",
    "# Multiply both distributions by constant such that the maximum value is 1\n",
    "likelihood /= np.max(likelihood)\n",
    "posterior /= np.max(posterior)\n",
    "\n",
    "# Separately plot the likelihood and posterior\n",
    "plt.plot(tmp_r, likelihood, label='Likelihood')\n",
    "plt.plot(tmp_r, posterior, label='Posterior')\n",
    "\n",
    "\n",
    "# Make the plot pretty\n",
    "plt.xlabel('Distance (pc)')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.legend(loc=1)\n",
    "\n",
    "plt.xlim(0, 140)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spend 5 minutes with your partner experimenting with different values of $r_{\\rm obs}$, $r_{\\rm max}$, and $\\sigma$ in the code block above. Make sure you understand why the distribution behaves in the way it does. Is the likelihood always symmetrically distributed around $r_{\\rm obs}$? Is the posterior distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we measure distances to stars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallaxes\n",
    "\n",
    "Astrometric parallax is perhaps the most accurate method of measuring the distances to stars. It is essentially model dependent, relying on geometry only. The image below demonstrates how it works. By observing the same star at different times of the year, one can determine the star's distance from how much it moves on the sky. Fun fact: a parsec is defined as the distance a star is to cause a shift in the parallax of 1 arcsecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(data = 'images/parallax.gif', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to convert distances ($r$) to parallaxes ($\\varpi$):\n",
    "\n",
    "$$ \\varpi = \\frac{1}{r} $$\n",
    "\n",
    "You just need to be careful about your units. Knowing that 1 arcsecond corresponds to 1 pc can help.\n",
    "\n",
    "Rather than measure the distance, astrometric observations seek to measure the parallax. So, our uncertainty will be measured as an angle, rather than a distance. Let's say we may measure the astrometric parallax for a star of 10$\\pm$2 mas. We know that implies a distance of 100 pc, but what is the posterior probability distribution of distances to the star? Let's start with Bayes's Theorem. We want to express this in terms of our observable, $\\varpi$, and our measurement uncertainty on the parallax, $\\sigma_{\\varpi}$:\n",
    "\n",
    "$$ P(r | \\varpi_{\\rm obs}) = \\frac{1}{\\mathcal{Z}} P(\\varpi_{\\rm obs} | r) P(r). $$\n",
    "\n",
    "We only have a slight difference in our likelihood, since everything is now expressed in terms of the parallax:\n",
    "\n",
    "$$ P(\\varpi_{\\rm obs} | r) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\varpi}^2}} \\exp\\left[ - \\frac{\\left( 1/r - \\varpi_{\\rm obs} \\right)^2}{2 \\sigma_{\\varpi}^2} \\right]. $$\n",
    "\n",
    "We can write the new likelihood and posterior probability functions below. The prior has not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_likelihood_parallax(r, parallax_obs, sigma_parallax):\n",
    "    \"\"\" This is just a Gaussian distribution, as discussed above \"\"\"\n",
    "    \n",
    "    return 1.0/np.sqrt(2.0*np.pi*sigma_parallax**2) * np.exp( - (1.0/r-parallax_obs)**2/(2.0*sigma_parallax**2) )\n",
    "\n",
    "\n",
    "def calc_prior(r, r_max):\n",
    "    \"\"\" Calculate the prior on r \"\"\"\n",
    "    \n",
    "    prior_r = 3.0 * r**2 / r_max**3  # Our prior from above\n",
    "    prior_r[r<0] = 0.0  # We cannot have negative distances\n",
    "    prior_r[r>r_max] = 0.0  # Stars cannot be (by assumption) farther than 100 pc away\n",
    "    \n",
    "    return prior_r\n",
    "\n",
    "\n",
    "def calc_posterior_parallax(r, parallax_obs, sigma_parallax, r_max):\n",
    "    \"\"\" Calculate the posterior probability, from the product of the prior probability and likelihood \"\"\"\n",
    "        \n",
    "    return calc_prior(r, r_max) * calc_likelihood_parallax(r, parallax_obs, sigma_parallax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Our observations\n",
    "parallax_obs = 10.0*1.0e-3  # Observed distance\n",
    "sigma_parallax = 2.0*1.0e-3  # Measurement precision\n",
    "r_max = 100.0  # Maximum distance of the distribution\n",
    "\n",
    "# Dummy array for testing different distances\n",
    "tmp_r = np.linspace(1.0, 150, 500)\n",
    "\n",
    "# Call the functions to calculate the likelihood and posterior\n",
    "likelihood = calc_likelihood_parallax(tmp_r, parallax_obs, sigma_parallax)\n",
    "posterior = calc_posterior_parallax(tmp_r, parallax_obs, sigma_parallax, r_max)\n",
    "\n",
    "# Multiply both distributions by constant such that the maximum value is 1\n",
    "likelihood /= np.max(likelihood)\n",
    "posterior /= np.max(posterior)\n",
    "\n",
    "# Separately plot the likelihood and posterior\n",
    "plt.plot(tmp_r, likelihood, label='Likelihood')\n",
    "plt.plot(tmp_r, posterior, label='Posterior')\n",
    "\n",
    "\n",
    "# Make the plot pretty\n",
    "plt.xlabel('Distance (pc)')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.xlim(0, 130)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With your partner, rerun the previous code block, experimenting with different values of parallax, parallax measurement uncertainty, and especially $r_{\\rm max}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving for the prior\n",
    "\n",
    "Let's imagine that we do not know the exponential in the power law prior on $r$, $\\alpha$, and we want to solve for it. The correct answer is 2 ($P(r)\\sim r^2$). With parallax observations to enough stars, we ought to be able to recover the prior distribution. Let's see if we can do that.\n",
    "\n",
    "We start with our probability notation. We want to determine $P(\\alpha | \\{ \\varpi \\})$, where the brackets indicate a set of parallax observations. We use our Bayesian formalism to split this into a prior and a likelihood:\n",
    "\n",
    "$$ P(\\alpha | \\{ \\varpi \\}) = \\frac{1}{\\mathcal{Z}} P(\\{ \\varpi \\} | \\alpha) P(\\alpha). $$\n",
    "\n",
    "#### What should the prior look like?\n",
    "\n",
    "In this case the prior will have a minimal effect, but we would like to constrain it to positive values though. Our prior will be:\n",
    "$$ P(\\alpha) = \\cases{0 : \\alpha <= 0 \\\\ 1 : \\alpha > 0} $$\n",
    "\n",
    "NOTE: This prior is technically unnormalized and is therefore called an \"improper prior.\" We could fix this by additionally setting a maximum value for $\\alpha$ and setting the prior to $1/\\alpha_{\\rm max}$ for $0 < \\alpha < \\alpha_{\\rm max}$. For simplicity, we will keep this prior unnormalized though.\n",
    "\n",
    "#### What about the likelihood?\n",
    "\n",
    "This is seemingly quite simple but can lead to much hand-wringing if you think about it too much. I will state it unproven here: the overal likelihood is the product of the likelihood of each data point, and the likelihood of each data point is the *normalized* probability density function evaluated at each data point:\n",
    "\n",
    "$$ P(\\{ \\varpi \\} | \\alpha) = \\prod_i P(\\varpi_i | \\alpha) $$\n",
    "where\n",
    "$$ P(\\varpi_i | \\alpha) $$ is the likelihood of the $i$th observation.\n",
    "\n",
    "##### Likelihood of an observed value\n",
    "\n",
    "The likelihood of a true underlying distance, $r_{\\rm true}$ is\n",
    "\n",
    "$$ P(r_{\\rm true} | \\alpha) = (\\alpha+1) \\frac{r_{\\rm true}^{\\alpha}}{r_{\\rm max}^{\\alpha + 1}} $$\n",
    "\n",
    "Let's define the difference between the observed parallax ($\\varpi_{\\rm obs}$) and the true, underlying distance ($r_{\\rm true}$). First, we take our likelihood for an individual observation and marginalize over the underlying distance, $r_{\\rm true}$:\n",
    "\n",
    "$$ P( \\varpi | \\alpha) = \\int_{-\\infty}^{\\infty}  P(r_{\\rm true}, \\varpi | \\alpha)\\ dr_{\\rm true} $$\n",
    "\n",
    "We can separate the integral into two, and limit the limits of integration over the allowed range:\n",
    "\n",
    "$$ P( \\varpi | \\alpha) = \\int_{0}^{r_{\\rm max}}  P(\\varpi | r_{\\rm true})\\  P(r_{\\rm true} | \\alpha)\\ dr_{\\rm true}$$\n",
    "\n",
    "$P(\\varpi | r_{\\rm true})$ is the same Gaussian distribution described in the previous example. Remember that the power law is the distribution in $r_{\\rm true}$ not $\\varpi_{\\rm obs}$ or $r_{\\rm obs}$.\n",
    "\n",
    "> Notice that the code will take a substantially longer time to run - this is due to the addition of only one integral within the minimzation routine. If you are marginalizing over two or three parameters (simultaneously dealing with two or three observational uncertainties) the problem can quickly become intractable.\n",
    "\n",
    "\n",
    "#### Converting to the log of the posterior\n",
    "\n",
    "For numerical simplicity, we use the (natural) log of the posterior, and therefore the log of the prior and the likelihood:\n",
    "\n",
    "$$ \\log P(\\alpha | \\{ \\varpi \\}) = \\log P(\\alpha) + \\log P(\\{ \\varpi \\} | \\alpha) $$\n",
    "\n",
    "### Now, some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def func_pdf(alpha, dist_max, dist):\n",
    "    \"\"\" Distance prior with a variable alpha \"\"\"\n",
    "    \n",
    "    # This is the same distance prior as above, but with an arbitrary exponent, alpha\n",
    "    P_dist = (alpha + 1.0) * dist ** alpha / dist_max ** (alpha + 1.0)\n",
    "\n",
    "    # If distance is less than zero or greater than the maximum distance, the likelihood is zero\n",
    "    if type(dist) is np.ndarray:\n",
    "        P_dist[dist > dist_max] = 0.0 \n",
    "        P_dist[dist < 0.0] = 0.0\n",
    "    elif dist > dist_max:\n",
    "        return 0.0\n",
    "    elif dist < 0.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return P_dist\n",
    "\n",
    "\n",
    "def func_integrand(dist, alpha, dist_max, plx_obs, plx_err):\n",
    "    \"\"\" Calculate the integrand in the marginalization written above \"\"\"\n",
    "    \n",
    "    return st.norm.pdf(1.0/dist, loc=plx_obs, scale=plx_err) * func_pdf(alpha, dist_max, dist)\n",
    "\n",
    "\n",
    "def ln_prior(alpha):\n",
    "    \"\"\" Prior distribution on alpha \"\"\"\n",
    "    \n",
    "    return 0.0 if alpha >= 0.0 else -np.inf\n",
    "\n",
    "\n",
    "def ln_likelihood(alpha, dist_max, plx_obs, plx_err):\n",
    "    \"\"\" Likelihood function requires the integration of a function \"\"\"\n",
    "    \n",
    "    ln_likelihood = 0.0\n",
    "    \n",
    "    # Cycle through each observed star\n",
    "    for i in range(len(plx_obs)):\n",
    "        \n",
    "        # Limits are either the limits of the pdf or 5 sigma from the observed value\n",
    "        a = max(0.0, 1.0/(plx_obs[i] + 5.0 * plx_err[i]))\n",
    "        b = min(dist_max, 1.0/(np.max([1.0e-5, plx_obs[i] - 5.0 * plx_err[i]])))\n",
    "\n",
    "        # Calculate the integral\n",
    "        val = quad(func_integrand, a, b, \n",
    "                   args=(alpha, dist_max, plx_obs[i], plx_err[i]), \n",
    "                   epsrel=1.0e-4, epsabs=1.0e-4)\n",
    "\n",
    "        # Add the log likelihood to the overall sum\n",
    "        ln_likelihood += np.log(val[0])\n",
    "\n",
    "    return ln_likelihood\n",
    "\n",
    "\n",
    "def ln_posterior(alpha, dist_max, plx_obs, plx_err):\n",
    "    \"\"\" Log of the posterior function is the sum of the log of the prior and likelihood functions \"\"\"\n",
    "    \n",
    "    return ln_prior(alpha) + ln_likelihood(alpha, dist_max, plx_obs, plx_err)\n",
    "\n",
    "def neg_ln_posterior(alpha, dist_max, plx_obs, plx_err):\n",
    "    \"\"\" The minimization algorithm finds the minimum of a function, \n",
    "    therefore we provide a wrapper function that returns the negative of the log posterior. \"\"\"\n",
    "    \n",
    "    return -ln_posterior(alpha, dist_max, plx_obs, plx_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the code above and discuss with your partner the following questions:\n",
    "1. Why do we work in log-space (log prior, log likelihood, log posterior) rather than linear space?\n",
    "2. Go over the code for the integral calculation above. What are each of the individual terms provided to the function?\n",
    "3. Why do we require the neg_ln_posterior function above, in addition to the ln_posterior function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is hidden here\n",
    "\n",
    "[//]: # (1. We work in log-space because then the prior and likelihoods can be easily added together. One quickly reaches numerical overflow/underflow when using the prior and likelihood. 2. func_integrand: the function to be integrated, a: the lower integration limit, b: the upper integration limit, args: list tuple of additional arguments to be passed to the integrand function, epsrel: the relative accuracy of the integration, epsabs: the absolute accuracy of the integration. 3. Numerical optimization techniques typically search for the minimum of a function. Since we want to maximize the log-posterior, we can alternatively minimize the negative of the log-posterior.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mock data to fit below\n",
    "\n",
    "We will generate 100 stars, evenly distributed within the nearest 100 pc. then we will observe them with a parallax precision of 2 mas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_stars = 50\n",
    "dist_max = 100.0\n",
    "\n",
    "# Generate random distances within nearest 100 pc\n",
    "distance = dist_max*np.random.rand(N_stars)**(1.0/3.0)\n",
    "\n",
    "plx = 1.0/distance\n",
    "plx_err = 2.0*1.0e-3 * np.ones(len(plx))\n",
    "plx_obs = plx + np.random.normal(0.0, plx_err, size=N_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle through different values of $\\alpha$, and calculate the log posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dist = np.linspace(0.1, 4.0, 30)\n",
    "tmp_prob = np.zeros(len(tmp_dist))\n",
    "for i in range(len(tmp_dist)):\n",
    "    tmp_prob[i] = neg_ln_posterior(tmp_dist[i], dist_max, plx_obs, plx_err)\n",
    "#     print(tmp_dist[i], tmp_prob[i])\n",
    "plt.plot(tmp_dist, tmp_prob)\n",
    "\n",
    "\n",
    "plt.axvline(2.0, color='k', linestyle='dashed')\n",
    "\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$-\\ln\\ \\mathcal{L}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning minimization\n",
    "\n",
    "Using the exact same posterior function which we've previously calculated, we can run a minimizer to determine the most optimal value of $\\alpha$ given our data set. The only thing required is a starting value. Here, we will supply the minimizer with the wrong starting value and see what it finds as the best-fit to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "# First, plot the results from the previous simulation\n",
    "plt.plot(tmp_dist, tmp_prob)\n",
    "\n",
    "\n",
    "# We will give it the wrong starting value\n",
    "p0 = 4.0\n",
    "\n",
    "print(\"Minimizing...\")\n",
    "start = time.time()\n",
    "res = minimize(neg_ln_posterior, p0, args=(dist_max, plx_obs, plx_err))\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "if res.success:\n",
    "    print(\"Expected result: 2.0\")\n",
    "    print(\"Result:\", res.x)\n",
    "else:\n",
    "    print(\"Optimization failed\")\n",
    "    \n",
    "print(\"Elapsed time:\", end-start, \"seconds\")\n",
    "\n",
    "\n",
    "plt.axvline(2.0, color='k', linestyle='dashed', label='Correct value')\n",
    "\n",
    "plt.axvline(res.x, color='k', label='Best-fit')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$-\\ln\\ \\mathcal{L}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Even when data seems to be reasonably distributed, prior distributions may lead to unexpected results. Please, please, please consider your priors! As long as you can write your prior distribution down, you have a generative model, and your likelihood function is well formed, you can calculate a posterior probability (ignoring the Bayesian evidence). Once having done so, you can solve for the result using a grid search (where you systematically test a range of values), some minimization technique (as was done above), or other methods which we will learn about in the afternoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
