{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A bit of introduction to ML\n",
    "\n",
    "## What is machine learning ?\n",
    "\n",
    "Arthur Samuel (1959): \n",
    "\n",
    ">*\"Field of study that gives computers the ability to learn without being explicitly programmed.\"*\n",
    "\n",
    "## How did we get here? \n",
    "\n",
    "<center><<img src=\"images/ml_history.png\" width=800> \n",
    "    Figure 1.1. Timeline from Artificial Intelligence, to Machine Learning, and Deep learning.<br>\n",
    "(Credit: <a href=\"https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/\" target=\"_blank\" rel=\"noopener noreferrer\"> Nvidia blog - (an interesting read!</a>)</center>\n",
    "\n",
    "## and ... why now? \n",
    "\n",
    "- More powerful, abundant, and cheap computation (CPUs/GPUs).\n",
    "- Growing data sets.\n",
    "- Advancements in underlying algorithms and implementation.\n",
    "\n",
    "This is true for both everyday and Astronomy applications !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML branches\n",
    "\n",
    "<center><<img src=\"images/ml_branches.png\" width=800> \n",
    "Figure 2.1. Branches and example applications of Machine Learning<br>\n",
    "(Credit: <a href=\"https://www.cognub.com/index.php/cognitive-platform/\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\"> CogHub</a>)</center>\n",
    "\n",
    "- **Supervised**: Labelled data where the algorithms learn to predict the output from the input data.\n",
    "- **Unsupervised**: Non-labelled data where the algorithms learn to identify structures from the input data.\n",
    "- **Semi-supervised**: Some labelled data - most is not - and a mixture of supervised and unsupervised techniques can be implemented.\n",
    "\n",
    "## Unsupervised approaches:\n",
    "\n",
    "- Clustering: discover groupings or/and structures in the data, i.e. concentrations of datapoints or overdensities (e.g. the locations of galaxies in a BPT diagram)\n",
    "- Association:  discover the rule(s) describing between variables or features in a dataset (e.g. customer recommendations)\n",
    "- Dimentionality reduction: tool to reduce the number of input variables or features in a dataset (the more you have the more challenging it becomes to build a predictive model - referred to as *curse of dimentionality*) - also usefulf for visualization purposes.  \n",
    "\n",
    "## Pros \n",
    "+ They can make new discoveries, as often enough we don’t know what they’re looking for in data.\n",
    "+ They do not require training, which saves (huge) time on producing labels (manual classification tasks such as spectroscopic classification).\n",
    "+ It reduces the chance of human error and bias, which could occur during manual labeling processes.\n",
    "+ Unlabeled data is much easier and faster to get.<br>\n",
    "\n",
    "## Cons\n",
    "- Output needs careful proper interpretation: \n",
    "    - the groups may not match informational classes\n",
    "    - extra effort has to be made to validate the groups. \n",
    "- Less accurate predictive results, as the labels are not part of the process and the method has to learn it by itself. \n",
    "- More time is needed to train these algorithms:\n",
    "    - they need time to analyze and calculate all possibilities\n",
    "    - the deal with huge datases that may increase computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The most critical take-home points:\n",
    "\n",
    "- No matter which algorithm you pick, the goal of ML is to make **predictions** and **classifications**.\n",
    "- There is **no optimal** algorithm, it all depends on your specific problem!<details>\n",
    "<summary>( Click for an illustration of this point )</summary>\n",
    "<center><img src=\"images/my-precious-not.jpg\"> \n",
    "Figure 3.1. There is not a single algorithm to rule them all !!!<br>\n",
    "(Credit: <a href=\"https://knowyourmeme.com/memes/my-precious\"\n",
    " target=\"_blank\" rel=\"noopener noreferrer\"> knowyourmeme.com</a>)</center>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering algorithms overview\n",
    "\n",
    "We start by presenting a set of [**sklearn clustering**](http://scikit-learn.org/stable/modules/clustering.html) algorithms with toy datasets, and then we continue by applying some of them in various astrophyiscal datasets.\n",
    "\n",
    "This serves as a showcase of the available methods and how they compare. You can easily adapt any of these methods to the following examples or your own problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up clustering parameters\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and plotting\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizz time: take a few moments and explore the results - what do you notice? \n",
    "\n",
    "Write some points here (and even better send them to slack!):\n",
    "\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NOTE: The last dataset is an example of a 'null' situation for clustering, i.e. the data is homogeneous. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do they separate the clusters?\n",
    "\n",
    "Typically, clustering algorithms use two types of hyperparamters:\n",
    "\n",
    "- The properties of the data themselves<br>\n",
    "    *e.g. density of points (minimum number of points and the area), distance between points, etc.*\n",
    "- The number of clusters<br>\n",
    "    *might be provided by the user or dynamically derived*\n",
    "\n",
    "In the following sections we describe some of them in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. K-means\n",
    "\n",
    "The K-means algorithm tries to partition a sample of N observations (with each observation being a $d$-dimensional vector) into $k$ individual clusters $C_k$. The goal is to minimize the within-cluster sum-of-squares (or **inertia**) of the observations:\n",
    "\n",
    "$$min \\left (  \\sum_{k=1}^{K} \\sum_{i \\epsilon C_k} ||x_i-\\mu_k||^2 \\right )$$\n",
    "\n",
    "where $\\mu_k=\\frac{1}{N_k}\\sum_{i \\epsilon C_k} x_i$ is the mean/centroid of the $N_k$ points included in each of the $C_k$ clusters. \n",
    "\n",
    "<center><<img src=\"images/kmeans.gif\"> \n",
    "Figure 5.1. Evolution of K-means centroids through iterations. <br>\n",
    "(Credit: <a href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"  target=\"_blank\" rel=\"noopener noreferrer\">Clustering with Scikit with GIFs, by David Sheehan</a>)</center>\n",
    "\n",
    "**Steps**:\n",
    "1. Initiate algorithm by selecting $k$ means <br>\n",
    "    *e.g. select randomly $k$ observations as initial means - see also [Wiki:K-means initialization](http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods)*\n",
    "2. Assign each observation to the nearest cluster\n",
    "3. Calculate the new mean value for each cluster $C_k$ according to the new observations assiged\n",
    "4. Repeat steps 2 and 3 up to the point that there are no updates in the assigments to the clusters.\n",
    "\n",
    "A globally optimal minimum is not guaranteed (might converge to a local minimum). This is highly dependent on the initialization of the centroids. This is why, in practice, K-means is run multiple times with different starting values selecting the result with the lowest sum-of-squares error. To improve on that we can initially select centrodis that are generally distant from each other (sklearn implementation by using `init='k-means++'` parameter). For more see [Grouping data points with k-means clustering, by Jeremy Jordan](https://www.jeremyjordan.me/grouping-data-points-with-k-means-clustering/).\n",
    "\n",
    "**Complexity**\n",
    "\n",
    "$O(knT)$, where k, n and T are the number of clusters, samples and iterations, respectively.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Simple and intuitive\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- The number of clusters (K) must be provided (or cross-validated)\n",
    "- There is an inherit assumption of isotropic clusters (i.e. not well fitted for elongated clusters, or manifolds with irregular shapes)\n",
    "- Inertia is not a normalized metric: lower values are better , but as the dimensions increase so does the inertia\n",
    "\n",
    "## Mini Batch K-means\n",
    "\n",
    "For faster computations the sklearn offers the [Mini Batch K-means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans) method which simply breaks the initial set of observations/data points to smaller randomly selected subsamples.\n",
    "\n",
    "For each subsample in the mini batch the assigned centroid is updated by taking into account the average of that subsample and all previous subsamples assigned to that centroid. This is repeated until the predefined number of iterations is reached. Its results are generally only slightly worse then the standard algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mean Shift\n",
    "\n",
    "Mean Shift identifies arbitraty shaped _blobs_ by locating the peaks of a density estimate of the data. It works by updating cadidates for centroids to be the mean of the points within a given region. (The candidates are filtered in a post processing step to eliminate near-duplicates.)\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "For a given candidate centroid $x_i$ for iteration $t$, the candidate is updated by:\n",
    "\n",
    "$$ x_i^{t+1} = m(x_i^{t}) $$\n",
    "\n",
    "and their difference $m(x_i^{t}) - x_i^{t+1}$ is the **mean shift** vector. $m$ is calculated as:\n",
    "\n",
    "$$m(x)=\\frac{ \\sum_{x_i\\epsilon N(x)}K(x_i-x)x_i}{\\sum_{x_i\\epsilon N(x)}K(x_i-x)}$$.\n",
    "\n",
    "which corresponds to the weighted mean of the points within a certain neighborhood neighborhood $N(x_i)$. The weighting is provided by a kernel function $K$, typically a gaussian:\n",
    "\n",
    "$$ K(x_i-x)=e^{-c||x_i-x||^2}$$ \n",
    "\n",
    "The algorithm iteratively shifts the centroid of a cluster \"climbing\" the peak of the density distribution of the nearby data points. The search stops when the update in the centroid is below some threshold.\n",
    "\n",
    "Apart from the kernel function, the user must also define the neighborhood $N(x)$, through the **bandwith** $h$ parameter. This implicitly sets the number of K of clusters which will be found. (There is also an automated estimation of this parameter by using [`sklearn.cluster.estimate_bandwidth`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.estimate_bandwidth.html).)\n",
    "\n",
    "<center><<img src=\"images/mean_shift.gif\"> \n",
    "Figure 6.1. Evolution of a Mean Shift centroid through iterations. <br> The centroid \"climbs\" towards the center of the density distribution.<br>\n",
    "(Credit: <a href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"  target=\"_blank\" rel=\"noopener noreferrer\"> Clustering with Scikit with GIFs, by David Sheehan</a>)</center>\n",
    "\n",
    "**Complexity**\n",
    "\n",
    "$O(Tn^{2})$, where n and T are the number of samples and iterations, respectively.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Guaranteed to converge\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- The number of clusters implicitly set\n",
    "- There is an inherit assumption of isotropic clusters\n",
    "- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hierarchical Clustering\n",
    "\n",
    "This type of clustering groups points by ranking them **bottom-up** (**agglomerative**) or **top-down** (**divisive**). The first approach merges close points into clusters based on the distance between them, while the latter starts from one clusters and split each point at a time. \n",
    "\n",
    "There are two important parameters: the distance metric (typically Euclidean) and the type of linkage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "This is an example of the **bottom-up** case.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initiate with K$_{clusters}$ = N$_{points}$\n",
    "2. Calculate the distances between all the pairs of points\n",
    "3. Iteratively connect the nearest pairs of points until obtaining a single cluster<br>\n",
    "    *(see the dendrogram below)*\n",
    "4. Merge all clusters below a threshold, which could be:\n",
    "    - a given number of clusters (stop at K clusters)\n",
    "    - a given separation\n",
    "\n",
    "<center>\n",
    "<table><tr>\n",
    "    <td width=500>\n",
    "        <img src=\"images/dendogram.jpeg\">\n",
    "    </td>\n",
    "    <td width=500>\n",
    "        <img src=\"images/hierarchical.gif\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "    Figure 7.1.1. <i>Left:</i> Construction of dendogram and application of decision threshold. <i>Right:</i> Animated explanation of Hierarchical Clustering.<br>\n",
    "(Credit: <a href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"  target=\"_blank\" rel=\"noopener noreferrer\">Clustering with Scikit with GIFs, by David Sheehan</a>)    \n",
    "</center>\n",
    "\n",
    "**Type of  point linking for Agglomerative Clustering**\n",
    "\n",
    "For additional complexity, one can chose other solutions other than the pairwise coupling of points.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/cluster_distances.jpeg\" width=400> \n",
    "Figure 7.1.2. Type of point linking for Agglomerative Clustering.  <br>\n",
    "(Credit: <a href=\"https://www.youtube.com/watch?v=VMyXc3SiEqs\"  target=\"_blank\" rel=\"noopener noreferrer\"> Hierarchical Clustering 3: single-link vs. complete-link, by  Victor Lavrenko</a>)    \n",
    "</center>\n",
    "\n",
    "**Complexity**\n",
    "\n",
    "$O(n^2 log(n))$, where n is the number of points. \n",
    "\n",
    "**Pros**\n",
    "\n",
    "- No need to specify number of clusters in advance\n",
    "- Eary to implement and explain the results\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Cannot update connections \n",
    "- Use of different distance metrics can yield different results\n",
    "- Very slow and computationally expensive\n",
    "- Not scalable to large datasets \n",
    "\n",
    "\n",
    "## Ward\n",
    "\n",
    "Is also an aggomerative algorithm in which case the linkage is performed by minimizing the sum of the squared differences of the point within all clusters (variance - minimizing approach). In practice it is similar to k-means but with a heararchical approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. DBSCAN or Density Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Thus, clusters found by DBSCAN can be of any shape - as opposed to other algorithms (K-means for example) which assume that clusters are convex shaped.\n",
    "\n",
    "It uses 2 parameters:\n",
    "* $eps$ : neighborhood size\n",
    "* $minPts$ : minimum number of points for a neighborhood to be considered dense\n",
    "\n",
    "<center>\n",
    "<table><tr>\n",
    "    <td width=500>\n",
    "        <img src=\"images/dbscan.png\">\n",
    "    </td>\n",
    "    <td width=500>\n",
    "        <img src=\"images/dbscan-smiley.gif\">\n",
    "    </td>\n",
    "</tr></table>\n",
    "    Figure 8.1. Definition of core, border, and noise points, according to DBSCAN (left), and an example of how it constructs clusters (right).<br>\n",
    "(Credit: <a href=\"https://arogozhnikov.github.io/2017/07/10/opera-clustering.html\"  target=\"_blank\" rel=\"noopener noreferrer\">Clustering applied to showers in the OPERA, by Alex Rogozhnikov</a>)    \n",
    "</center>\n",
    "\n",
    "With a sinlge (1) scan we can label the points as: **core**, **border**, **noise**. How?\n",
    "\n",
    "-  A point $p$ is defined **core** if at least $minPts$ points are within the area defined by $eps$\n",
    "-  A **border** point is a non-core point that has at least 1 core point in its neighborhood\n",
    "-  A **noise** point is neither a core nor a border point. There represent outliers in the data set\n",
    "\n",
    "Defined these, DBSCAN operates as follows.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Algorithm picks 1 unassigned core point ($p_c$)\n",
    "2. Let $p_c$ be the current point being explored\n",
    "3. Add all points ($q$) of $p_c$’s neighborhood to the same cluster\n",
    "4. Some of these $q$ points are also core points so: recursively apply this search on each unexplored core point of this neighborhood\n",
    "5. When all neighbourhood points have been joined, DBSCAN proceeds with a new cluster\n",
    "\n",
    "Process ends when all points have been assigned to a cluster or identified as noise.  \n",
    "\n",
    "**Complexity**\n",
    "\n",
    "$O(n^2)$, where n is the number of points. \n",
    "\n",
    "**Pros**\n",
    "- Any number of clusters\n",
    "- Clusters of varying size and shape\n",
    "- Finds and ignores outliers\n",
    "\n",
    "**Cons**\n",
    "- Relatively slow\n",
    "- Extremely sensitive to parameters choice\n",
    "- In rare cases, border points move to an other cluster when DBSCAN is re-run\n",
    "- Serious troubles with clusters with varying density<br>\n",
    "  *(OPTICS and HDBSCAN are variations which address this problem)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Which to select ? \n",
    "| Algorithm                | fixed n$_{clusters}$ ? |\n",
    "| :----------------------: | :--: |\n",
    "| KMeans                   | yes  |\n",
    "| Mean Shift               | implicitly |\n",
    "| Agglomerative Clustering | yes or no |\n",
    "| DBSCAN                   | no |\n",
    "\n",
    "&#8618; check also [sklearn's table on clustering overview](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Application 1: Classify Star-forming objects in a BPT diagram\n",
    "\n",
    "The \"Baldwin, Phillips & Terlevich\" (BPT) diagrams are used to distinguish sources based on specific spectral emission lines. The strengths of these lines depend on the heating source (see e.g. [BPT diagram, NED](https://ned.ipac.caltech.edu/level5/Glossary/Essay_bpt.html)).\n",
    "\n",
    "BPTs allow to distinguish:\n",
    "- AGNs (Seyfert)\n",
    "- LINERs\n",
    "- Star-forming galaxies\n",
    "- Composite objects\n",
    "\n",
    "<center><img src=\"images/BPT.png\"> \n",
    "Figure 10.1. Example of classification via BPT diagram.<br> Theoretical or observationally-calibrated curves allow to distinguish the different subpopulations.<br>\n",
    "(From <a href=\"https://ui.adsabs.harvard.edu/abs/2010ApJ...720..555P/abstract\" target=\"_blank\" rel=\"noopener noreferrer\"> Parra et al. (2010), ApJ, 720, 555</a>)</center>\n",
    "\n",
    "## The sample\n",
    "\n",
    "We will use the data by [Stampoulis et al. (2019), MNRAS, 485, 1085](https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.1085S/abstract), which provides the OIII, NII, SII, and OI diagnostics for ~130 000 objects.\n",
    "\n",
    "The work also gives classifications, which we will use as a reference.\n",
    "\n",
    "--- \n",
    "\n",
    "**TASK 1: Find the best clustering algorithm for separating star-forming objects**\n",
    "\n",
    "**TASK 2: Plot results and check consistency with BPT theoretical curves**\n",
    "\n",
    "**CHALLENGE: Consult the sklearn's pages to tune more hyperparameters!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data and classifications from Stampoulis+19:\n",
    "\n",
    "PATH_Stampoulis_data = \"data/Stampoulis+19_Table_2.csv\"\n",
    "\n",
    "data = np.genfromtxt(PATH_Stampoulis_data, delimiter=\",\")\n",
    "# The data file is organized in 138799 lines (i.e. different objects), and 12 columns\n",
    "\n",
    "# To check file dimensions:\n",
    "# print(data.shape)\n",
    "\n",
    "ID               = data[:,0]  # object ID\n",
    "NII_diagnostic   = data[:,3]  # log10 ( NII_6584  / H_alpha )\n",
    "SII_diagnostic   = data[:,4]  # log10 ( SII_6717  / H_alpha )\n",
    "OI_diagnostic    = data[:,5]  # log10 ( OI        / H_alpha )\n",
    "OIII_diagnostic  = data[:,6]  # log10 ( OIII_5007 / H_beta )\n",
    "\n",
    "labels = np.genfromtxt(PATH_Stampoulis_data, delimiter=',', usecols=-1, dtype=str)\n",
    "# reading labels from last column\n",
    "# Activity class labelling scheme:\n",
    "#   0 <-> SFG (Star Forming Galaxy)\n",
    "#   1 <-> SEY (Seyfert)\n",
    "#   2 <-> LIN (LINER)\n",
    "#   3 <-> COM (Composite)\n",
    "\n",
    "# Dictionary containg class name and associated label:\n",
    "from collections import OrderedDict\n",
    "classes = OrderedDict()\n",
    "classes[\"SFG\"] = 0\n",
    "classes[\"SEY\"] = 1\n",
    "classes[\"LIN\"] = 2\n",
    "classes[\"COM\"] = 3\n",
    "\n",
    "labels = [int(float(label)) for label in labels]\n",
    "# converting labels from strings to integers\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X_sample = np.stack((OIII_diagnostic,NII_diagnostic,SII_diagnostic,OI_diagnostic),axis=-1)\n",
    "y_sample = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only 1 every \"sampling_factor\" objects for two reasons:\n",
    "\n",
    "- to speed up the exercise\n",
    "- to avoid crashes due to memory limitations.\n",
    "\n",
    "You can try to use the full sample when confident with the setup (and your computer power!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUGGESTION: Use only 1 every <sampling_factor> objects \n",
    "\n",
    "sampling_factor = 50\n",
    "# sample 1 every <sampling_factor> data to avoid computational delay\n",
    "\n",
    "X = X_sample[::sampling_factor]\n",
    "y = y_sample[::sampling_factor]\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"_____________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' samples x ' + str(X.shape[1]) + ' diagnostics' )\n",
    "print(\"-----|-------------------------------\")\n",
    "print('  y  | ' + str(len(y)) + ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although available, we will not use the labels for the analysis, but only for the first representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "Essentially reproducing Figure 5 in Stampoulis+ 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Limit scatter plots (not histograms) in showing a maximum of <N_plot> objects:\n",
    "# for full sample size, use: N_plot = len(X)\n",
    "N_plot = 5000\n",
    "# NOTE: reducing the sample in the plot helps visualizing the density\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   red    <-> SFG\n",
    "#   yellow <-> SEY\n",
    "#   blue   <-> LIN\n",
    "#   green  <-> COM\n",
    "cmap = mpl.colors.ListedColormap(['red','orange','blue','green'])\n",
    "\n",
    "# Remeber that the sample X is organized as:\n",
    "#  X[:,0] <-> OIII_diagnostic\n",
    "#  X[:,1] <-> NII_diagnostic\n",
    "#  X[:,2] <-> SII_diagnostic\n",
    "#  X[:,3] <-> OI_diagnostic\n",
    "\n",
    "\n",
    "# PLOT THE DIAGNOSITCS\n",
    "\n",
    "# > Classification lines\n",
    "#   NII:\n",
    "x1 = np.linspace(-2, 0.05, 100)\n",
    "x2 = np.linspace(-2, 0.47, 100)\n",
    "x3 = np.linspace(-0.1839, 1)\n",
    "ke01_NII = 0.61 / (x1-0.05) + 1.3   # Kewley+01\n",
    "ka03_NII  = 0.61 / (x2-0.47) + 1.19 # Kuffmann+03\n",
    "sc07_NII  = 1.05 * x3 + 0.45        # Schawinski+07\n",
    "#   SII:\n",
    "x4 = np.linspace(-2, 0.05, 100)\n",
    "x5 = np.linspace(-0.3, 1)\n",
    "ke01_SII  = 0.72 / (x4-0.32) + 1.3  # Kewley+01\n",
    "ke06_SII  = 1.89 * x5 + 0.76        # Kewley+06\n",
    "#   OI:\n",
    "x6 = np.linspace(-2, -0.8, 100)\n",
    "x7 = np.linspace(-1.1, 0)\n",
    "ke01_OI = 0.72 / (x6+0.59) + 1.33  # Kewley+01\n",
    "ke06_OI = 1.18 * x7 + 1.30         # Kewley+06\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ylim = [-1.2,1.5] # OIII_diagnostic range\n",
    "\n",
    "# > left plot\n",
    "\n",
    "xlim = [-2,1] # NII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "im = ax.scatter(X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([NII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x1, ka03_NII, \"--\", color='red',  linewidth = 1.0, label='Ka03')\n",
    "ax.plot(x2, ke01_NII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x3, sc07_NII, \"-\",  color='blue', linewidth = 1.0, label='Sc07')\n",
    "ax.legend()\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.25, \"SFG\", color='red',    transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.20, \"SEY\", color='orange', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.15, \"LIN\", color='blue',   transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"COM\", color='green',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > central plot\n",
    "\n",
    "xlim = [-1.4,0.7] # SII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "im = ax.scatter(X[-N_plot:, 2], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([SII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x4, ke01_SII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x5, ke06_SII, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# > right plot\n",
    "\n",
    "xlim = [-2.2,0.0] # OI_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "im = ax.scatter(X[-N_plot:, 3], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([OI]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x6, ke01_OI, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x7, ke06_OI, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# NOTE: Ignore the warning, due to the plotting of the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running clustering algorithms and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 8))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "params = {'quantile': ...,\n",
    "            'eps': ...,\n",
    "            'damping': ...,\n",
    "            'preference': ...,\n",
    "            'n_neighbors': ...,\n",
    "            'n_clusters': ...}\n",
    "\n",
    "# normalize dataset for easier parameter selection\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# estimate bandwidth for mean shift\n",
    "bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "# connectivity matrix for structured Ward\n",
    "connectivity = kneighbors_graph(\n",
    "    X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "# make connectivity symmetric\n",
    "connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "# ============\n",
    "# Create cluster objects\n",
    "# ============\n",
    "ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "k_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "ward = cluster.AgglomerativeClustering(\n",
    "    n_clusters=params['n_clusters'], linkage='ward',\n",
    "    connectivity=connectivity)\n",
    "average_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"average\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "complete_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"complete\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "spectral = cluster.SpectralClustering(\n",
    "    n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "    affinity=\"nearest_neighbors\")\n",
    "dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "affinity_propagation = cluster.AffinityPropagation(\n",
    "    damping=params['damping'], preference=params['preference'])\n",
    "birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "clustering_algorithms = (\n",
    "    ('MiniBatchKMeans', k_means),\n",
    "    ('AffinityPropagation', affinity_propagation),\n",
    "    ('MeanShift', ms),\n",
    "    ('SpectralClustering', spectral),\n",
    "    ('Ward', ward),\n",
    "    ('AgglomerativeClustering', average_linkage),\n",
    "    ('DBSCAN', dbscan),\n",
    "    ('Birch', birch),\n",
    "    ('GaussianMixture', gmm)\n",
    ")\n",
    "\n",
    "# restoring normalization for plotting\n",
    "X_plot = X_sample[::sampling_factor]\n",
    "\n",
    "for name, algorithm in clustering_algorithms:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # catch warnings related to kneighbors_graph\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"the number of connected components of the \" +\n",
    "            \"connectivity matrix is [0-9]{1,2}\" +\n",
    "            \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "            category=UserWarning)\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"Graph is not fully connected, spectral embedding\" +\n",
    "            \" may not work as expected.\",\n",
    "            category=UserWarning)\n",
    "        algorithm.fit(X)\n",
    "\n",
    "    t1 = time.time()\n",
    "    if hasattr(algorithm, 'labels_'):\n",
    "        y_pred = algorithm.labels_.astype(int)\n",
    "    else:\n",
    "        y_pred = algorithm.predict(X)\n",
    "        \n",
    "    plt.subplot(1, len(clustering_algorithms), plot_num)\n",
    "    plt.title(name, size=18)\n",
    "\n",
    "    plt.plot(x1, ka03_NII, \"--\", color='grey',  linewidth = 1.0, label='Ka03')\n",
    "    plt.plot(x2, ke01_NII, \"-\",  color='grey',  linewidth = 1.0, label='Ke01')\n",
    "    plt.plot(x3, sc07_NII, \"-\",  color='grey', linewidth = 1.0, label='Sc07')\n",
    "    \n",
    "    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                         '#f781bf', '#a65628', '#984ea3',\n",
    "                                         '#999999', '#e41a1c', '#dede00']),\n",
    "                                  int(max(y_pred) + 1))))\n",
    "    plt.scatter(X_plot[:, 2], X_plot[:, 0], s=2, color=colors[y_pred])\n",
    "    plt.scatter(X_plot[y_pred==-1,2], X_plot[y_pred==-1,0], s=100, facecolors='none', edgecolors='black', color='black', label='background', alpha=0.1, zorder=0)\n",
    "    # marking background class for DBSCAN\n",
    "\n",
    "    plt.gca().set_xlim(-2,1)     # NII_diagnostic range\n",
    "    plt.gca().set_ylim(-1.2,1.5) # OIII_diagnostic range\n",
    "\n",
    "    plt.xticks(np.arange(xlim[0], xlim[1], step=1))\n",
    "    plt.yticks(np.arange(ylim[0], ylim[1], step=0.5))\n",
    "\n",
    "    plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n",
    "    plot_num += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Which algorithm performs best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Application 2: Count stellar clusters\n",
    "\n",
    "In [Bitsakis, et al. (2017), ApJ, 845, 56]( https://ui.adsabs.harvard.edu/abs/2017ApJ...845...56B/abstract) we describe a technique for the detection of star clusters in the Large Magellanic Cloud. The basic step for the cluster detection was converting the observed images into pixel-maps, where each star was represented by a single pixel.\n",
    "\n",
    "In this exercise we use clustering algorithms to detect clusters in simulated stellar fields, which we have already converted to pixel-maps. Each pixel-map includes an arbitrary (unknown to the student) number of clusters. The first 3 images present increasing noise levels. One last map has been smoothed to simulate observational data.\n",
    "\n",
    "--- \n",
    "\n",
    "**TASK: Guess the number of stellar clusters in the provided images using cluster algorithms**\n",
    "\n",
    "**CHALLENGE: Consult the sklearn's pages to tune more hyperparameters!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# FITS manipulation:\n",
    "from astropy.io import fits\n",
    "\n",
    "PATH_cluster_files = [\n",
    "    \"data/clusters_0.fits\",\n",
    "    \"data/clusters_1.fits\",\n",
    "    \"data/clusters_2.fits\",\n",
    "    \"data/clusters_2_smooth.fits\"]\n",
    "\n",
    "# > Loading fits files:\n",
    "image_data = []\n",
    "\n",
    "for i, PATH_cluster_file in enumerate(PATH_cluster_files):\n",
    "\n",
    "    hdulist = fits.open(PATH_cluster_file)\n",
    "    image_data.append(hdulist[0].data)\n",
    "    hdulist.close()\n",
    "\n",
    "# > Displaying fits files:    \n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "for i in range(len(PATH_cluster_files)):\n",
    "\n",
    "    ax = plt.subplot(1, 4, int(i+1))  \n",
    "    ax.imshow(image_data[i], cmap='gray', vmin=-1, vmax=0.5*np.max(image_data[i]), origin='lower')\n",
    "\n",
    "print('All black pixels have a value of %s' % int(np.min(image_data[0])))\n",
    "print('All white pixels have a value of %s' % int(np.max(image_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTING FROM IMAGE FORMAT TO STANDARD FORMAT FOR CLASSIFIER\n",
    "\n",
    "'''\n",
    "The image shape loaded with fits.open() is:\n",
    "    <n_pixels, n_pixels>\n",
    "where each element represents an image row (intensity_1, intensity_2, ... , intensity_n).\n",
    "We will convert this to the more convenient shape:\n",
    "    X = <n_pixels^2, 3>\n",
    "where each element represents a single pixel (x, y, intensity).\n",
    "This is the format used by clustering algorithms.\n",
    "'''\n",
    "\n",
    "datasets = []\n",
    "# list containing all images\n",
    "\n",
    "for i, PATH_cluster_file in enumerate(PATH_cluster_files):\n",
    "\n",
    "    x = np.arange(image_data[i].shape[1])\n",
    "    y = np.arange(image_data[i].shape[0])\n",
    "    # NOTE: python inverts i with j index\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    X_image = np.array([xx.ravel(), yy.ravel(), image_data[i].ravel()]).T\n",
    "\n",
    "    X_image = X_image[X_image[:,2]!=0]\n",
    "    # removing background pixels (i.e. pixels with flux == 0)\n",
    "    # NOTE: noise pixels are still in the sample!\n",
    "    \n",
    "    if(PATH_cluster_file == \"data/clusters_2_smooth.fits\"):\n",
    "    # NOTE: For the smoothed image, we will apply a background thershold or else\n",
    "    #       too many data points will be used and the clustering algorithms will\n",
    "    #       crash due to memory limitations.\n",
    "        X_image = X_image[X_image[:,2]>100]\n",
    "    \n",
    "    # After dealing with background, now going back to 2D (x, y):\n",
    "    X_image = X_image[:,0:2]\n",
    "    # NOTE: We will only be using the (x,y) position of the stars (not the pixel \"intensity\"\n",
    "    \n",
    "    print(\"Image \" + str(PATH_cluster_file) + \" shape | \" + str(X_image.shape))\n",
    "        \n",
    "    datasets.append(X_image)\n",
    "    \n",
    "# > Displaying new image format:\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "for i in range(len(PATH_cluster_files)):\n",
    "\n",
    "    X_image = datasets[i]\n",
    "    \n",
    "    ax = plt.subplot(1, 4, int(i+1))  \n",
    "    ax.scatter(X_image[:,0],X_image[:,1], s=1)\n",
    "    \n",
    "print('\\nNOTE: These are not images of stars: they only represent the positions of the stars.')\n",
    "print('Therefore, we will only be using the (x,y) position of the stars (not the pixel \"intensity\").')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing the number of stellar clusters in the provided images using cluster algorithms\n",
    "\n",
    "HINT: To speed up the procedure, you might want to comment out the algorithms which use a fixed number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 16))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': ...,\n",
    "                'eps': ...,\n",
    "                'damping': ...,\n",
    "                'preference': ...,\n",
    "                'n_neighbors': ...,\n",
    "                'n_clusters': ...,\n",
    "                'threshold': ...}\n",
    "\n",
    "for i_dataset, dataset in enumerate(datasets):\n",
    "    params = default_base.copy()\n",
    "\n",
    "    X = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=None, linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(\n",
    "        n_clusters=params['n_clusters'],threshold=params['threshold'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch)\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    print(\"> %-30s\" % PATH_cluster_files[i_dataset])\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "        plt.scatter(X[y_pred==-1,0], X[y_pred==-1,1], s=100, facecolors='none', edgecolors='black', color='black', label='background', alpha=0.1, zorder=0)\n",
    "        # marking background class for DBSCAN\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "        \n",
    "        n_clusters_ = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "        n_noise_    = list(y_pred).count(-1)\n",
    "\n",
    "        print(\"  %-25s | n_clusters %-3s | n_noise points %-3s\" % (name, n_clusters_, n_noise_))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Which algorithm performs best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Should  we be _also_ using the 3rd dimension, i.e. the pixel intensity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Application 3: Separate stellar types \n",
    "\n",
    "The [Morgan-Keenan spectral classification scheme](https://en.wikipedia.org/wiki/Stellar_classification) is a series of temperature, from the highest (O-type stars; 50-25kK) to the lowest (M-type; 3.5-2.5kK). Because of this, we see a developmet of different spectral lines as we move from the earliest to the latest spectral types. \n",
    "\n",
    "The strength of a spectral line can be measured by its equivalent width (EQW). The EQWs of the HeII 4200 line and HeI 4471 line can be used to separate stellar spectral types. The first one is a good indicator of **O-type** stars while the second is stronger in **early B-type** stars. The absence of both characterizes late **B-type stars**. \n",
    "\n",
    "&#9733; For more details see [Maravelias (2014), PhD thesis](http://skinakas.physics.uoc.gr/en/research/theses/GrMaraveliasPhD.pdf) and for a more elaborated work on the same topic see [Kyritsis et al. 2022, A&A, 657A, 62](https://ui.adsabs.harvard.edu/abs/2022A%26A...657A..62K/abstract).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/equivalent_width-wiki.jpg\"> \n",
    "Figure 12.1. Defining EQW: the width of a line with intensity equal to the local continuum and total flux equal to that of the line.   <br>\n",
    "(Credit: <a href=\"https://en.wikipedia.org/wiki/Equivalent_width\" \n",
    " target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia: Equivalent Width, by Szdori </a>)\n",
    "    </img>\n",
    "    </div>\n",
    "\n",
    "\n",
    "## The sample\n",
    " \n",
    "From [Evans et al. (2004)](http://adsabs.harvard.edu/abs/2004MNRAS.353..601E) we selected a sample of 697 OB stars with HeII 4200 and HeI 4471 line measurements.\n",
    "\n",
    "--- \n",
    "\n",
    "**TASK 1: Use K-means (or MiniBatchKMeans) to separate the stellar classes**\n",
    "\n",
    "**TASK 2: Plot results and highlight cluster centers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def flospecConv(arg):\n",
    "    \"\"\"\n",
    "    Function to convert from spectral types to \n",
    "    float numbers (e.g. B0,O9.5 to 20.0,19.5)\n",
    "    and backwards.\n",
    "    \"\"\" \n",
    "    try:\n",
    "        float(arg)\n",
    "        if str(arg)[0]=='1':\n",
    "            sp = 'O'\n",
    "        elif str(arg)[0]=='2':\n",
    "            sp = 'B'\n",
    "        elif str(arg)[0]=='3':\n",
    "            sp = 'A'\n",
    "        else:\n",
    "            sys.exit(' ! ERROR: more than O/B stars! Adjust conversion function.')\n",
    "        new_arg = sp+str(arg)[1:]\n",
    "    except ValueError:\n",
    "        if arg[0]=='O' or arg[0]=='o':\n",
    "            fl = '1'\n",
    "        elif arg[0]=='B' or arg[0]=='b':\n",
    "            fl = '2'\n",
    "        elif arg[0]=='A' or arg[0]=='a':\n",
    "            fl = '3'\n",
    "        else:\n",
    "            sys.exit(' ! ERROR: Check input! If more than O/B stars adjust conversion function.')\n",
    "        new_arg = float(arg.replace(arg[0],fl))\n",
    "\n",
    "    return new_arg\n",
    "\n",
    "# Reading the data file:\n",
    "\n",
    "PATH_data = \"data/stellar_types.dat\"\n",
    "\n",
    "stars=defaultdict(list)\n",
    "with open(PATH_data,'r') as inf:\n",
    "    for line in inf:\n",
    "        if line[0]!='#':\n",
    "            cols = line.split()\n",
    "            item = cols[0]\n",
    "            spline = cols[1]\n",
    "            if spline=='HeI/4471':\n",
    "                ewHeI = cols[2]\n",
    "                stars[item].append(ewHeI)\n",
    "            elif spline=='HeII/4200':\n",
    "                ewHeII = cols[2]\n",
    "                stars[item].append(ewHeII)\n",
    "\n",
    "# Creating data structures:\n",
    "\n",
    "sptype, flosptype, ewHeI, ewHeII = [], [], [], []\n",
    "for s in stars.keys():\n",
    "    sptype.append(s.split('-')[0])\n",
    "    flosptype.append(flospecConv(s.split('-')[0]))\n",
    "    ewHeII.append(float(stars[s][0]))\n",
    "    ewHeI.append(float(stars[s][1]))\n",
    "    \n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X = np.column_stack((ewHeII,ewHeI))\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"___________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' samples x ' + str(X.shape[1]) + ' diagnostics' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "\n",
    "fig = plt.figure(figsize=(12,10)\n",
    "                )\n",
    "scat = plt.scatter(ewHeII, ewHeI, c=flosptype, edgecolors='face', cmap=\"viridis\")\n",
    "cb = plt.colorbar(scat, ticks=[15,22,29])   # range of available spectral types\n",
    "cb.set_ticklabels(['O5','B2','B9'])\n",
    "cb.set_label('Spectral Types')\n",
    "\n",
    "plt.ylabel(r\"EQW of HeI $\\lambda$4471 \")\n",
    "plt.xlabel(r\"EQW of HeII $\\lambda$4200 \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: negative values could correspond to emission lines, although in reality these values are mostly affected by errors in the calculation of the EQW (e.g. not well-fitted continuum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running clustering algorithm and plotting\n",
    "\n",
    "HINT: Check sklearn documentation for KMeans to find how to easily retrieve the cluster centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.... import ...\n",
    "import matplotlib\n",
    "\n",
    "Clusters_kmeans = \n",
    "\n",
    "kmeans_model = KMeans(n_clusters=Clusters_kmeans, random_state=0)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "print(\"Cluster centers:\")\n",
    "print(kmeans_model....)\n",
    "\n",
    "cc_x = kmeans_model...\n",
    "cc_y = kmeans_model...\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.plot(cc_x, cc_y, 'k+', ms=100)\n",
    "\n",
    "new_map = matplotlib.cm.gray.from_list('whatever', ('blue', 'red'), N=Clusters_kmeans)\n",
    "scat2 = plt.scatter(ewHeII, ewHeI, c=kmeans_model.labels_, edgecolors='face', cmap=new_map)\n",
    "cb = plt.colorbar(scat2, ticks=range(0,Clusters_kmeans+1,1))   # number of clusters\n",
    "cb.set_ticklabels(range(1,Clusters_kmeans+2,1))\n",
    "cb.set_label('Cluster Label')\n",
    "\n",
    "plt.ylabel(r\"EQW of HeI $\\lambda$4471 \")\n",
    "plt.xlabel(r\"EQW of HeII $\\lambda$4200 \")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Change the number of clusters to 2, what do you notice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What can you say about the results if you start increasing the number of clusters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
