{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**Machine Learning Practises**</font>\n",
    "\n",
    "    or: \"How I Learned to Stop Worrying and Love the Black Box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will focus on fundamental principles of the **Machine Learning** (**ML**) approach.<br>\n",
    "We will:\n",
    "\n",
    "- formalize some **concepts** encountered in previous classes\n",
    "- outline procedural **protocols**\n",
    "- highlight **good practises** and common **mistakes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is **Machine Learning**?_\n",
    "<br>\n",
    "It is a branch of _Artificial Intelligence_ (_AI_) which regards the creation of algorithms that can <u>improve their performance through experience and data</u>.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ML_AI.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 0.  Relationship between ML and AI.\n",
    "            <br>\n",
    "            (From <a href=\"https://www.researchgate.net/figure/Relationship-between-artificial-intelligence-AI-machine-learning-ML-and-deep_fig1_338083201\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from the hard-coded approaches followed by traditional modelling, ML proposes a **train-application** scheme.\n",
    "\n",
    "The idea is that:\n",
    "- a machine (a computer) can learn the **parameters** of an arbitrarily complex model by **training** over some input **data**\n",
    "- once the model is trained, the machine can predict the response for previously **unseen data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ingredients for a successful ML model\n",
    "\n",
    "1. **Clean data**<br>\n",
    "    > _\"garbage in, garbage out\"_<br>\n",
    "    > _~any data analyst on the internet_\n",
    "\n",
    "2. **Test and training samples**\n",
    "    > _Divide et impera_\n",
    "\n",
    "3. **Metrics of performance**\n",
    "    > _\"Wait, is this a bug or a feature?\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=3, n_informative=1,\n",
    "                           n_redundant=1, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1, weights=None, flip_y=0.01,\n",
    "                           class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick visualization: corner plot with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corner plot:\n",
    "sns.pairplot(df_data, hue=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are:\n",
    "- redundant?\n",
    "- informative?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "We just need X$_{1}$ or X$_{2}$ for the classification of these data!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Correlation matrices\n",
    "\n",
    "We can see the _redundancy_ in a quantitative way, by calculating and displaying the **covariance** and the **correlation** between each pair of features.\n",
    "\n",
    "> **Covariance** $\\rightarrow Cov(X, Y)$ = ${{1}\\over{1-N}} \\sum_{i=1}^{N} (x_i - E(X))~(y_i - E(Y))$\n",
    ">\n",
    "> **Correlation** $\\rightarrow Cor(X, Y)$ = ${{Cov~(X, Y)}\\over{\\sigma_X ~ \\sigma_Y}}$ \n",
    "\n",
    "We can additionally plot the significance of the correlation, using the $p$-values from a Pearson correlation test (see Hypothesis Testing session!).  Using the `scipy.stats.personr` package ([link](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html)) words:\n",
    "\n",
    "\n",
    ">_The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "     \n",
    "# Covariance matrix:\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# Pearson's correlation and p-value matrices:\n",
    "#  See: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html\n",
    "cor   = np.zeros((np.shape(X)[1], np.shape(X)[1]))\n",
    "pvals = np.zeros((np.shape(X)[1], np.shape(X)[1]))\n",
    "\n",
    "for i in range(np.shape(X)[1]):\n",
    "    for j in range(np.shape(X)[1]):\n",
    "        cor[i, j], pvals[i, j] = pearsonr(X[:, i].ravel(), X[:, j].ravel())\n",
    "\n",
    "# Plotting -------------------------------------------------------------------\n",
    "\n",
    "# Configure a custom diverging colormap:\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Configure a custom diverging colormap:\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Converting matrices to dataframes for plotting with seaborn:\n",
    "def convert_M_to_pd(M, feature_names):\n",
    "    df = pd.DataFrame(data=M, columns=feature_names)\n",
    "    df = df.rename(index={i: feature_names[i] for i in range(len(feature_names))})\n",
    "    return df\n",
    "\n",
    "df_cov   = convert_M_to_pd(cov, feature_names)\n",
    "df_cor   = convert_M_to_pd(cor, feature_names)\n",
    "df_pvals = convert_M_to_pd(pvals, feature_names)\n",
    "\n",
    "# If you wish to mask the upper (redundant) traingle, uncomment this:\n",
    "# mask_cor = np.triu(np.ones_like(cor, dtype=bool))\n",
    "# mask_cov = np.triu(np.ones_like(cov, dtype=bool))\n",
    "mask_cor = None\n",
    "mask_cov = None\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 5]\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Covariance matrix\")\n",
    "sns.heatmap(df_cov, annot=True, mask=mask_cov, cmap=cmap)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Pearson's correlation matrix\")\n",
    "sns.heatmap(df_cor, annot=True, mask=mask_cor, cmap=cmap)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Correlation p-values matrix\")\n",
    "sns.heatmap(df_pvals, annot=True, mask=mask_cor, cmap=cmap)\n",
    "\n",
    "plt.show()\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of redundant features, we may apply a threshold on **covariance** or **correlation**, and remove one of the two redundant variables.\n",
    "\n",
    "However, watch out! These matrices only present the _linear_ relations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring datasets with many features\n",
    "\n",
    "Let's look at the same techniques seen above, when the number of features is untractable:\n",
    "\n",
    "<code>n_features</code> = 1000<br>\n",
    "<code>n_redundant</code> = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=1000, n_informative=700,\n",
    "                           n_redundant=300, n_repeated=0, n_classes=5,\n",
    "                           n_clusters_per_class=2, weights=None, flip_y=0.01,\n",
    "                           class_sep=3.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display only the last 7 features, since it is impractical to display them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This might take 10+ seconds to plot ...\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corner plot:\n",
    "g = sns.pairplot(df_data.iloc[:,-8:], hue=\"y\")\n",
    "g.fig.set_size_inches(10,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we cannot visualize the full correlation matrix, so we might just calculate it and **sort it** by correlation value.\n",
    "\n",
    "We might only print the significant correlations, e.g. |Cor| > 0.95.\n",
    "\n",
    "_Some other useful tricks can be found [here](https://likegeeks.com/python-correlation-matrix/)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cor = df_data[feature_names].corr()\n",
    "# NOTE1: This time we used a pandas tool to calculate the correlation\n",
    "#        matrix (while before we used the equivalent numpy tool) just\n",
    "#        to show an alternative method for the same thing.\n",
    "#        This returns a pandas dataframe.\n",
    "\n",
    "cor_pairs = df_cor.unstack()\n",
    "# converting correlation dataframe to 1D series\n",
    "\n",
    "cor_pairs_sort = cor_pairs.sort_values(kind=\"quicksort\")\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('Distribution of correlation coefficients')\n",
    "plt.hist(cor_pairs, bins=50)\n",
    "plt.show()\n",
    "\n",
    "df_sorted_pairs = pd.DataFrame(cor_pairs_sort).reset_index()\n",
    "df_sorted_pairs.columns=['feature_A', 'feature_B', 'Cor']\n",
    "\n",
    "# Let's exclude same-variable correlations, which are 1 by definition:\n",
    "df_nonsame = df_sorted_pairs[ df_sorted_pairs['feature_A'] != df_sorted_pairs['feature_B'] ]\n",
    "\n",
    "# Let's sort by absolute value:\n",
    "df_sorted = df_nonsame.sort_values(by='Cor', key=abs)\n",
    "\n",
    "# Let's keep the top 10 significant correlations:\n",
    "df_significant_5 = df_sorted.iloc[-5:, :].reset_index(drop=True)\n",
    "\n",
    "print('Top 5 Significant correlations (excluding same-variable):')\n",
    "display(df_significant_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ah! Why this time we cannot spot the redundant features?!**\n",
    "\n",
    "Meaning, why we do not see anything close to a 1-to-1 correlation, despite having requested `sklearn.datasets.make_classification` to include redundant features?\n",
    "\n",
    "This is because a \"redundant\" feature might be a **linear combination** of the informative features!\n",
    "\n",
    "<u>SUMMARY:</u> Life is more complicated than simple 1-to-1 relationships, unfortunately, so we cannot easily spot redundant features ...\n",
    "\n",
    "_PS: Additionally, remember that the correlation/covariance matrix is ill-defined (singular) for $n_{features}$ > $n_{samples}$.\n",
    "     A nice explanation can be found [here](https://stats.stackexchange.com/questions/60622/why-is-a-sample-covariance-matrix-singular-when-sample-size-is-less-than-number)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do, when we face a **large number** of features?\n",
    "> - Sit in the corner and cry\n",
    "> - Visualize data with some tricks (see [$\\S$Feature reduction](#Feature-reduction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden rule and data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Train_Validation_Test.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.  Indicative recipe for splitting a dataset into the analysis sets.<br>\n",
    "            For very large datasets (above 10$^5$ entries), the percentages of validation and test can be substantially smaller.\n",
    "            <br>\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "What are the different sets for? We already had a hint from the _Classification_ and _Clustering_ sessions.\n",
    "\n",
    "- **Train** set $~~~~~~\\rightarrow$ **Learn** the model parameters\n",
    "\n",
    "- **Validation** set $\\rightarrow$ Check that the learnt model is not **overfitting/underfitting** the train set\n",
    "\n",
    "- **Test** set $~~~~~~~\\rightarrow$ **Assess** the model performance\n",
    "\n",
    "Looks obvious. Right? Let's see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "Algorithm **Do_Classification**\n",
    "\n",
    "**Input**: **$X$** (data matrix), $y$ (labels), classifier<br>\n",
    "**Output**: classifier accuracy\n",
    "\n",
    "- - -\n",
    "> 1. **Normalize data**<br>\n",
    "> $\\mu_X$ $\\leftarrow$ $mean~$(**$X$**)<br>\n",
    "> $X_n$ = $X$ / $\\mu_X$\n",
    ">\n",
    ">\n",
    "> 2. **Split train/test**<br>\n",
    "> $indexes\\_train, indexes\\_test \\leftarrow split\\_indexes(~size(X)~)$<br><br>\n",
    "> $X_n\\_train \\leftarrow X_n~[indexes\\_train]$<br>\n",
    "> $X_n\\_test ~~\\leftarrow X_n~[indexes\\_test]$<br>\n",
    "> $y\\_train \\leftarrow~y~[indexes\\_train]$<br>\n",
    "> $y\\_test ~~\\leftarrow~y~[indexes\\_test]$<br>\n",
    ">\n",
    ">\n",
    "> 3. **Train on $train$ set**<br>\n",
    "> <code>classifier.fit</code>($X_n\\_train, y\\_train$)\n",
    ">\n",
    ">\n",
    "> 4. **Predict $test$ labels with trained classifier**<br>\n",
    "> $\\hat{y}\\_test$ $\\leftarrow$ <code>classifier.predict</code>($X_n\\_test$)\n",
    ">\n",
    ">\n",
    "> 5. **Calculate accuracy**<br>\n",
    "> $acc \\leftarrow calculate\\_accuracy~(\\hat{y}\\_test, y\\_test)$<br>\n",
    "> return $acc$\n",
    "- - -\n",
    "\n",
    "Where was the mistake?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>    \n",
    "    \n",
    "<table><tr>\n",
    "    <td width=1600>\n",
    "        <img src=\"images/Refit.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Space-time continuum is at risk when you use the test data during the <i>learning</i> process.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><center>**Golden Rule of Machine Learning**</center></font>\n",
    "<center>Treat the <b>test</b> data as if they come from the future!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the **Confusion Matrix** for a **binary** classification (the problem can be easily generalized to _multi_-class):\n",
    "\n",
    "**T**, **F** $\\rightarrow$ _real_ class\n",
    "\n",
    "**P**, **N** $\\rightarrow$ _predicted_ class\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Confusion_Matrix.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.  Confusion Matrix for a binary classification problem.\n",
    "            <br>\n",
    "            (From <a href=\"https://www.r-bloggers.com/2020/12/weighting-confusion-matrices-by-outcomes-and-observations/\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "By combining these quantities, we can create different **metrics of performance**, e.g.:\n",
    "\n",
    "**True Positive Rate (TPR)**:\n",
    "\n",
    "$$TPR = Recall = Sensitivity = {TP \\over TP + FN}$$\n",
    "\n",
    "**False Positive Rate (FPR)**:\n",
    "\n",
    "$$FPR = (1 - Specificity) = {FP \\over TN + FP}$$\n",
    "\n",
    "**Precision**:\n",
    "\n",
    "$${TP \\over TP + FP}$$\n",
    "\n",
    "**Accuracy**:\n",
    "\n",
    "$${TP + TN \\over P + N}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Q:** What **metric of performance** shall we maximize, to optimize our model parameters?<br>\n",
    "**R:** <u>It depends on the objective of the model.</u>\n",
    "\n",
    "For example, if we need: \n",
    "\n",
    " - to identify best follow-up candidates of GW sources, we want to waste no precious time on wrong targets\n",
    "\n",
    " _$\\rightarrow$ we want to minimize our contamination (FP) $\\rightarrow$ we care for **Precision**_\n",
    "\n",
    "\n",
    " - to create a catalog of OB stars, with conservative candidates for that class:\n",
    "\n",
    " _$\\rightarrow$ we want to minimize our rejections (FN) $\\rightarrow$ we care for **Recall**_ \n",
    "\n",
    "\n",
    "or just get a good mix of TP and TN across the classes $\\rightarrow$ _**Accuracy**_\n",
    "<br><br>\n",
    "\n",
    "- - -\n",
    "\n",
    "But there is more: we need to decide the **threshold**: i.e.,  the _probability score_ at which an object is classified as class 0 or 1.\n",
    "\n",
    "<br>\n",
    "<center><i>By default, we and <code>sklearn</code> classifiers assume <b>50%</b> as the score threshold</i></center>\n",
    "\n",
    "... but it can be changed!  For example, considering the previous scenarios:\n",
    "\n",
    " - to identify best follow-up candidates of GW sources, we want to waste no precious time on wrong targets\n",
    "\n",
    "  _$\\rightarrow$ **increase** threshold $\\rightarrow$ reduce contamination (FP), at the risk more rejections (FN)$\\rightarrow$ maximize **Precision** (but decrease **Recall**)_\n",
    "\n",
    "\n",
    " - to create a catalog of OB stars, with conservative candidates for that class:\n",
    "\n",
    "  _$\\rightarrow$ **decrease** threshold $\\rightarrow$ reduce rejections (FN), at the cost of contamination (FP) $\\rightarrow$ maximize **Recall** (but decrease **Precision**)_ \n",
    "\n",
    "Therefore, the  **metrics** change based on the chosen threshold.\n",
    "<br><br>\n",
    "\n",
    "**Q:** Can I visualize my classifier's expected behaviour when changing the threshold?<br>\n",
    "**R:** Yes, plot the **Receiver Operating Characteristic (ROC)** curve.\n",
    "\n",
    "The **ROC** shows the classifier's **TPR** and **FPR** across all possible classification thresholds.\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ROC.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.A.  Example of a Receiver Operating Characteristic (ROC) with an AUC of 0.79.\n",
    "            <br>\n",
    "            (From <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\"><code>sklearn</code> ROC page</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "- - -\n",
    "\n",
    "**Q:** Right, but if I have <u>multiple models</u> (each one with its ROC curve) is the _best_ model **over all thresholds**?!<br>\n",
    "**R:** We can claim it is the one that strikes the <u>best balance</u> between **TPR**s and **FPR**s  across all possible classification thresholds.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/ROCs.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2.B.  ROC curves for different classifiers.\n",
    "            <br>\n",
    "            (From <a href=\"https://cuitandokter.com/\"><code>sklearn</code> ROC page</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "The **Area Under the ROC Curve (AUC)** measures exaclty this.\n",
    "\n",
    "NOTE: \n",
    "\n",
    "- The 1-to-1 line in Figure 2 indicates a **random** classifier (in binary case, it means it randomly assigns class 1 or 0).<br>\n",
    "  For this reason, the minimal AUC is _de facto_ 0.5.\n",
    "- The **furthest** from the 1-to-1 line, the **better** the classifier.\n",
    "- The **ideal** classifier has AUC = 1.\n",
    "\n",
    "For more info about the interpretation of the AUC, you can read [this page](https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077) and [this post on Google Developers](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3><u>**Is AUC a metric superseeding all the others?**</u><font>\n",
    "\n",
    "- In general: <u>no</u>, they measure different things\n",
    "    \n",
    "  _AUC gives performance across all thresholds, but in practice you you pick **one** threshold when you use the model_  \n",
    "  \n",
    "\n",
    "    \n",
    "- If your classes are heavily unbalanced: <u>probably</u>\n",
    "\n",
    "  _In this case it is possible that you don't know the right threshold anyways_\n",
    "    \n",
    "You can find an interesting discussion about this topic [here](https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression problems, we have to compare a numerical $y$ against its predicted value $\\hat{y}$.\n",
    "\n",
    "Among the most used metrics are those which:\n",
    "\n",
    "- evaluate the **residuals**, e.g.:<br><br>\n",
    "\n",
    "  - **Mean Absolute Error (MAE)**\n",
    "$$MAE = {1 \\over N}\\sum_i^N{|  y_i-\\hat{y_i} |}$$\n",
    "\n",
    "  - **Mean Squared Error (MSE)**\n",
    "$$MSE = {1 \\over N}\\sum_i^N{(y_i-\\hat{y_i})^2}$$\n",
    "\n",
    "- evaluate the [expected linear] **correlation**, e.g.:<br><br>\n",
    "  - **Coefficient of Determination (R^2)**\n",
    "$$R^2 = 1 - {\\sum_i^N{(y_i-\\hat{y_i})^2} \\over \\sum_i^N{(\\bar{y_i} - y_i)^2}} $$, \n",
    "\n",
    "\n",
    "<font size=3><u>**Warning about relative metrics**</u><font>\n",
    "\n",
    "Most of metrics based on residuals are \"<u>absolute</u>\".\n",
    "    \n",
    "When your $y$ has a large dynamic range, you would be tempted to use <u>relative</u> metrics, e.g. **Mean Absolute Percentage Error (MAPE)**:\n",
    "    \n",
    "$$MAPE = {100 \\over N}\\sum_i^N{\\bigg|  {y_i-\\hat{y_i} \\over y_i} \\bigg|}$$\n",
    "\n",
    "But be careful! For values of $y \\sim 0$ these metrics _might_ explode! $\\rightarrow$ they compromize the convergence of algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn metrics\n",
    "\n",
    "You can find a list of <code>sklearn</code> metrics for **Classification**, **Regression**, and **Clustering** at [this docs page](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x1 = np.random.randint(10, size=10)\n",
    "x2 = np.random.randint(10, 20, size=10)\n",
    "y = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "df_data = pd.DataFrame(np.array([x1, x2, y]).T, columns=['x1', 'x2', 'class'])\n",
    "\n",
    "display(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df_data[['x1','x2']].values\n",
    "y = df_data[['class']].values.ravel()\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('LogisticRegression | accuracy: %.2f' % clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this result?\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>    \n",
    "    \n",
    "A **Majority Class** classifier, i.e. a _trivial_ classifier which always returns the most frequent class seen in training, would have scored 90% accuracy!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Always compare against dummy estimators!**</u><font>\n",
    "\n",
    "... or at least keep in mind the minimum achievable score $=$ **baseline**.  Having a comparison reference is <u>very important</u> when publishing, to convince about the goodness of your model.\n",
    "    \n",
    "In some cases, the baseline can also be found **theoretically**, it does not matter. \n",
    "\n",
    "Some examples of dummy estimators:\n",
    "    \n",
    " - <u>Classification</u>:\n",
    "   - **Random (Trivial) Classifier**: a classifier which returns a random class\n",
    "   - **Stratified Classifier**: a classifier which returns a class with a probability following the distribution of classes seen in training\n",
    "    \n",
    "    [<code>sklearn</code> Dummy Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n",
    "    \n",
    "        \n",
    " - <u>Regression</u>:\n",
    "   - **Constant Regressor**: a regressor which alwyas returns a value, e.g. the mean($y_{train}$) or median($y_{train}$)\n",
    "    \n",
    "    [<code>sklearn</code> Dummy Regressors](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n",
    "\n",
    "Create your own based on the problem you are addressing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding (Sometimes a necessity!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms can natively deal with all data types (e.g. **Random Forests**), but others only expect numerical inputs.\n",
    "\n",
    "**_Encoding_** is a method to deal with **categorical** data as if they were binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([], columns=['Galaxy', 'Morphology', 'Something'])\n",
    "df['Galaxy']     = ['NGC4261', 'NGC4452', 'M31', 'M51', 'Hoag']\n",
    "df['Morphology'] = ['elliptical', 'elliptical', 'spiral', 'spiral', 'ring']\n",
    "df['Something']  = [10, 65, 78, 98, 35]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with categorical data which are **not ordinal**, i.e. they don't have a specific order, such as \"_Morphology_\"?\n",
    "\n",
    "If we were to attribute **integer** values, e.g.:\n",
    "\n",
    "- elliptical = 0\n",
    "- spiral$~~~~$ = 1\n",
    "- ring$~~~~~~~$= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ordinal = pd.DataFrame([], columns=['Galaxy', 'Morphology', 'Something'])\n",
    "df_ordinal['Galaxy']     = ['NGC4261', 'NGC4452', 'M31', 'M51', 'Hoag']\n",
    "df_ordinal['Morphology'] = ['0', '0', '1', '1', '2']\n",
    "df_ordinal['Something']  = [10, 65, 78, 98, 35]\n",
    "\n",
    "display(df_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would get into <u>trouble</u> when using algorithms that would consider the \"distance\" between two consecutive classes.<br>\n",
    "In this case, an algorithm will see '_elliptical_' closer to '_spiral_' than to '_ring_', for no real reason.\n",
    "\n",
    "- - -\n",
    "\n",
    "A solution is **One Hot Encoding (OHE)**!\n",
    "\n",
    "> OHE consists in creating arrays of **0** and **1**, which uniquely identify the class.\n",
    "\n",
    "We can do this with <code>pandas</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['Morphology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. or with <code>sklearn</code>.<br>\n",
    "It is a bit more complex but useful when we want to apply the **same** encoding to an other set, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "X_train = [['elliptical'], ['elliptical'], ['spiral'], ['spiral'], ['ring']]\n",
    "# NOTE: sklearn methods always want a 2D input\n",
    "\n",
    "X_train_ohe = enc.fit_transform(X_train).toarray()\n",
    "\n",
    "print('Encoding of input data:\\n\\n%s\\n' % X_train_ohe)\n",
    "\n",
    "# Now converting a new example - say we want to convert \"spiral\":\n",
    "\n",
    "X_test = [['spiral']]\n",
    "\n",
    "X_test_ohe = enc.transform(X_test).toarray()\n",
    "\n",
    "print('Encoding of test data (\"spiral\"):\\n\\n%s\\n' % X_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Warning: OHE creates co-linearity!**</u><font>\n",
    "\n",
    "This is because we can deduce one class given the others, e.g., if:\n",
    "    \n",
    "    Morphology_elliptical = 0 and Morphology_ring = 0\n",
    "\n",
    "then, necessarily:\n",
    "    \n",
    "    Morphology_spiral = 1\n",
    "    \n",
    "$\\rightarrow$ we are introducing a correlation between the \"dummy\" features.\n",
    " \n",
    "This is an issue for algorithms which assume **feature independence** (e.g. _linear models_, _Logistic Regression_, _Naive Bayesian Classifier_).\n",
    "\n",
    "<br>\n",
    "    \n",
    "<font size=3><u>**To drop or not to drop?**</u><font>\n",
    "    \n",
    "We can bypass this issue by **dropping** one dummy class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['Morphology'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are _not_ losing information because we can always reconstruct the third class given the other 2.\n",
    "\n",
    "**But** ... this opens lots of debates $-$ e.g.:\n",
    "- _Which class shall I drop?_\n",
    "- _How do I interpret my results?_\n",
    " \n",
    "This post presents a comprehensive [discussion on dropping OHE columns](https://inmachineswetrust.com/posts/drop-first-columns/).\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3><u>**TL;DR**</u><font>\n",
    "\n",
    "Most times you should probably be fine <u>without</u> dropping columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling features\n",
    "\n",
    "NOTE: \"Scaling\" shall be interpreted as a generic form of **normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do you need to normalize?\n",
    "\n",
    "- Normalizing is <u>fundamental</u> when adopting algorithms that make use of **distance**.\n",
    "\n",
    "    _Example: **SVM**, **$k$NN**, or some **hierarchical clustering**_<br>\n",
    "    $k$NN Measures the distance between a _test_ point $\\hat{X}$ and all the _training_ points $X_i$: e.g. || $\\hat{X}$ -  ${X_i}$ ||$^2$\n",
    "  \n",
    "  \n",
    "- Normalizing is <u>fundamental</u> when adopting algorithms that **compare** the contribution of **features**.\n",
    "\n",
    "     _Example: **PCA**_<br>\n",
    "     PCA requires to scale and center features around 0 [$\\S$Feature reduction](#Feature-reduction)\n",
    "\n",
    "\n",
    "- Normalizing <u>is not necessary</u> with algorithms which look at each feature **independently**\n",
    "\n",
    "    _Example: **Random Forests**_<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "... However, normalizing features is usually a good habit because:\n",
    "\n",
    "- Scaling <u>helps</u> convergence when features have very different **dynamic ranges**.\n",
    " \n",
    "    _Example: algorithms using **gradient descent** (including **neural networks**)_<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Example**</u><font>\n",
    "\n",
    "Let's try to see what happens to the accuracy when we try a $k$NN classifier **with** and **without** normalizing.\n",
    "    \n",
    "> 3 classes, 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Let's create the usual mock data:\n",
    "X, y = make_classification(n_samples=1000, n_classes=3, n_features=2, n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, class_sep=2, flip_y=0.5,\n",
    "                           shift=3, scale=None, random_state=42)\n",
    "# NOTE1: We set \"scale\" to \"None\" to generate randomly scale features!\n",
    "# NOTE2: flip_y=0.5 makes the classification more difficult!\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Let's fit directly:\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Accuracy on non-normalized data: %.2f' % clf.score(X_test, y_test))\n",
    "\n",
    "# Now, let's normalize with respect to the max of each feature, and re-fit:\n",
    "scaler = MinMaxScaler()\n",
    "X_train_n = scaler.fit_transform(X_train)\n",
    "X_test_n  = scaler.transform(X_test)\n",
    "\n",
    "clf_n = KNeighborsClassifier(n_neighbors=10)\n",
    "clf_n.fit(X_train_n, y_train)\n",
    "\n",
    "print('Accuracy on normalized data: %.2f' % clf_n.score(X_test_n, y_test))\n",
    "\n",
    "# Plotting decision boundaries:\n",
    "# See:\n",
    "#    https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf, X_train, cmap=cmap_light, ax=axes[0], response_method=\"auto\")\n",
    "\n",
    "axes[0].scatter(X_train[:,0], X_train[:,1], c=y_train, alpha=0.5)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf_n, X_train_n, cmap=cmap_light, ax=axes[1], response_method=\"auto\")\n",
    "\n",
    "axes[1].scatter(X_train_n[:,0], X_train_n[:,1], c=y_train, alpha=0.5)\n",
    "\n",
    "axes[0].set_title('Decision boundaries on native X_train')\n",
    "axes[1].set_title('Decision boundaries on normalized X_train')\n",
    "\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "axes[1].set_xlabel('X1$_n$')\n",
    "axes[1].set_ylabel('X2$_n$')\n",
    "\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of scalers\n",
    "\n",
    "Theoretically, infinite.\n",
    "\n",
    "The <code>sklearn</code> library implements several scalers:\n",
    "- check this [showcase of <code>sklearn</code>scalers](https://scikit-learn.org/stable/modules/preprocessing.html) \n",
    "- check the [effects of different scalers on outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) for their effects on oultiers\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1600>\n",
    "        <img src=\"images/Scalers_Showcase.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3. A showcase of the effect of different <code>sklearn</code> scalers applied to different distributions.<br>\n",
    "            (From <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: do I need to normalize $y$ ?\n",
    "\n",
    "In regression problems, $y$ is a **continuous** variable (i.e., not a _label_, as for classification).\n",
    "\n",
    "Shall we normalize it?\n",
    "\n",
    "> **Depends on the specific problem** $\\rightarrow$ try it out.<br>\n",
    "> _e.g., you might want to fit better some ranges or equally at all ranges_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature construction (dipoles, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Kaggle's [Stellar Classification Dataset - SDSS17](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_star = pd.read_csv(\"data/star_classification.csv\")\n",
    "df_star = df_star.sample(n=100, random_state=12)\n",
    "# keeping only a few objects to make the problem more difficult\n",
    "\n",
    "#display(df_star.head(5))\n",
    "\n",
    "print('Class demographics:')\n",
    "display(df_star.groupby(['class']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let's only use a few features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = df_star[['u', 'g', 'r', 'i', 'z', 'redshift']]\n",
    "y = np.array(df_star[['class']]).ravel()\n",
    "\n",
    "display(X.head(5))\n",
    "\n",
    "print('Number of features in X:', np.shape(X)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try a simple **Random Forests** classifier on the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Agnostic feature construcions**</u><font>\n",
    "\n",
    "There are many _agnostic_ methods, but one basic feature construction is to create **all polynomial combinations** of the features.\n",
    "\n",
    "E.g., the polynomials of degree 2 are:\n",
    "    \n",
    "> $X_1,~X_1^2,~X_1\\cdot{}X_2,~X_2^2,~X_2,$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ext = pd.DataFrame(poly.fit_transform(X))\n",
    "\n",
    "display(X_ext.head(5))\n",
    "\n",
    "print('New number of features in X:', np.shape(X_ext)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ext, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ _That's some improvement (we got 0.83, before)!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Prior knowledge feature construction**</u><font>\n",
    "\n",
    "What we did before by creating polynomial was to **relate** _magnitudes_ ('u', 'g', 'r', 'i', 'z') with each other and with a _distance_ ('redshift').\n",
    "    \n",
    "\n",
    "But, since we are cool and smart _Astronomers_, we could as well used our Astrophysics intuition to create **colors**, which we know are distance-_invariant_ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "df_mags = df_star[['u', 'g', 'r', 'i', 'z']]\n",
    "\n",
    "cc = list(combinations(df_mags, 2))\n",
    "df_colors = pd.concat([df_mags[c[0]].sub(df_mags[c[1]]) for c in cc], axis=1, keys=cc)\n",
    "df_colors.columns = df_colors.columns.map('-'.join)\n",
    "\n",
    "X_ext = df_colors\n",
    "\n",
    "display(X_ext.head(5))\n",
    "\n",
    "print('New number of features in X:', np.shape(X_ext)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ext, y, test_size=0.3, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('==> Accuracy: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Argh!_ We got a worst result than using the magnitudes themselves (0.83)!<br>\n",
    "_Well, Astronomy intuition is overrated, anyways!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different algorithms based on how many assumptions we want to make about the data.\n",
    "\n",
    "<font size=3><u>**Based solely on data properties**</u><font>\n",
    "    \n",
    "E.g., the <code>sklearn</code> selector <code>SlectKBest</code> allows to keep the best $k$ features based on some **statistical tests**.\n",
    "\n",
    "For example, we can use the $\\chi^2$ test to evaluate the dependency of $y$ from each feature, and select the top 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=1,\n",
    "                           n_redundant=1, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1, weights=None, flip_y=0.01,\n",
    "                           class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "X = np.abs(X)\n",
    "# NOTE: This is because the Chi2 test is assumed to work on counts or frequencies,\n",
    "#       hence it expects only positive values\n",
    "\n",
    "feature_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "X_reduced = feature_selector.fit_transform(X, y)\n",
    "\n",
    "# NOTE: Conveniently, the sklearn feature selectors have the same exact methods\n",
    "#       as the classifiers, regressors, or any other sklearn estimator:\n",
    "#         .fit()           -> learns the transformation\n",
    "#         .transform()     -> applies the transformation\n",
    "#         .fit_transform() -> learns and applies the transformation\n",
    "\n",
    "print('Selected features: ', feature_selector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Based on some performance**</u><font>\n",
    "    \n",
    "We want to keep the features which yield the best **performance**.<br>\n",
    "Yes, but which performance? $\\rightarrow$ the one of **an estimator of our choice**!\n",
    "(_e.g., for a classification problem, we need to pick a classifier._)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see the case of a **Sequential Selector**, and more specifically, of a **_forward_** feature selector, applied to a classification problem. \n",
    "\n",
    "**Sequential Forward Selection (SFS) algorithm:**\n",
    "\n",
    "The algorithm has to pick $p$ features among a set of input features $X = \\{x_1, x_2, ... x_d\\}$.<br>\n",
    "Let's call $J(F)$ the score of the classifier when applied to the feature subset $F \\subset X$.\n",
    "\n",
    "> 1. Start from an empty set: $Y_k\\{\\emptyset\\}$, $k=0$\n",
    ">\n",
    ">\n",
    "> 2. Select the best left-out feature:\n",
    ">\n",
    ">    $x^+$ = $arg max J(Y_k + x)$ where $x \\in X - Y_k$<br>\n",
    ">    _This is the feature that, when added, maximises the score J of the classifier_\n",
    "> \n",
    ">    $Y_{k+1} = Y_k + x^+$<br>\n",
    ">    _Add the selected feature to the pool_\n",
    ">\n",
    ">    $k = k + 1$<br>\n",
    ">    _Increase the counter_\n",
    ">\n",
    ">\n",
    "> 3. Stop if $k = p$, else go to 2\n",
    "\n",
    "The **_backward_** feature selector works similarly ...\n",
    "\n",
    "The Python library <code>mlxtend</code> contains several **Sequential Selectors**,  ([docs](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/); but check also their other algorithms [link](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/)).\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try <code>mlxtend</code> SFS using a <code>sklearn</code> classifier to evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "sfs = SFS(clf, k_features=3, forward=True, floating=False, verbose=2, scoring='accuracy', cv=0)\n",
    "\n",
    "sfs = sfs.fit(X, y)\n",
    "# notice how mlxtend seamlessly integrates with sklearn ...\n",
    "\n",
    "print('Selected features indexes: ', sfs.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Final remarks on feature selection**</u><font>\n",
    "    \n",
    "- Remember the **Golden Rule** $\\rightarrow$ Use only the **training set** to select (_not_ the _test_, for God's sake!)!\n",
    "\n",
    "- Shall I use a different classifier (estimator) to **select features** and to **classify**? $\\rightarrow$ Depends who you ask, both valid.\n",
    "\n",
    "- Sequential Selection is **computationally expensive** (looks at many feature combinations) $\\rightarrow$ May bu unfeasible for large datasets.\n",
    "\n",
    "<font size=3><u>**A much more advanced agorithm**</u><font>\n",
    "    \n",
    "[-] [Bourboudakis & Tsamardinos, 2017](https://arxiv.org/abs/1705.10770): \"_Forward-Backward Selection with Early Dropping_\"<br>\n",
    "\n",
    "> _Made in University of Crete!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, _visualizing multidimensonal data_ / _feature selection_ becomes quickly **untractable** as the number of features grows.\n",
    "\n",
    "How can we get an hint of, e.g. **outliers** and clusters?\n",
    "\n",
    "We can use tools that allow to \"summarize\" the features into a lower-dimensional space (**embedding**).<br>\n",
    "For example, we can reduce to 2 or 3 dimensions, so that we can directly visualize the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a \"easy\" dataset with lots of features ...\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=1000, n_informative=900,\n",
    "                           n_redundant=100, n_repeated=0, n_classes=5,\n",
    "                           n_clusters_per_class=1, weights=None, flip_y=0.01,\n",
    "                           class_sep=3.5, hypercube=True, shift=0.0, scale=1.0,\n",
    "                           shuffle=True, random_state=42)\n",
    "# NOTE: <class_sep> sets how far apart are the clusters: larger values produce\n",
    "#       easier sets, in the sense that they are more easily separable.\n",
    "\n",
    "classes = np.unique(y)\n",
    "print('There are %s classes' % len(classes))\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "feature_names = [\"X\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Linear methods: PCA**</u><font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Principal Component Analysis_** (**PCA**) is an unsupervised method which finds the directions (**Principal Components; PCs**) which maximize the variance of the data.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=1024>\n",
    "        <img src=\"images/PCA.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.  Detection of Principal Components ($b$) and reduction of dimensionality from 3D to 2D  ($c$).\n",
    "            <br>\n",
    "            (From <a href=\"https://www.davidzeleny.net/anadat-r/doku.php/en:pca\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The algorithm essentially **diagonalizes** at the covariance matrix: we <u>must</u> normalize the features or else the ones with the largest _dynamic range_ will dominate!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_n = StandardScaler().fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we can apply the PCA decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "embedding = pca.fit(X_n).transform(X_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how our 100-dimensional data look like when reduced to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fontsize_first  = 16\n",
    "fontsize_second = 12\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "axes[0].set_title('Data embedded via PCA', fontsize=fontsize_first)\n",
    "axes[0].set_xlabel('PC1', fontsize=fontsize_second)\n",
    "axes[0].set_ylabel('PC2', fontsize=fontsize_second)\n",
    "img = axes[0].scatter(embedding[:, 0], embedding[:, 1], s=20, c=y, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(axes[0], xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(img, boundaries=np.arange(len(classes)+1)-0.5, ax=axes[0])\n",
    "cbar.set_ticks(np.arange(len(classes)))\n",
    "cbar.set_ticklabels(classes)\n",
    "\n",
    "ax2 = axes[1].twinx()\n",
    "axes[1].set_title('PC importance', fontsize=fontsize_first)\n",
    "axes[1].set_xlabel('PC index', fontsize=fontsize_second)\n",
    "axes[1].set_ylabel('Explained variance [%]', fontsize=fontsize_second)\n",
    "ax2.set_ylabel('Explained variance (cumulative) [%]', fontsize=fontsize_second, color='rebeccapurple')\n",
    "axes[1].bar(np.arange(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_*100, width=0.8)\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_)*100, color='rebeccapurple')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty is: the PCs are **sorted** by importance!\n",
    "\n",
    "We can look at the Explained variance and decide ow many to keep!<br>\n",
    "_E.g. the first **k** components that together explain 50% of the variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Non-linear methods: $t$-SNE and UMAP**</u><font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most known techniques are arguably:\n",
    "> - **t-SNE** (_**t**-distributed **S**tochastic **N**eighbor **E**mbedding_)\n",
    "> - **UMAP** (_**U**niform **M**anifold **A**pproximation and **P**rojection_)\n",
    "\n",
    "They are **iterative**, **unsupervised** algorithms (although UMAP can be also used in **supervised** manner).\n",
    "\n",
    "Their rationale briefly consists in\n",
    "\n",
    "> _reducing to a lower-dimensional feature space (with a non-linear transformation)_,\n",
    "\n",
    "while\n",
    "\n",
    "> _arranging similar samples close to each other, and the dissimilar ones far away from each other_,\n",
    "\n",
    "by assessing\n",
    "\n",
    ">  _the similarity between data points in the original and in the final space._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having said that, **UMAP** surclasses **$t$-SNE** in all aspects $\\rightarrow$ it is mentioned because you may encounter it, but use **UMAP**.\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/tSNE_is_dead.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Current existential status of $t$-SNE.\n",
    "            <br>\n",
    "            (From <a href=\"https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see UMAP in action with the [Galaxy10SDSS](https://astronn.readthedocs.io/en/latest/galaxy10sdss.html) dataset (200 MB)!\n",
    "\n",
    "(A more complete example with this dataset can be found [here](https://umap-learn.readthedocs.io/en/latest/auto_examples/galaxy10sdss.html#sphx-glr-auto-examples-galaxy10sdss-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "script_path = str(Path().absolute())\n",
    "print('Current path:', script_path)\n",
    "\n",
    "path_data = script_path + '/data'\n",
    "\n",
    "# Downloading if dataset not found:\n",
    "if not os.path.isfile(path_data+\"/Galaxy10.h5\"):\n",
    "    url = \"http://astro.utoronto.ca/~bovy/Galaxy10/Galaxy10.h5\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(path_data+\"/Galaxy10.h5\", \"wb\").write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type: images (21785 images)\n",
    "\n",
    "Classes<br>\n",
    " Class 0 (3461 images): Disk, Face-on, No Spiral<br>\n",
    " Class 1 (6997 images): Smooth, Completely round<br>\n",
    " Class 2 (6292 images): Smooth, in-between round<br>\n",
    " Class 3 (394 images): Smooth, Cigar shaped<br>\n",
    " Class 4 (1534 images): Disk, Edge-on, Rounded Bulge<br>\n",
    " Class 5 (17 images): Disk, Edge-on, Boxy Bulge<br>\n",
    " Class 6 (589 images): Disk, Edge-on, No Bulge<br>\n",
    " Class 7 (1121 images): Disk, Face-on, Tight Spiral<br>\n",
    " Class 8 (906 images): Disk, Face-on, Medium Spiral<br>\n",
    " Class 9 (519 images): Disk, Face-on, Loose Spiral<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loading catalogue:\n",
    "import h5py\n",
    "\n",
    "with h5py.File(path_data+\"/Galaxy10.h5\", \"r\") as F:\n",
    "    images = np.array(F[\"images\"])\n",
    "    y      = np.array(F[\"ans\"])\n",
    "    \n",
    "# Dropping class 5, which only has 17 galaxies:\n",
    "idxs_valid = np.where(y != 5)[0]\n",
    "images = images[idxs_valid]\n",
    "y      = y[idxs_valid]\n",
    "    \n",
    "print('Image array shape: %s\\n' % str(np.shape(images)))\n",
    "\n",
    "# Flattening images to 1D arrays (and casting to float):\n",
    "X = images.reshape([len(images), np.shape(images)[1]*np.shape(images)[2]*np.shape(images)[3]])\n",
    "X = X.astype(np.float32)\n",
    "# TRICK: These are RGB images (integer values 0--256): we can convert to a\n",
    "#        low-encoding float (16 instead of e.g. 64 bits) without losing information.\n",
    "\n",
    "classes = sorted(np.unique(y))\n",
    "print('There are %s classes' % len(classes))\n",
    "\n",
    "feature_names = [\"pixel\"+str(i) for i in range(np.shape(X)[1])]\n",
    "\n",
    "# Converting data to dataframe:\n",
    "df_data_orig = pd.DataFrame(data=np.append(X, y[:,None],1), columns=feature_names+['y'])\n",
    "\n",
    "# Balancing classes using pandas:\n",
    "#\n",
    "# For each class, keeping as many objects as for the class with min number\n",
    "# of objects ...\n",
    "df_data_g = df_data_orig.groupby('y')\n",
    "df_data = pd.DataFrame(df_data_g.apply(lambda x: x.sample(df_data_g.size().min()).reset_index(drop=True)))\n",
    "# and re-exctracting the matrices after rebalancing ...\n",
    "X = df_data[feature_names].values\n",
    "y = df_data['y'].values\n",
    "\n",
    "print('Each class contains ~%s galaxies' % int(round(len(y)/len(classes))))\n",
    "\n",
    "# Keeping only a [random] subset of the data to speed up calculations:\n",
    "n_samples_keep = 1000 # max: len(X)\n",
    "idxs_keep = np.random.randint(len(X), size=n_samples_keep)\n",
    "X = X[idxs_keep]\n",
    "y = y[idxs_keep]\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data shape')\n",
    "table.field_names = ['X', 'y']\n",
    "table.add_row([np.shape(X), np.shape(y)])\n",
    "print(table)\n",
    "\n",
    "# Displaying first 5 rows:\n",
    "display(df_data.head(5))\n",
    "\n",
    "# Displaying one image for each class:\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, len(classes), figsize=(14, 5))\n",
    "plt.suptitle('Galaxy class examples', y=0.7, fontsize=14)\n",
    "\n",
    "for i, classs in enumerate(classes):\n",
    "    idx = np.where(y == classs)[0][0]\n",
    "    # index of first galaxy in the list, for class <classs>\n",
    "    ax = axes[i]\n",
    "    ax.set_title('Class: %s' % classs)\n",
    "    ax.imshow(images[idx])\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For 1000 objects, this will take half a minute or so ...\n",
    "import umap\n",
    "\n",
    "mapper = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.1, metric='euclidean')\n",
    "embedding = mapper.fit_transform(X)\n",
    "# NOTE: using default params\n",
    "\n",
    "# For supervised transformation, add y (and witness the magic!):\n",
    "#mapper = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.1, metric='euclidean')\n",
    "#embedding = mapper.fit_transform(X, y=y)\n",
    "# ... but for now we are only displaying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Hyperparameters**</u><font>\n",
    "> **n_neigbors** is the most important parameter $-$ it tells how much _locally_ the algorithm shall focus:\n",
    "> - small n_neigbors $\\rightarrow$ UMAP will produce plenty of localized clusters\n",
    "> - large n_neighbors $\\rightarrow$ UMAP will look at the global structure\n",
    "\n",
    "> **min_dist**: minimum distance between embedded points:\n",
    "> - small min_dist $\\rightarrow$ more clustered/clumped embedding\n",
    "> - large min_dist $\\rightarrow$ more even dispersal of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_UMAP(X, y, embedding, title='UMAP projection'):\n",
    "    \n",
    "    n_components = np.shape(embedding)[1]\n",
    "    \n",
    "    fontsize_title = 14\n",
    "    cmap = 'Spectral_r'\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    ax0 = fig.add_subplot(121,  projection='3d')\n",
    "    ax0.set_title('First 3 dimensions of data', fontsize=fontsize_title)\n",
    "    img0 = ax0.scatter(np.log10(X[:,0]+1), np.log10(X[:,1]+1), np.log10(X[:,2]+1), c=y, s=100, alpha=0.5, cmap=cmap)\n",
    "    # NOTE: The \"+ 1\" is to avoid issues with log(0) \n",
    "    ax0.set_xlabel('feature 1 [log]')\n",
    "    ax0.set_ylabel('feature 2 [log]')\n",
    "    ax0.set_zlabel('feature 3 [log]')\n",
    "    \n",
    "    if n_components == 1:\n",
    "        ax1 = fig.add_subplot(122)\n",
    "        img1 = ax1.scatter(embedding[:,0], range(len(embedding)), c=y, cmap=cmap)\n",
    "    if n_components == 2:\n",
    "        ax1 = fig.add_subplot(122)\n",
    "        img1 = ax1.scatter(embedding[:,0], embedding[:,1], c=y, cmap=cmap, alpha=0.7)\n",
    "    if n_components == 3:\n",
    "        ax1 = fig.add_subplot(122, projection='3d')\n",
    "        img1 = ax1.scatter(embedding[:,0], embedding[:,1], embedding[:,2], c=y, s=100, cmap=cmap)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=fontsize_title)\n",
    "    ax1.set_xlabel('UMAP feature 1')\n",
    "    ax1.set_ylabel('UMAP feature 2')\n",
    "    \n",
    "    cbar = plt.colorbar(img1, boundaries=np.arange(len(classes)+1)-0.5)\n",
    "    cbar.set_ticks(np.arange(len(classes)))\n",
    "    cbar.set_ticklabels(classes)\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "plot_original_vs_UMAP(X, y, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**What we observe:**</u><font>\n",
    "\n",
    "- The different classes are mildly separated, but anyways more separated than in the original data\n",
    "\n",
    "- UMAP creates an embedding space with smooth transitions from a class to the next<br>\n",
    "\n",
    "_This welcome since contiguous morphological classes are indeed similar (i.e., there is a smooth transition from class 0, to 1, to 2, etc.)_\n",
    "\n",
    "Notice that, here, UMAP did <u>not</u> use the information about the labels: we added the colors only in the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**UMAP mapping is reversible!**</u><font>\n",
    "\n",
    "Given an embedded point (x, y), we can reconstruct a fake image by inverse transformation!<br>\n",
    "We expect this fake galaxy to look like the dominant class nearby the chosen point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_point = (1, 1)\n",
    "embedding_point = np.array([embedding_point])\n",
    "X_fake = mapper.inverse_transform(embedding_point)\n",
    "# NOTE: scikit-learn transformations always return 2D arrays, even when there\n",
    "#       is only 1 object\n",
    "\n",
    "images_fake = X_fake.reshape([len(embedding_point), np.shape(images)[1], np.shape(images)[2], np.shape(images)[3]])\n",
    "\n",
    "# Normalizing for RGB representation:\n",
    "mins = np.min(images_fake[:,:,:,:],axis=(1,2))\n",
    "maxs = np.max(images_fake[:,:,:,:],axis=(1,2))\n",
    "images_fake_n = (images_fake - mins)/(maxs - mins)\n",
    "\n",
    "plt.title('A synthetic image!')\n",
    "plt.imshow(images_fake_n[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voil: your trained UMAP is actually a galaxy class image generator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Sensitivity to hypepars**</u><font>\n",
    "\n",
    "Let's see how UMAP behaves when varying the **n_neighbors** parameter.<br>\n",
    "\n",
    "WARNING: Computational time rapidly grows with n_neighbors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# For 1000 objects, this will take couple of minutes or so ...\n",
    "\n",
    "import umap\n",
    "\n",
    "# Suggested range, from the documentation: 2 to 100\n",
    "for n_neighbors in (2, 5, 8, 12, 20, 100):\n",
    "    embedding = umap.UMAP(n_neighbors=n_neighbors, n_components=2, min_dist=0.1, metric='euclidean').fit_transform(X)\n",
    "    plot_original_vs_UMAP(X, y, embedding, 'UMAP projection | n_neighbors=%s' % n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose the right n_neighbors?\n",
    "It depends on the task, but $\\rightarrow$ see [Hyperparameter tuning](#hyperparameter_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Conclusions**</u><font>\n",
    "\n",
    "**PROS**:\n",
    "1. Quite **fast** (not as PCA, though)\n",
    "2. **Adaptable** to any problem with numerical features\n",
    "3. Can be used in **unsupervised** or **supervised** mode (see [here](https://umap-learn.readthedocs.io/en/latest/supervised.html#:~:text=While%20UMAP%20can%20be%20used,reduction%2C%20and%20even%20metric%20learning))\n",
    "4. Will make you look cool with your Biologist friends\n",
    "\n",
    "**CONS:**\n",
    "1. Cluster **distances in embedding space not representative** of cluster differences (consequence of _non-linearity_ and _locality_)\n",
    "2. It uses Stochastic Gradient Descent $\\rightarrow$ **stochasticity** (re-running lead to slightly different results)\n",
    "3. Tricky **hyperparameter tuning**: _small_ variations lead to _great_ embedding differences\n",
    "\n",
    "**TLDR:** Use it only for **visualization**, and for clustering/classification <u>only once you understand it</u> inside-out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**Studying material**</u><font>\n",
    "\n",
    "Fantastic introductory videos by _StatQuest_:\n",
    "- **_t_-SNE**: https://www.youtube.com/watch?v=NEaUSP4YerM\n",
    "- **UMAP**: https://www.youtube.com/watch?v=eN0wFzBA4Sc \n",
    "\n",
    "Original **UMAP** paper:<br>\n",
    "[-] [McInnes, Healy & Melville, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426)<br>\n",
    "\n",
    "Python **UMAP** documentation:<br>\n",
    "[-] [Official website](https://umap-learn.readthedocs.io/en/latest/)<br>\n",
    "[-] [Parameters](https://umap-learn.readthedocs.io/en/latest/api.html)\n",
    "\n",
    "Other info and Python dimensionality reduction tools:<br>\n",
    "[-] [How exactly UMAP works](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668)<br>\n",
    "[-] [Dimensionality Reduction Toolbox](https://towardsdatascience.com/dimensionality-reduction-toolbox-in-python-9a18995927cd)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Aside from Encoding, Scaling, Feature Selection, and Feature Engeneering, more generically one might need to perform other **Data Preparation** steps.\n",
    "\n",
    "This includes:\n",
    "\n",
    "- **Inputing** missing values\n",
    "\n",
    "- Removing **outliers** (?)\n",
    "\n",
    "- Dealing with **imbalanced classes**\n",
    "\n",
    "- Handling numerical **exceptions** (zeros!)\n",
    "\n",
    "... and many more!\n",
    "\n",
    "Have a look at this [post about Data Cleaning](https://towardsdatascience.com/automated-data-cleaning-with-python-94d44d854423) as a starting point! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (a.k.a. $k$-fold Cross Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Am I overfitting the train set?\n",
    "\n",
    "This is a common doubt when we split our data in just a **train**, [a **validation**] and a **test** set (as seen [before](#Golden-rule-and-data-splitting)).\n",
    "\n",
    "_Would the results change if we shuffled the data differently?_\n",
    "\n",
    "- - -\n",
    "\n",
    "**Cross Validation** is a tecnique used assess how a model would **generalize** to unseen data.\n",
    "\n",
    "It involves _re-fitting_ the model over different partitionings of the dataset:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=800>\n",
    "        <img src=\"images/CV_k4.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 5.  Data splitting for a Cross Validation with $k = 4$ folds.\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "A **$k$-fold CV** splits the data in $k$ parts (**folds**), and, at each iteration:\n",
    "   - uses **$k$-1** folds for training\n",
    "   - uses the **remaining** fold for testing (_although we indistinctively call it **validation** set, in this context_)\n",
    "\n",
    "The model is tested $k$ times, always on unseen data.\n",
    "\n",
    "In this way, CV actually tackles 2 issues:\n",
    "\n",
    "- Assessment of **bias**<br>\n",
    "  _(i.e. how much the model depends on a specific training set)_\n",
    "\n",
    "- Exploitation of the **whole dataset**<br>\n",
    "  _(i.e. all data are eventually used for both training and testing)_\n",
    "\n",
    "Each model fit produces a performance score, therefore we can obtain both an **average** estimate, and a **uncertainty** on it.\n",
    "\n",
    "<u>IMPORTANT:</u> While the CV gives performance estimates, the final model shall be **_re_-trained on all** the data.<br>\n",
    "$~~~~~~~~~~~~~~~~~~~$ We can reasonably assume that the re-trained model will be at least as performing as the CV average.<br>\n",
    "$~~~~~~~~~~~~~~~~~~~$ (Actually, you shall _re_-fit on all data also when you use the simpler [train/validation/test](#Golden-rule-and-data-splitting) protocol).\n",
    "\n",
    "- - -\n",
    "\n",
    "**Q:** What is the right $k$?<br> \n",
    "**R:** Common values range between 3 and 10, but depends on:\n",
    "- how many data we have<br>\n",
    "- how much time we have<br>\n",
    "\n",
    "because for larger $k$ we got more estimates, but in exchange we get:\n",
    "\n",
    "- a smaller test set at each iteration\n",
    "- more computations\n",
    "\n",
    "\n",
    "<font size=3><u>**Variants**</u><font>\n",
    "\n",
    "- **Repeated CV**: CV, but repeated $n$ times with different shuffling\n",
    "\n",
    "- **Stratified CV**: CV, but the demographics of each fold is representative of the data (e.g. same class demoographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (low sample size)\n",
    "<a id='hyperparameter_tuning'></a>\n",
    "\n",
    "**Hyperparameter tuning** == **select** the best hyperparameters of a model.\n",
    "\n",
    "- <u>Large sample size</u> $\\rightarrow$ train / validation / test split<br>\n",
    "     _**train** $\\rightarrow$  try all configurations on **validation**, **select** $\\rightarrow$ **assess on test**_\n",
    "\n",
    "\n",
    "- <u>Low sample size</u> $\\rightarrow$ **CV**\n",
    "\n",
    "E.g., consider <code>sklearn.ensemble</code> Random Forests parameters:\n",
    "- <code>n_estimators</code> (_the number of trees_)\n",
    "- <code>max_depth</code> (_the _max ramification of the trees_)\n",
    "- <code>max_features</code> (_the max number of features to consider in each split_)\n",
    "\n",
    "We want to find the specific set of these hyperparameters (**configuration**) which maximizes the score on a validation set. \n",
    "\n",
    "To do this, we simply have to <u>train each configuration</u> at each CV folding, and <u>record</u> their score on the validation fold:\n",
    "\n",
    "<table><tr>\n",
    "    <td width=640>\n",
    "        <img src=\"images/Hyperpar_Tuning.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 6.  CV protocol used for hyperparameter tuning.<br>\n",
    "            (From <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\">here</a>)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "$\\rightarrow$ The **best** configuration is the one that on _average_ returns the best score.\n",
    "\n",
    "E.g. if we want to try:\n",
    "\n",
    "- <code>n_estimators</code> = [10, 50]\n",
    "- <code>max_depth</code> = [2, 8]\n",
    "- <code>max_features</code> = [3, 6]\n",
    "\n",
    "that is 2$^3$ = 8 configurations, on a 5-fold CV $\\rightarrow$ we will train 8 x 5 + 1 = 41 models.\n",
    "\n",
    "_NOTE: The final \"+1\" is because the selected configuration is **retrained on all data**._\n",
    "\n",
    "- - -\n",
    "\n",
    "<u>IMPORTANT:</u> You <u>cannot</u> use your data to tune the hyperparameters _**and**_ to assess the performance $\\rightarrow$ we would _overfit_.<br>\n",
    "$~~~~~~~~~~~~~~~~~~$ To assess the final performance, we need a **hold-out** test set (just like in Figure 6).\n",
    "\n",
    "We will come back to this in the _ML Practices Workshop_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:astrostat22] *",
   "language": "python",
   "name": "conda-env-astrostat22-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.326px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
